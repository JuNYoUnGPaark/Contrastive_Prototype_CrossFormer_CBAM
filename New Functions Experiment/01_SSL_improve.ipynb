{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad7451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Random Seed ========================\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f78ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"실험 설정\"\"\"\n",
    "    data_dir: str = \"C://Users/park9/HAR/SSL_HAR/data\"\n",
    "    save_dir: str = \"C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\"\n",
    "\n",
    "    # SSL Pretrain 파라미터\n",
    "    pretrain_epochs: int = 100\n",
    "    pretrain_batch_size: int = 512  # ✅ InfoNCE 성능 향상\n",
    "    pretrain_lr: float = 1e-3\n",
    "    pretrain_warmup_epochs: int = 10  # ✅ Warmup\n",
    "\n",
    "    # Supervised / Linear Eval / Fine-tune 파라미터\n",
    "    finetune_epochs: int = 50\n",
    "    finetune_batch_size: int = 128\n",
    "    finetune_lr: float = 3e-4\n",
    "    finetune_warmup_epochs: int = 5  # ✅ Warmup\n",
    "    finetune_backbone_lr_ratio: float = 0.1  # ✅ 백본 LR 비율\n",
    "\n",
    "    # 공통 파라미터\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.05\n",
    "    use_ema: bool = True  # ✅ EMA 사용\n",
    "    ema_decay: float = 0.9995  # ✅ EMA decay\n",
    "    consistency_weight: float = 0.2  # ✅ 일관성 손실 가중치\n",
    "\n",
    "    # 모델 파라미터\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    hyperbolic_c_init: float = 1.0  # ✅ 초기값 (학습 가능)\n",
    "\n",
    "    # SSL 파라미터\n",
    "    temperature: float = 0.07\n",
    "    projection_dim: int = 128\n",
    "\n",
    "    # Augmentation 파라미터 (✅ 전이 강건성 최적화)\n",
    "    aug_jitter_scale: float = 0.05\n",
    "    aug_scale_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    aug_channel_drop_prob: float = 0.2\n",
    "    aug_time_warp_prob: float = 0.10  # ✅ 0.3 → 0.10\n",
    "    aug_cutout_prob: float = 0.20     # ✅ 0.3 → 0.20\n",
    "    aug_cutout_ratio: float = 0.10    # ✅ 0.2 → 0.10\n",
    "\n",
    "    # 시스템 파라미터\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c29fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Dataset Configuration ========================\n",
    "\n",
    "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
    "RAW_CHANNELS = [\n",
    "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
    "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
    "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
    "]\n",
    "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
    "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
    "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
    "\n",
    "def load_split_raw(root: str, split: str):\n",
    "    assert split in (\"train\", \"test\")\n",
    "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
    "\n",
    "    # 존재 확인(문제 있으면 바로 어디가 없는지 알려줌)\n",
    "    if not os.path.isdir(inertial_path):\n",
    "        raise FileNotFoundError(f\"[Missing dir] {inertial_path}\")\n",
    "\n",
    "    X_list = []\n",
    "    for p, e in RAW_CHANNELS:\n",
    "        fpath = os.path.join(inertial_path, f\"{p}{split}.{e}\")  # ex) body_acc_x_train.txt\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise FileNotFoundError(f\"[Missing file] {fpath}\")\n",
    "        # URL 오인 방지: 파일 핸들로 전달\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            arr = np.loadtxt(f)       # (N, 128)\n",
    "        X_list.append(arr[..., None]) # (N, 128, 1)\n",
    "\n",
    "    # 채널 모두 같은 샘플 수인지 체크(안전장치)\n",
    "    n_samples = {x.shape[0] for x in X_list}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(f\"채널별 샘플 수 불일치: {n_samples}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)  # (N, 9, 128)\n",
    "\n",
    "    y_path = os.path.join(root, split, f\"y_{split}.txt\")\n",
    "    if not os.path.isfile(y_path):\n",
    "        raise FileNotFoundError(f\"[Missing file] {y_path}\")\n",
    "    with open(y_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        y = np.loadtxt(f).astype(int) - 1  # 0-based\n",
    "\n",
    "    print(f\"[OK] {split}: X{X.shape}, y{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "class UCIHARInertial(Dataset):\n",
    "    \"\"\"UCI-HAR Dataset (✅ 정규화 버그 수정)\"\"\"\n",
    "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
    "                 preloaded_data: Tuple[np.ndarray, np.ndarray] = None):\n",
    "        super().__init__()\n",
    "        if preloaded_data is not None:\n",
    "            X, y = preloaded_data\n",
    "        else:\n",
    "            X, y = load_split_raw(root, split)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
    "\n",
    "        # mean/std 세팅\n",
    "        if mean is not None and std is not None:\n",
    "            self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
    "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
    "\n",
    "        # ✅ preloaded_data 여부와 무관하게 항상 train 통계로 정규화\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c846a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Label-Independent Augmentations ========================\n",
    "\n",
    "def random_jitter(x: torch.Tensor, scale: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = torch.randn_like(x) * scale\n",
    "    return x + noise\n",
    "\n",
    "def random_scaling(x: torch.Tensor, scale_range: Tuple[float, float] = (0.8, 1.2)) -> torch.Tensor:\n",
    "    \"\"\"Random scaling of amplitudes\"\"\"\n",
    "    scale = torch.empty(x.size(0), x.size(1), 1, device=x.device).uniform_(*scale_range)\n",
    "    return x * scale\n",
    "\n",
    "def random_channel_drop(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly drop channels (set to zero)\"\"\"\n",
    "    B, C, T = x.shape\n",
    "    mask = torch.rand(B, C, 1, device=x.device) > drop_prob\n",
    "    return x * mask.float()\n",
    "\n",
    "def random_time_warp(x: torch.Tensor, warp_prob: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Simple time warping by random interpolation\"\"\"\n",
    "    if random.random() > warp_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    warp_factor = random.uniform(0.8, 1.2)\n",
    "    new_T = int(T * warp_factor)\n",
    "\n",
    "    x_warped = F.interpolate(x, size=new_T, mode='linear', align_corners=False)\n",
    "\n",
    "    if new_T > T:\n",
    "        start = random.randint(0, new_T - T)\n",
    "        x_warped = x_warped[:, :, start:start+T]\n",
    "    elif new_T < T:\n",
    "        pad_total = T - new_T\n",
    "        pad_left = random.randint(0, pad_total)\n",
    "        pad_right = pad_total - pad_left\n",
    "        x_warped = F.pad(x_warped, (pad_left, pad_right), mode='replicate')\n",
    "\n",
    "    return x_warped\n",
    "\n",
    "def random_cutout(x: torch.Tensor, cutout_prob: float = 0.20, cutout_ratio: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Randomly mask out a temporal segment\"\"\"\n",
    "    if random.random() > cutout_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    cutout_len = int(T * cutout_ratio)\n",
    "    start = random.randint(0, T - cutout_len)\n",
    "    x_cut = x.clone()\n",
    "    x_cut[:, :, start:start+cutout_len] = 0\n",
    "    return x_cut\n",
    "\n",
    "def augment_time_series(x: torch.Tensor, cfg: Config) -> torch.Tensor:\n",
    "    \"\"\"Label-independent augmentation pipeline\"\"\"\n",
    "    x_aug = x.clone()\n",
    "    x_aug = random_jitter(x_aug, scale=cfg.aug_jitter_scale)\n",
    "    x_aug = random_scaling(x_aug, scale_range=cfg.aug_scale_range)\n",
    "    x_aug = random_channel_drop(x_aug, drop_prob=cfg.aug_channel_drop_prob)\n",
    "    x_aug = random_time_warp(x_aug, warp_prob=cfg.aug_time_warp_prob)\n",
    "    x_aug = random_cutout(x_aug, cutout_prob=cfg.aug_cutout_prob, cutout_ratio=cfg.aug_cutout_ratio)\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Tail-Head Stitch (전이 유사 혼합) ========================\n",
    "\n",
    "def tail_head_stitch(x_a: torch.Tensor, x_b: torch.Tensor, mix: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tail-Head Stitch: x_a의 앞부분 + x_b의 뒷부분\n",
    "    전이 테스트셋과 유사한 경계 혼합 생성\n",
    "    \"\"\"\n",
    "    B, C, T = x_a.shape\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    x_mix = x_a.clone()\n",
    "    x_mix[:, :, -mix_pts:] = x_b[:, :, :mix_pts]\n",
    "\n",
    "    return x_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2a6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== ResNet Building Blocks ========================\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"1D Residual Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_blocks=[2, 2, 2]):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(d_model, num_blocks[2], stride=2)\n",
    "\n",
    "        self.stride = 16\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResBlock1D(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock1D(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6ca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transformer Encoder ========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Module\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a59e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Backbone ========================\n",
    "\n",
    "class ResNetTransformerBackbone(nn.Module):\n",
    "    \"\"\"ResNet + Transformer Encoder Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet1D(in_channels=in_channels, d_model=d_model)\n",
    "        self.transformer = TransformerEncoder(d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.stride = self.resnet.stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = self.resnet(x)\n",
    "        fmap = self.transformer(fmap)\n",
    "        return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b872092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Projection Head ========================\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"MLP projection head for contrastive learning\"\"\"\n",
    "    def __init__(self, d_model, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06574fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Classification Heads ========================\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Linear Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "class HyperbolicProjection(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Projection (✅ 학습 가능한 c)\"\"\"\n",
    "    def __init__(self, c_init=1.0):\n",
    "        super().__init__()\n",
    "        self.c = nn.Parameter(torch.tensor(c_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ✅ Feature norm clipping\n",
    "        x = torch.clamp(x, -5.0, 5.0)\n",
    "\n",
    "        c = torch.clamp(self.c, min=0.1, max=10.0)\n",
    "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
    "        max_norm = (1.0 / math.sqrt(c)) - 1e-4\n",
    "        scale = torch.clamp(norm, max=max_norm) / norm\n",
    "        return x * scale\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int, c_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pre_proj = nn.Linear(d_model, d_model)\n",
    "        self.hyperbolic_proj = HyperbolicProjection(c_init=c_init)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        h = self.pre_proj(pooled)\n",
    "        h_hyp = self.hyperbolic_proj(h)\n",
    "        logits = self.fc(h_hyp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249fa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SSL Model ========================\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    \"\"\"Self-Supervised Learning Model\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetTransformerBackbone(\n",
    "            in_channels=9, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(d_model, projection_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns normalized projection\"\"\"\n",
    "        fmap = self.backbone(x)\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        z = self.projection_head(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42036e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== EMA Utility ========================\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model parameters\"\"\"\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: param.clone().detach()\n",
    "                       for name, param in model.named_parameters() if param.requires_grad}\n",
    "        self.backup = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        \"\"\"Update EMA parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                self.shadow[name].mul_(self.decay).add_(param.data, alpha=1 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        \"\"\"Apply EMA parameters to model\"\"\"\n",
    "        self.backup = {name: param.data.clone() for name, param in model.named_parameters() if param.requires_grad}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        \"\"\"Restore original parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f86599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Learning Rate Scheduler ========================\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Cosine annealing with linear warmup\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48ebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Contrastive Loss ========================\n",
    "\n",
    "def contrastive_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"NT-Xent Loss (InfoNCE)\"\"\"\n",
    "    B = z1.shape[0]\n",
    "    device = z1.device\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
    "\n",
    "    labels = torch.arange(B, device=device)\n",
    "    labels = torch.cat([labels + B, labels], dim=0)\n",
    "\n",
    "    mask = torch.eye(2 * B, device=device, dtype=torch.bool)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc243dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Consistency Loss ========================\n",
    "\n",
    "def consistency_loss(model: nn.Module, head: nn.Module, x_a: torch.Tensor, x_b: torch.Tensor,\n",
    "                     mix: float = 0.5, device: str = \"cuda\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    전이-유사 일관성 손실\n",
    "    Tail-Head Stitch 후 예측 분포의 KL divergence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        fmap_a = model.backbone(x_a) if hasattr(model, 'backbone') else model(x_a)\n",
    "        fmap_b = model.backbone(x_b) if hasattr(model, 'backbone') else model(x_b)\n",
    "\n",
    "        logits_a = head(fmap_a)\n",
    "        logits_b = head(fmap_b)\n",
    "\n",
    "        p_a = F.softmax(logits_a, dim=-1)\n",
    "        p_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    x_mix = tail_head_stitch(x_a, x_b, mix=mix)\n",
    "\n",
    "    fmap_mix = model.backbone(x_mix) if hasattr(model, 'backbone') else model(x_mix)\n",
    "    logits_mix = head(fmap_mix)\n",
    "    p_mix = F.log_softmax(logits_mix, dim=-1)\n",
    "\n",
    "    # Soft target: 0.5*p_a + 0.5*p_b\n",
    "    p_target = 0.5 * p_a + 0.5 * p_b\n",
    "\n",
    "    loss = F.kl_div(p_mix, p_target, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Training Functions ========================\n",
    "\n",
    "def pretrain_one_epoch(model: SSLModel, loader: DataLoader, opt: torch.optim.Optimizer,\n",
    "                       scheduler, ema: Optional[EMA], cfg: Config):\n",
    "    \"\"\"SSL Pretrain: No labels, only contrastive loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        x1 = augment_time_series(x, cfg)\n",
    "        x2 = augment_time_series(x, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = contrastive_loss(z1, z2, temperature=cfg.temperature)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "    return {\"ssl_loss\": total_loss / total_samples}\n",
    "\n",
    "def linear_eval_epoch(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                      opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Linear evaluation: Freeze backbone, train head only\"\"\"\n",
    "    if train:\n",
    "        backbone.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fmap = backbone(x)\n",
    "\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "def finetune_epoch(model: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   opt: torch.optim.Optimizer, scheduler, ema: Optional[EMA],\n",
    "                   cfg: Config, train: bool = True):\n",
    "    \"\"\"Fine-tuning: Train both backbone and head (✅ 일관성 손실 추가)\"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "        head.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "    total_correct, total_samples = 0, 0\n",
    "\n",
    "    data_iter = iter(loader)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        # Forward pass\n",
    "        fmap = model.backbone(x) if hasattr(model, 'backbone') else model(x)\n",
    "        logits = head(fmap)\n",
    "        loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        # ✅ 일관성 손실 (학습 시에만)\n",
    "        loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "        if train and cfg.consistency_weight > 0:\n",
    "            try:\n",
    "                x_b, _ = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(loader)\n",
    "                x_b, _ = next(data_iter)\n",
    "\n",
    "            x_b = x_b.to(cfg.device)\n",
    "            if x_b.size(0) == x.size(0):\n",
    "                loss_cons = consistency_loss(model, head, x, x_b, mix=0.5, device=cfg.device)\n",
    "\n",
    "        loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(head.parameters()), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_ce_loss += loss_ce.item() * y.size(0)\n",
    "        total_cons_loss += loss_cons.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"ce_loss\": total_ce_loss / total_samples,\n",
    "        \"cons_loss\": total_cons_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   ema: Optional[EMA], cfg: Config, use_ema: bool = False):\n",
    "    \"\"\"Evaluate model (✅ EMA 지원)\"\"\"\n",
    "    if use_ema and ema is not None:\n",
    "        ema.apply_shadow(backbone)\n",
    "\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        fmap = backbone(x)\n",
    "        logits = head(fmap)\n",
    "        y_pred.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    if use_ema and ema is not None:\n",
    "        ema.restore(backbone)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8275be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transitional Test Set ========================\n",
    "\n",
    "def create_transitional_test_set(\n",
    "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
    ") -> Tuple[UCIHARInertial, dict]:\n",
    "    \"\"\"Create transitional test set (✅ 정규화 보장)\"\"\"\n",
    "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
    "    N, C, T = X.shape\n",
    "\n",
    "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
    "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
    "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
    "    for t, s in zip(targets_A, sources_B):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
    "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
    "    for t, s in zip(targets_B, sources_A):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    # (수정) 이중 정규화 방지: 원본 스케일로 복원\n",
    "    X_restored = (X * orig_dataset.std) + orig_dataset.mean\n",
    "\n",
    "    # ✅ train 통계로 정규화하도록 mean/std 전달\n",
    "    mod_dataset = UCIHARInertial(\n",
    "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
    "        preloaded_data=(X_restored, y)\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        'class_A': class_A,\n",
    "        'class_B': class_B,\n",
    "        'p': p,\n",
    "        'mix': mix,\n",
    "        'modified_samples': len(targets_A) + len(targets_B),\n",
    "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
    "    }\n",
    "    return mod_dataset, info\n",
    "\n",
    "# ======================== JSON Encoder ========================\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON Encoder for NumPy types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# ======================== Main Experiment Function ========================\n",
    "\n",
    "def run_full_comparison(cfg: Config):\n",
    "    \"\"\"Run complete supervised vs SSL comparison\"\"\"\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\n📦 Loading UCI-HAR Dataset...\")\n",
    "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
    "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
    "    print(f\"   - Train samples: {len(train_set)}\")\n",
    "    print(f\"   - Test samples: {len(test_set_orig)}\")\n",
    "\n",
    "    # Create transitional test sets (✅ 2레벨 강도)\n",
    "    scenarios = [\n",
    "        # Level 1: Moderate (중간 강도)\n",
    "        (\"STANDING\", \"SITTING\", 0.50, 0.40),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.55, 0.42),\n",
    "        # Level 2: Strong (강한 강도)\n",
    "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
    "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"    🔬 TRANSITIONAL TEST SETS 생성 (2레벨 강도)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    transition_test_data = []\n",
    "    for i, (clsA, clsB, p, mix) in enumerate(scenarios):\n",
    "        test_set_mod, info = create_transitional_test_set(test_set_orig, clsA, clsB, p=p, mix=mix)\n",
    "        transition_test_data.append((test_set_mod, info))\n",
    "        level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "        print(f\"   - [{level}] {clsA}↔{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}개 샘플 변형\")\n",
    "\n",
    "    # Experiment configurations\n",
    "    experiment_configs = [\n",
    "        {\"name\": \"Supervised_Linear\", \"method\": \"supervised\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"Supervised_Hyperbolic\", \"method\": \"supervised\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_LinearEval_Linear\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_LinearEval_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_FineTune_Linear\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_FineTune_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": True},\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    for exp_cfg in experiment_configs:\n",
    "        print(f\"\\n{'='*80}\\n   실험: {exp_cfg['name']}\\n{'='*80}\")\n",
    "\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "        if exp_cfg['method'] == 'supervised':\n",
    "            # Supervised Learning\n",
    "            print(\"\\n📚 Supervised Learning (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            backbone = ResNetTransformerBackbone(\n",
    "                in_channels=9, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "                n_layers=cfg.n_layers, dropout=cfg.dropout\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            params = list(backbone.parameters()) + list(head.parameters())\n",
    "            opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ✅ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "            warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "            scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "            # ✅ EMA\n",
    "            ema = EMA(backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            def train_supervised_epoch(backbone, head, loader, opt, scheduler, ema, cfg, train=True):\n",
    "                if train:\n",
    "                    backbone.train()\n",
    "                    head.train()\n",
    "                else:\n",
    "                    backbone.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "                total_correct, total_samples = 0, 0\n",
    "\n",
    "                data_iter = iter(loader)\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "                    fmap = backbone(x)\n",
    "                    logits = head(fmap)\n",
    "                    loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "                    # ✅ 일관성 손실\n",
    "                    loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "                    if train and cfg.consistency_weight > 0:\n",
    "                        try:\n",
    "                            x_b, _ = next(data_iter)\n",
    "                        except StopIteration:\n",
    "                            data_iter = iter(loader)\n",
    "                            x_b, _ = next(data_iter)\n",
    "\n",
    "                        x_b = x_b.to(cfg.device)\n",
    "                        if x_b.size(0) == x.size(0):\n",
    "                            loss_cons = consistency_loss(\n",
    "                                type('Model', (), {'backbone': backbone})(),\n",
    "                                head, x, x_b, mix=0.5, device=cfg.device\n",
    "                            )\n",
    "\n",
    "                    loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "                    if train:\n",
    "                        opt.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "                        opt.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                        if ema is not None:\n",
    "                            ema.update(backbone)\n",
    "\n",
    "                    pred = logits.argmax(dim=-1)\n",
    "                    total_correct += (pred == y).sum().item()\n",
    "                    total_loss += loss.item() * y.size(0)\n",
    "                    total_ce_loss += loss_ce.item() * y.size(0)\n",
    "                    total_cons_loss += loss_cons.item() * y.size(0)\n",
    "                    total_samples += y.size(0)\n",
    "\n",
    "                return {\n",
    "                    \"loss\": total_loss / total_samples,\n",
    "                    \"ce_loss\": total_ce_loss / total_samples,\n",
    "                    \"cons_loss\": total_cons_loss / total_samples,\n",
    "                    \"acc\": total_correct / total_samples\n",
    "                }\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"Training for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_supervised_epoch(backbone, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "                te_acc, te_f1 = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'backbone': copy.deepcopy(backbone.state_dict()),\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if ema is not None:\n",
    "                        best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Supervised {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                backbone.load_state_dict(best_wts['backbone'])\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if ema is not None and 'ema_shadow' in best_wts:\n",
    "                    ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(backbone, head, test_loader_mod, ema, cfg, use_ema=cfg.use_ema)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}↔{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ✅ 레벨별 분석\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"supervised\",\n",
    "                \"mode\": \"supervised\",\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "        else:  # SSL\n",
    "            # Stage 1: SSL Pretrain\n",
    "            print(\"\\n📚 Stage 1: Self-Supervised Pretraining (No Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            ssl_model = SSLModel(\n",
    "                d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers,\n",
    "                dropout=cfg.dropout, projection_dim=cfg.projection_dim\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            pretrain_loader = DataLoader(train_set, cfg.pretrain_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            ssl_opt = torch.optim.AdamW(ssl_model.parameters(), lr=cfg.pretrain_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ✅ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.pretrain_epochs * len(pretrain_loader)\n",
    "            warmup_steps = cfg.pretrain_warmup_epochs * len(pretrain_loader)\n",
    "            ssl_scheduler = get_cosine_schedule_with_warmup(ssl_opt, warmup_steps, total_steps)\n",
    "\n",
    "            # ✅ EMA\n",
    "            ssl_ema = EMA(ssl_model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            print(f\"Pretraining for {cfg.pretrain_epochs} epochs...\")\n",
    "            for epoch in range(1, cfg.pretrain_epochs + 1):\n",
    "                stats = pretrain_one_epoch(ssl_model, pretrain_loader, ssl_opt, ssl_scheduler, ssl_ema, cfg)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Pretrain {epoch:03d}/{cfg.pretrain_epochs}] SSL Loss: {stats['ssl_loss']:.4f}\")\n",
    "\n",
    "            # ✅ EMA 적용\n",
    "            if ssl_ema is not None:\n",
    "                ssl_ema.apply_shadow(ssl_model)\n",
    "\n",
    "            print(\"✅ Pretraining Complete!\")\n",
    "\n",
    "            # Stage 2: Linear Eval or Fine-tune\n",
    "            print(f\"\\n📚 Stage 2: {exp_cfg['mode'].upper()} (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            if exp_cfg['mode'] == 'linear_eval':\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                opt = torch.optim.AdamW(head.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                ema = None  # Linear eval에서는 백본이 frozen이므로 EMA 불필요\n",
    "\n",
    "                train_fn = lambda: linear_eval_epoch(ssl_model.backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            else:  # finetune\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                # ✅ 백본/헤드 분리 학습률\n",
    "                backbone_params = list(ssl_model.backbone.parameters())\n",
    "                head_params = list(head.parameters())\n",
    "\n",
    "                opt = torch.optim.AdamW([\n",
    "                    {'params': backbone_params, 'lr': cfg.finetune_lr * cfg.finetune_backbone_lr_ratio},\n",
    "                    {'params': head_params, 'lr': cfg.finetune_lr},\n",
    "                ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                # ✅ EMA (백본만)\n",
    "                ema = EMA(ssl_model.backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "                train_fn = lambda: finetune_epoch(ssl_model, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"{exp_cfg['mode']} for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_fn()\n",
    "                te_acc, te_f1 = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if exp_cfg['mode'] == 'finetune':\n",
    "                        best_wts['backbone'] = copy.deepcopy(ssl_model.backbone.state_dict())\n",
    "                        if ema is not None:\n",
    "                            best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    if 'cons_loss' in stats:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if exp_cfg['mode'] == 'finetune':\n",
    "                    ssl_model.backbone.load_state_dict(best_wts['backbone'])\n",
    "                    if ema is not None and 'ema_shadow' in best_wts:\n",
    "                        ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(ssl_model.backbone, head, test_loader_mod, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}↔{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ✅ 레벨별 분석\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"ssl\",\n",
    "                \"mode\": exp_cfg['mode'],\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"   📊 SUPERVISED vs TRUE SSL 실험 결과 (최적화 버전)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Method':<12} {'Mode':<12} {'Classifier':<12} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<10}\")\n",
    "    print(\"-\" * 115)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['method']:<12} {r['mode']:<12} {r['classifier']:<12} \"\n",
    "              f\"{r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
    "              f\"{r['avg_drop']:.4f}  {r['retention']:.2f}%\")\n",
    "\n",
    "    # ✅ 레벨별 결과 추가\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 레벨별 Retention 분석\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Overall':<12} {'Moderate':<12} {'Strong':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['retention']:>6.2f}%      {r['retention_moderate']:>6.2f}%       {r['retention_strong']:>6.2f}%\")\n",
    "\n",
    "    # Detailed analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 상세 비교 분석\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Final ranking\n",
    "    sorted_results = sorted(results_table, key=lambda x: x['retention'], reverse=True)\n",
    "    print(\"\\n🏆 최종 성능 랭킹 (Overall Retention 기준)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for rank, r in enumerate(sorted_results, 1):\n",
    "        method_mode = f\"{r['method']}-{r['mode']}\" if r['method'] == 'ssl' else r['method']\n",
    "        print(f\"   {rank}. {r['config']:<35} ({method_mode:<20}) \"\n",
    "              f\"Retention: {r['retention']:.2f}% (Mod: {r['retention_moderate']:.2f}% | Str: {r['retention_strong']:.2f}%)\")\n",
    "\n",
    "    best_config = sorted_results[0]\n",
    "    best_ssl = max([r for r in results_table if r['method'] == 'ssl'], key=lambda x: x['retention'])\n",
    "    best_sup = max([r for r in results_table if r['method'] == 'supervised'], key=lambda x: x['retention'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 결론\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - 최고 성능: {best_config['config']} (Retention: {best_config['retention']:.2f}%)\")\n",
    "    print(f\"   - Supervised baseline: {best_sup['retention']:.2f}% (Mod: {best_sup['retention_moderate']:.2f}% | Str: {best_sup['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - SSL best: {best_ssl['retention']:.2f}% (Mod: {best_ssl['retention_moderate']:.2f}% | Str: {best_ssl['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - Performance gap: {abs(best_ssl['retention'] - best_sup['retention']):.2f}pp\")\n",
    "\n",
    "    # ✅ 개선 효과 분석\n",
    "    print(\"\\n   ✨ 최적화 개선사항:\")\n",
    "    print(\"   - 정규화 버그 수정 ✅\")\n",
    "    print(\"   - Cosine + Warmup 스케줄러 ✅\")\n",
    "    print(\"   - EMA (Exponential Moving Average) ✅\")\n",
    "    print(\"   - 백본/헤드 분리 학습률 (Fine-tune) ✅\")\n",
    "    print(\"   - 전이-유사 일관성 손실 (Tail-Head Stitch) ✅\")\n",
    "    print(f\"   - 증강 강도 최적화 (warp: 0.10, cutout: 0.10) ✅\")\n",
    "    print(\"   - 학습 가능한 Hyperbolic c ✅\")\n",
    "    print(\"   - 2레벨 전이 시나리오 (Moderate/Strong) ✅\")\n",
    "\n",
    "    # Save results\n",
    "    save_path = os.path.join(cfg.save_dir, \"supervised_vs_ssl_results_optimized.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    visualization_data = {\n",
    "        'configs': [r['config'] for r in results_table],\n",
    "        'methods': [r['method'] for r in results_table],\n",
    "        'modes': [r['mode'] for r in results_table],\n",
    "        'classifiers': [r['classifier'] for r in results_table],\n",
    "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
    "        'orig_f1': [r['orig_f1'] for r in results_table],\n",
    "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
    "        'trans_f1': [r['avg_trans_f1'] for r in results_table],\n",
    "        'retention': [r['retention'] for r in results_table],\n",
    "        'retention_moderate': [r['retention_moderate'] for r in results_table],\n",
    "        'retention_strong': [r['retention_strong'] for r in results_table],\n",
    "        'avg_drop': [r['avg_drop'] for r in results_table]\n",
    "    }\n",
    "\n",
    "    viz_path = os.path.join(cfg.save_dir, \"visualization_data_optimized.json\")\n",
    "    with open(viz_path, \"w\") as f:\n",
    "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n✅ Results saved to:\")\n",
    "    print(f\"   - {save_path}\")\n",
    "    print(f\"   - {viz_path}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c68009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   🧪 UCI-HAR Comprehensive Comparison (✨ OPTIMIZED)\n",
      "   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\n",
      "   Architecture: ResNet + Transformer Encoder\n",
      "================================================================================\n",
      "\n",
      "   📋 실험 설계:\n",
      "   1. 동일한 백본 (ResNet + Transformer)\n",
      "   2. 2레벨 전이 데이터셋 (Moderate/Strong)\n",
      "   3. 6가지 설정 비교:\n",
      "      ├─ Supervised × (Linear, Hyperbolic)\n",
      "      ├─ SSL Linear Eval × (Linear, Hyperbolic)\n",
      "      └─ SSL Fine-tune × (Linear, Hyperbolic)\n",
      "================================================================================\n",
      "\n",
      "   ⚙️  Supervised 설정:\n",
      "   - Epochs: 50\n",
      "   - Batch size: 128\n",
      "   - Learning rate: 0.0003\n",
      "   - Warmup: 5 epochs\n",
      "   - EMA decay: 0.9995\n",
      "   - Consistency weight: 0.2\n",
      "   - Training: End-to-end with labels + consistency loss\n",
      "\n",
      "   ⚙️  SSL 설정:\n",
      "   - Stage 1 (Pretrain): 100 epochs, batch=512, lr=0.001\n",
      "     → Contrastive learning only (NO LABELS)\n",
      "     → Label-independent augmentation\n",
      "     → Warmup: 10 epochs\n",
      "     → EMA decay: 0.9995\n",
      "   - Stage 2 (Eval/FT): 50 epochs, batch=128, lr=0.0003\n",
      "     → Linear Eval: Freeze backbone\n",
      "     → Fine-tune: Train all (backbone LR × 0.1)\n",
      "     → Consistency weight: 0.2\n",
      "\n",
      "   🔧 Augmentations (SSL - Optimized):\n",
      "   - Jitter (scale=0.05)\n",
      "   - Scaling (range=(0.8, 1.2))\n",
      "   - Channel Drop (prob=0.2)\n",
      "   - Time Warp (prob=0.1) ✅ 0.3→0.10\n",
      "   - Cutout (prob=0.2, ratio=0.1) ✅ 0.2→0.10\n",
      "   - ALL label-independent!\n",
      "\n",
      "   🏗️  Architecture:\n",
      "   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads=4, layers=2)\n",
      "   - d_model: 128, dropout: 0.1\n",
      "   - Classifier: Linear vs Hyperbolic (c_init=1.0, learnable ✅)\n",
      "   - Projection dim (SSL): 128\n",
      "\n",
      "   🔬 SSL Contrastive Learning:\n",
      "   - Loss: NT-Xent (InfoNCE)\n",
      "   - Temperature: 0.07\n",
      "   - Negative samples: 2*batch_size - 2\n",
      "\n",
      "   ✨ 최적화 개선사항:\n",
      "   - 정규화 버그 수정 (전이 테스트셋)\n",
      "   - Cosine Annealing + Warmup\n",
      "   - EMA (Exponential Moving Average)\n",
      "   - 백본/헤드 분리 학습률 (Fine-tune)\n",
      "   - 전이-유사 일관성 손실 (Tail-Head Stitch)\n",
      "   - 증강 강도 최적화 (경계 정보 보존)\n",
      "   - 학습 가능한 Hyperbolic c\n",
      "   - 2레벨 전이 시나리오 (Moderate/Strong)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "📦 Loading UCI-HAR Dataset...\n",
      "[OK] train: X(7352, 9, 128), y(7352,)\n",
      "[OK] test: X(2947, 9, 128), y(2947,)\n",
      "   - Train samples: 7352\n",
      "   - Test samples: 2947\n",
      "\n",
      "================================================================================\n",
      "    🔬 TRANSITIONAL TEST SETS 생성 (2레벨 강도)\n",
      "================================================================================\n",
      "   - [Moderate] STANDING↔SITTING (p=0.50, mix=0.40): 511개 샘플 변형\n",
      "   - [Moderate] WALKING↔WALKING_UPSTAIRS (p=0.55, mix=0.42): 531개 샘플 변형\n",
      "   - [Strong] STANDING↔SITTING (p=0.70, mix=0.55): 715개 샘플 변형\n",
      "   - [Strong] WALKING↔WALKING_UPSTAIRS (p=0.65, mix=0.52): 628개 샘플 변형\n",
      "   - [Strong] SITTING↔LAYING (p=0.75, mix=0.58): 770개 샘플 변형\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.2416 CE:1.2327 Cons:0.0440 A:0.5866 | Test A:0.4591 F1:0.3215\n",
      "[Supervised 10/50] Train L:0.3228 CE:0.3192 Cons:0.0181 A:0.9607 | Test A:0.5110 F1:0.4325\n",
      "[Supervised 20/50] Train L:0.3022 CE:0.2994 Cons:0.0140 A:0.9701 | Test A:0.7044 F1:0.6445\n",
      "[Supervised 30/50] Train L:0.2891 CE:0.2864 Cons:0.0134 A:0.9769 | Test A:0.8544 F1:0.8459\n",
      "[Supervised 40/50] Train L:0.2723 CE:0.2689 Cons:0.0168 A:0.9857 | Test A:0.9216 F1:0.9206\n",
      "[Supervised 50/50] Train L:0.2712 CE:0.2678 Cons:0.0172 A:0.9857 | Test A:0.9253 F1:0.9251\n",
      "✅ Best Test Acc: 0.9294\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.8867 F1=0.8848 (Drop=0.0428)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.8775 F1=0.8725 (Drop=0.0519)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8409 F1=0.8360 (Drop=0.0886)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8405 F1=0.8267 (Drop=0.0889)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.8120 F1=0.7906 (Drop=0.1174)\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.7034 CE:1.7033 Cons:0.0006 A:0.4396 | Test A:0.4079 F1:0.2542\n",
      "[Supervised 10/50] Train L:0.6709 CE:0.6693 Cons:0.0078 A:0.9610 | Test A:0.1853 F1:0.0845\n",
      "[Supervised 20/50] Train L:0.4042 CE:0.4021 Cons:0.0105 A:0.9648 | Test A:0.7133 F1:0.6760\n",
      "[Supervised 30/50] Train L:0.3339 CE:0.3312 Cons:0.0130 A:0.9744 | Test A:0.7034 F1:0.6484\n",
      "[Supervised 40/50] Train L:0.3047 CE:0.3019 Cons:0.0141 A:0.9849 | Test A:0.8446 F1:0.8319\n",
      "[Supervised 50/50] Train L:0.2996 CE:0.2966 Cons:0.0152 A:0.9878 | Test A:0.8731 F1:0.8683\n",
      "✅ Best Test Acc: 0.8731\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.8493 F1=0.8374 (Drop=0.0238)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.8290 F1=0.8227 (Drop=0.0441)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8249 F1=0.8062 (Drop=0.0482)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.7615 F1=0.7482 (Drop=0.1116)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.7743 F1=0.7417 (Drop=0.0987)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 10/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 20/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 30/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 40/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 50/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "✅ Best Test Acc: 0.1666\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 10/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 20/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 30/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 40/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 50/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "✅ Best Test Acc: 0.1785\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.1775 F1=0.0652 (Drop=0.0010)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.1785 F1=0.0658 (Drop=0.0000)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.1778 F1=0.0655 (Drop=0.0007)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.1795 F1=0.0674 (Drop=-0.0010)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.1819 F1=0.0670 (Drop=-0.0034)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.8212 CE:1.8143 Cons:0.0343 A:0.1974 | Test A:0.1452 F1:0.1147\n",
      "[finetune 10/50] Train L:0.3364 CE:0.3205 Cons:0.0795 A:0.9618 | Test A:0.3536 F1:0.3221\n",
      "[finetune 20/50] Train L:0.3127 CE:0.3002 Cons:0.0625 A:0.9706 | Test A:0.3519 F1:0.3056\n",
      "[finetune 30/50] Train L:0.2958 CE:0.2845 Cons:0.0561 A:0.9796 | Test A:0.3641 F1:0.3323\n",
      "[finetune 40/50] Train L:0.2929 CE:0.2816 Cons:0.0566 A:0.9810 | Test A:0.3526 F1:0.3161\n",
      "[finetune 50/50] Train L:0.2857 CE:0.2748 Cons:0.0548 A:0.9848 | Test A:0.3380 F1:0.3133\n",
      "✅ Best Test Acc: 0.3946\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.3780 F1=0.3540 (Drop=0.0166)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.3543 F1=0.3043 (Drop=0.0404)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.3410 F1=0.3288 (Drop=0.0536)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.3488 F1=0.2934 (Drop=0.0458)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.3604 F1=0.3248 (Drop=0.0343)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.7690 CE:1.7689 Cons:0.0004 A:0.2216 | Test A:0.2257 F1:0.1328\n",
      "[finetune 10/50] Train L:0.6626 CE:0.6589 Cons:0.0185 A:0.9593 | Test A:0.6485 F1:0.6061\n",
      "[finetune 20/50] Train L:0.4208 CE:0.4155 Cons:0.0262 A:0.9705 | Test A:0.6634 F1:0.6368\n",
      "[finetune 30/50] Train L:0.3565 CE:0.3505 Cons:0.0299 A:0.9786 | Test A:0.6264 F1:0.5814\n",
      "[finetune 40/50] Train L:0.3340 CE:0.3277 Cons:0.0314 A:0.9827 | Test A:0.6318 F1:0.5905\n",
      "[finetune 50/50] Train L:0.3313 CE:0.3253 Cons:0.0301 A:0.9838 | Test A:0.6284 F1:0.5843\n",
      "✅ Best Test Acc: 0.6634\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.6566 F1=0.6293 (Drop=0.0068)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.6407 F1=0.6060 (Drop=0.0227)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.6390 F1=0.6082 (Drop=0.0244)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.6240 F1=0.5850 (Drop=0.0394)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.5005 F1=0.4954 (Drop=0.1629)\n",
      "\n",
      "================================================================================\n",
      "   📊 SUPERVISED vs TRUE SSL 실험 결과 (최적화 버전)\n",
      "================================================================================\n",
      "Config                              Method       Mode         Classifier   Orig Acc   Trans Acc   Drop       Retention \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Supervised_Linear                   supervised   supervised   Linear       0.9294     0.8515      0.0779  91.62%\n",
      "Supervised_Hyperbolic               supervised   supervised   Hyperbolic   0.8731     0.8078      0.0653  92.52%\n",
      "SSL_LinearEval_Linear               ssl          linear_eval  Linear       0.1666     0.1666      0.0000  100.00%\n",
      "SSL_LinearEval_Hyperbolic           ssl          linear_eval  Hyperbolic   0.1785     0.1790      -0.0005  100.30%\n",
      "SSL_FineTune_Linear                 ssl          finetune     Linear       0.3946     0.3565      0.0381  90.34%\n",
      "SSL_FineTune_Hyperbolic             ssl          finetune     Hyperbolic   0.6634     0.6121      0.0512  92.28%\n",
      "\n",
      "================================================================================\n",
      "📊 레벨별 Retention 분석\n",
      "================================================================================\n",
      "Config                              Overall      Moderate     Strong      \n",
      "--------------------------------------------------------------------------------\n",
      "Supervised_Linear                    91.62%       94.91%        89.42%\n",
      "Supervised_Hyperbolic                92.52%       96.11%        90.13%\n",
      "SSL_LinearEval_Linear               100.00%      100.00%       100.00%\n",
      "SSL_LinearEval_Hyperbolic           100.30%       99.71%       100.70%\n",
      "SSL_FineTune_Linear                  90.34%       92.78%        88.71%\n",
      "SSL_FineTune_Hyperbolic              92.28%       97.77%        88.61%\n",
      "\n",
      "================================================================================\n",
      "📊 상세 비교 분석\n",
      "================================================================================\n",
      "\n",
      "🏆 최종 성능 랭킹 (Overall Retention 기준)\n",
      "--------------------------------------------------------------------------------\n",
      "   1. SSL_LinearEval_Hyperbolic           (ssl-linear_eval     ) Retention: 100.30% (Mod: 99.71% | Str: 100.70%)\n",
      "   2. SSL_LinearEval_Linear               (ssl-linear_eval     ) Retention: 100.00% (Mod: 100.00% | Str: 100.00%)\n",
      "   3. Supervised_Hyperbolic               (supervised          ) Retention: 92.52% (Mod: 96.11% | Str: 90.13%)\n",
      "   4. SSL_FineTune_Hyperbolic             (ssl-finetune        ) Retention: 92.28% (Mod: 97.77% | Str: 88.61%)\n",
      "   5. Supervised_Linear                   (supervised          ) Retention: 91.62% (Mod: 94.91% | Str: 89.42%)\n",
      "   6. SSL_FineTune_Linear                 (ssl-finetune        ) Retention: 90.34% (Mod: 92.78% | Str: 88.71%)\n",
      "\n",
      "================================================================================\n",
      "🎯 결론\n",
      "================================================================================\n",
      "   - 최고 성능: SSL_LinearEval_Hyperbolic (Retention: 100.30%)\n",
      "   - Supervised baseline: 92.52% (Mod: 96.11% | Str: 90.13%)\n",
      "   - SSL best: 100.30% (Mod: 99.71% | Str: 100.70%)\n",
      "   - Performance gap: 7.78pp\n",
      "\n",
      "   ✨ 최적화 개선사항:\n",
      "   - 정규화 버그 수정 ✅\n",
      "   - Cosine + Warmup 스케줄러 ✅\n",
      "   - EMA (Exponential Moving Average) ✅\n",
      "   - 백본/헤드 분리 학습률 (Fine-tune) ✅\n",
      "   - 전이-유사 일관성 손실 (Tail-Head Stitch) ✅\n",
      "   - 증강 강도 최적화 (warp: 0.10, cutout: 0.10) ✅\n",
      "   - 학습 가능한 Hyperbolic c ✅\n",
      "   - 2레벨 전이 시나리오 (Moderate/Strong) ✅\n",
      "\n",
      "✅ Results saved to:\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\\supervised_vs_ssl_results_optimized.json\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\\visualization_data_optimized.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================== Main Entry Point ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"   🧪 UCI-HAR Comprehensive Comparison (✨ OPTIMIZED)\")\n",
    "    print(\"   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\")\n",
    "    print(\"   Architecture: ResNet + Transformer Encoder\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   📋 실험 설계:\")\n",
    "    print(\"   1. 동일한 백본 (ResNet + Transformer)\")\n",
    "    print(\"   2. 2레벨 전이 데이터셋 (Moderate/Strong)\")\n",
    "    print(\"   3. 6가지 설정 비교:\")\n",
    "    print(\"      ├─ Supervised × (Linear, Hyperbolic)\")\n",
    "    print(\"      ├─ SSL Linear Eval × (Linear, Hyperbolic)\")\n",
    "    print(\"      └─ SSL Fine-tune × (Linear, Hyperbolic)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   ⚙️  Supervised 설정:\")\n",
    "    print(f\"   - Epochs: {config.finetune_epochs}\")\n",
    "    print(f\"   - Batch size: {config.finetune_batch_size}\")\n",
    "    print(f\"   - Learning rate: {config.finetune_lr}\")\n",
    "    print(f\"   - Warmup: {config.finetune_warmup_epochs} epochs\")\n",
    "    print(f\"   - EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"   - Training: End-to-end with labels + consistency loss\")\n",
    "    print(f\"\\n   ⚙️  SSL 설정:\")\n",
    "    print(f\"   - Stage 1 (Pretrain): {config.pretrain_epochs} epochs, batch={config.pretrain_batch_size}, lr={config.pretrain_lr}\")\n",
    "    print(f\"     → Contrastive learning only (NO LABELS)\")\n",
    "    print(f\"     → Label-independent augmentation\")\n",
    "    print(f\"     → Warmup: {config.pretrain_warmup_epochs} epochs\")\n",
    "    print(f\"     → EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Stage 2 (Eval/FT): {config.finetune_epochs} epochs, batch={config.finetune_batch_size}, lr={config.finetune_lr}\")\n",
    "    print(f\"     → Linear Eval: Freeze backbone\")\n",
    "    print(f\"     → Fine-tune: Train all (backbone LR × {config.finetune_backbone_lr_ratio})\")\n",
    "    print(f\"     → Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"\\n   🔧 Augmentations (SSL - Optimized):\")\n",
    "    print(f\"   - Jitter (scale={config.aug_jitter_scale})\")\n",
    "    print(f\"   - Scaling (range={config.aug_scale_range})\")\n",
    "    print(f\"   - Channel Drop (prob={config.aug_channel_drop_prob})\")\n",
    "    print(f\"   - Time Warp (prob={config.aug_time_warp_prob}) ✅ 0.3→0.10\")\n",
    "    print(f\"   - Cutout (prob={config.aug_cutout_prob}, ratio={config.aug_cutout_ratio}) ✅ 0.2→0.10\")\n",
    "    print(\"   - ALL label-independent!\")\n",
    "    print(f\"\\n   🏗️  Architecture:\")\n",
    "    print(f\"   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads={config.n_heads}, layers={config.n_layers})\")\n",
    "    print(f\"   - d_model: {config.d_model}, dropout: {config.dropout}\")\n",
    "    print(f\"   - Classifier: Linear vs Hyperbolic (c_init={config.hyperbolic_c_init}, learnable ✅)\")\n",
    "    print(f\"   - Projection dim (SSL): {config.projection_dim}\")\n",
    "    print(f\"\\n   🔬 SSL Contrastive Learning:\")\n",
    "    print(f\"   - Loss: NT-Xent (InfoNCE)\")\n",
    "    print(f\"   - Temperature: {config.temperature}\")\n",
    "    print(f\"   - Negative samples: 2*batch_size - 2\")\n",
    "    print(f\"\\n   ✨ 최적화 개선사항:\")\n",
    "    print(\"   - 정규화 버그 수정 (전이 테스트셋)\")\n",
    "    print(\"   - Cosine Annealing + Warmup\")\n",
    "    print(\"   - EMA (Exponential Moving Average)\")\n",
    "    print(\"   - 백본/헤드 분리 학습률 (Fine-tune)\")\n",
    "    print(\"   - 전이-유사 일관성 손실 (Tail-Head Stitch)\")\n",
    "    print(\"   - 증강 강도 최적화 (경계 정보 보존)\")\n",
    "    print(\"   - 학습 가능한 Hyperbolic c\")\n",
    "    print(\"   - 2레벨 전이 시나리오 (Moderate/Strong)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    run_full_comparison(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee20d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
