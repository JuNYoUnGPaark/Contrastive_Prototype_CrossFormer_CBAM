{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WNDGeHJt_igc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"실험 설정\"\"\"\n",
        "    mode: str = \"self_supervised_transition\"\n",
        "    data_dir: str = \"/content/drive/MyDrive/Colab Notebooks/UCI-HAR/data\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/Colab Notebooks/UCI-HAR/ssl_transition\"\n",
        "\n",
        "    # 학습 파라미터\n",
        "    epochs: int = 30\n",
        "    batch_size: int = 128\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Augmentation 파라미터\n",
        "    train_augment_prob: float = 0.4  # 전이 생성 확률 증가\n",
        "    train_augment_mix: float = 0.35\n",
        "\n",
        "    # 모델 파라미터\n",
        "    d_model: int = 128\n",
        "    use_hyperbolic: bool = False\n",
        "    hyperbolic_c: float = 1.0\n",
        "\n",
        "    # SSL 파라미터\n",
        "    ssl_weight: float = 0.3  # 전이 탐지 손실 가중치\n",
        "    consistency_weight: float = 0.2  # 시간적 일관성 손실 가중치\n",
        "    transition_threshold: float = 0.5  # 전이 탐지 임계값\n",
        "\n",
        "    # 시스템 파라미터\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2"
      ],
      "metadata": {
        "id": "d68uwfnj_vf4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
        "RAW_CHANNELS = [\n",
        "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
        "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
        "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}"
      ],
      "metadata": {
        "id": "9Iw2wJcV_wB6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
        "\n",
        "    X_list = [\n",
        "        np.loadtxt(os.path.join(inertial_path, p + split + \".\" + e))[..., None]\n",
        "        for p, e in RAW_CHANNELS\n",
        "    ]\n",
        "\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "\n",
        "    # ⭐️ 중요: UCI-HAR 라벨은 1~6이므로, 0~5로 변환\n",
        "    y = y - 1\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "Mo9CchAP_0sO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = load_split_raw(root, split)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
        "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "\n",
        "        if preloaded_data is None:\n",
        "             self.X = (self.X - self.mean) / self.std\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "f7iAXYZm_1EA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transition_batch(x: torch.Tensor, y: torch.Tensor, mix_ratio: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    전이 구간을 생성하고 ground truth 레이블 반환\n",
        "\n",
        "    Returns:\n",
        "        x_aug: 전이가 포함된 데이터\n",
        "        transition_labels: 각 timestep별 전이 여부 (B, T)\n",
        "    \"\"\"\n",
        "    B, C, T = x.shape\n",
        "    device = x.device\n",
        "\n",
        "    x_aug = x.clone()\n",
        "    transition_labels = torch.zeros(B, T, dtype=torch.float32, device=device)\n",
        "    mix_pts = int(T * mix_ratio)\n",
        "\n",
        "    for i in range(B):\n",
        "        if random.random() < 0.5:  # 50% 확률로 전이 생성\n",
        "            other_class_indices = (y != y[i]).nonzero(as_tuple=True)[0]\n",
        "            if len(other_class_indices) > 0:\n",
        "                j = other_class_indices[random.randint(0, len(other_class_indices)-1)]\n",
        "                x_aug[i, :, -mix_pts:] = x[j, :, :mix_pts].clone()\n",
        "                transition_labels[i, -mix_pts:] = 1.0  # 전이 구간 표시\n",
        "\n",
        "    return x_aug, transition_labels"
      ],
      "metadata": {
        "id": "8p7Y6gpK_3Ew"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([\n",
        "            nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1))\n",
        "            for k in branches\n",
        "        ])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_pre = self.pre(x)\n",
        "        x_branches = [b(x_pre) for b in self.branches]\n",
        "        return self.post(torch.cat(x_branches, dim=1))"
      ],
      "metadata": {
        "id": "FawZF1KA_7RZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, n_features=9, d_model=128, num_layers=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stride = 2\n",
        "\n",
        "        self.cnn_extractor = nn.Sequential(\n",
        "            nn.Conv1d(n_features, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_extractor(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # 마지막 은닉 상태(hn) 대신, 모든 시간 단계의 출력(lstm_out)을 받습니다.\n",
        "        lstm_out, _ = self.lstm(x) # lstm_out shape: (B, T_down, d_model)\n",
        "\n",
        "        # Head가 기대하는 (B, d_model, T_down) 형태로 축을 다시 바꿔줍니다.\n",
        "        fmap = lstm_out.permute(0, 2, 1)\n",
        "\n",
        "        return fmap"
      ],
      "metadata": {
        "id": "2W7fp4Vb_8cl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitionDetectionHead(nn.Module):\n",
        "    \"\"\"전이 구간 탐지를 위한 Self-Supervised Head\"\"\"\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvBNAct(d_model, d_model // 2, 3)\n",
        "        self.conv2 = nn.Conv1d(d_model // 2, 1, 1)  # Binary classification per timestep\n",
        "\n",
        "    def forward(self, fmap):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fmap: (B, d_model, T_down)\n",
        "        Returns:\n",
        "            transition_logits: (B, T_down)\n",
        "        \"\"\"\n",
        "        h = self.conv1(fmap)\n",
        "        logits = self.conv2(h).squeeze(1)  # (B, T_down)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rrR7HVjr_804"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    \"\"\"행동 분류를 위한 Main Head\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, fmap):\n",
        "        pooled = self.gap(fmap).squeeze(-1)\n",
        "        logits = self.fc(pooled)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "7z4kfCR1_-v0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperbolicProjection(nn.Module):\n",
        "    def __init__(self, c=1.0):\n",
        "        super().__init__()\n",
        "        self.c = c\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
        "        max_norm = (1.0 / math.sqrt(self.c)) - 1e-4\n",
        "        scale = torch.clamp(norm, max=max_norm) / norm\n",
        "        return x * scale"
      ],
      "metadata": {
        "id": "BiRsMr7QABs1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperbolicClassificationHead(nn.Module):\n",
        "    \"\"\"쌍곡 공간 기반 분류 Head\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int, c: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.c = c\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.pre_proj = nn.Linear(d_model, d_model)\n",
        "        self.hyperbolic_proj = HyperbolicProjection(c=c)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, fmap):\n",
        "        pooled = self.gap(fmap).squeeze(-1)\n",
        "        h = self.pre_proj(pooled)\n",
        "        h_hyp = self.hyperbolic_proj(h)\n",
        "        logits = self.fc(h_hyp)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "XlM3fM-UADCf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSLHARModel(nn.Module):\n",
        "    \"\"\"Self-Supervised Learning HAR Model\"\"\"\n",
        "    def __init__(self, d_model=128, num_classes=6, use_hyperbolic=False, hyperbolic_c=1.0):\n",
        "        super().__init__()\n",
        "        self.backbone = CNN_LSTM(d_model=d_model)\n",
        "\n",
        "        # Main task: 행동 분류\n",
        "        if use_hyperbolic:\n",
        "            self.classification_head = HyperbolicClassificationHead(d_model, num_classes, c=hyperbolic_c)\n",
        "        else:\n",
        "            self.classification_head = ClassificationHead(d_model, num_classes)\n",
        "\n",
        "        # Pretext task: 전이 탐지\n",
        "        self.transition_head = TransitionDetectionHead(d_model)\n",
        "\n",
        "        self.stride = self.backbone.stride\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, C, T)\n",
        "            return_features: True면 intermediate features 반환\n",
        "        Returns:\n",
        "            class_logits: (B, num_classes)\n",
        "            transition_logits: (B, T_down)\n",
        "            features: (B, d_model, T_down) if return_features\n",
        "        \"\"\"\n",
        "        fmap = self.backbone(x)  # (B, d_model, T_down)\n",
        "\n",
        "        class_logits = self.classification_head(fmap)\n",
        "        transition_logits = self.transition_head(fmap)\n",
        "\n",
        "        if return_features:\n",
        "            return class_logits, transition_logits, fmap\n",
        "        return class_logits, transition_logits"
      ],
      "metadata": {
        "id": "3dvU8SD6AFFJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_consistency_loss(features: torch.Tensor, transition_probs: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    시간적 일관성 손실: 비전이 구간에서 feature가 일관되도록 유도\n",
        "\n",
        "    Args:\n",
        "        features: (B, d_model, T)\n",
        "        transition_probs: (B, T) - sigmoid된 전이 확률\n",
        "    \"\"\"\n",
        "    B, D, T = features.shape\n",
        "\n",
        "    # 비전이 구간 가중치 (전이 확률이 낮을수록 높은 가중치)\n",
        "    consistency_weights = 1.0 - transition_probs.unsqueeze(1)  # (B, 1, T)\n",
        "\n",
        "    # 인접 timestep 간 차이\n",
        "    diff = features[:, :, 1:] - features[:, :, :-1]  # (B, D, T-1)\n",
        "    diff_norm = torch.norm(diff, dim=1)  # (B, T-1)\n",
        "\n",
        "    # 비전이 구간에서만 일관성 요구\n",
        "    weighted_diff = diff_norm * consistency_weights[:, 0, :-1]\n",
        "\n",
        "    return weighted_diff.mean()"
      ],
      "metadata": {
        "id": "rgrHR6dkAFcG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transition_detection_loss(pred_logits: torch.Tensor, target_labels: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    전이 탐지 손실 (Binary Cross Entropy)\n",
        "\n",
        "    Args:\n",
        "        pred_logits: (B, T_down)\n",
        "        target_labels: (B, T_orig) - downsampling 필요\n",
        "    \"\"\"\n",
        "    B, T_down = pred_logits.shape\n",
        "    T_orig = target_labels.shape[1]\n",
        "    stride = T_orig // T_down\n",
        "\n",
        "    # Target을 downsampling (max pooling: 하나라도 전이면 전이로 간주)\n",
        "    target_down = F.max_pool1d(\n",
        "        target_labels.unsqueeze(1),\n",
        "        kernel_size=stride,\n",
        "        stride=stride\n",
        "    ).squeeze(1)  # (B, T_down)\n",
        "\n",
        "    # BCE Loss with logits\n",
        "    loss = F.binary_cross_entropy_with_logits(pred_logits, target_down)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "3V3d0NfYAIL2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_ssl(model: SSLHARModel, loader: DataLoader, opt: torch.optim.Optimizer, cfg: Config):\n",
        "    model.train()\n",
        "    total_loss, total_cls_loss, total_ssl_loss, total_cons_loss = 0.0, 0.0, 0.0, 0.0\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "\n",
        "        # Self-supervised augmentation\n",
        "        if random.random() < cfg.train_augment_prob:\n",
        "            x_aug, transition_gt = create_transition_batch(x, y, cfg.train_augment_mix)\n",
        "        else:\n",
        "            x_aug = x\n",
        "            transition_gt = torch.zeros(x.shape[0], x.shape[2], device=cfg.device)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass\n",
        "        class_logits, transition_logits, features = model(x_aug, return_features=True)\n",
        "\n",
        "        # Loss 계산\n",
        "        # 1. Main task: 행동 분류\n",
        "        cls_loss = F.cross_entropy(class_logits, y, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "        # 2. Pretext task: 전이 탐지\n",
        "        ssl_loss = transition_detection_loss(transition_logits, transition_gt)\n",
        "\n",
        "        # 3. Temporal consistency\n",
        "        transition_probs = torch.sigmoid(transition_logits)\n",
        "        cons_loss = temporal_consistency_loss(features, transition_probs)\n",
        "\n",
        "        # Total loss\n",
        "        loss = cls_loss + cfg.ssl_weight * ssl_loss + cfg.consistency_weight * cons_loss\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        # Metrics\n",
        "        pred = class_logits.argmax(dim=-1)\n",
        "        total_correct += (pred == y).sum().item()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_cls_loss += cls_loss.item() * y.size(0)\n",
        "        total_ssl_loss += ssl_loss.item() * y.size(0)\n",
        "        total_cons_loss += cons_loss.item() * y.size(0)\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": total_loss / total_samples,\n",
        "        \"cls_loss\": total_cls_loss / total_samples,\n",
        "        \"ssl_loss\": total_ssl_loss / total_samples,\n",
        "        \"cons_loss\": total_cons_loss / total_samples,\n",
        "        \"acc\": total_correct / total_samples\n",
        "    }"
      ],
      "metadata": {
        "id": "W6YPw9ZBAMtN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_ssl(model: SSLHARModel, loader: DataLoader, cfg: Config):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    transition_probs_all = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        class_logits, transition_logits = model(x)\n",
        "\n",
        "        y_pred.append(class_logits.argmax(dim=-1).cpu().numpy())\n",
        "        y_true.append(y.cpu().numpy())\n",
        "        transition_probs_all.append(torch.sigmoid(transition_logits).cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    # 전이 탐지 능력 측정 (평균 confidence)\n",
        "    avg_transition_conf = np.concatenate(transition_probs_all).mean()\n",
        "\n",
        "    return acc, f1, avg_transition_conf"
      ],
      "metadata": {
        "id": "3DbG5B64ARVD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transitional_test_set_with_gt(\n",
        "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
        ") -> Tuple[UCIHARInertial, np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    전이 테스트셋 생성 + ground truth 반환\n",
        "\n",
        "    Returns:\n",
        "        mod_dataset: 전이가 포함된 데이터셋\n",
        "        transition_gt: (N, T) 전이 구간 ground truth\n",
        "        info: 통계 정보\n",
        "    \"\"\"\n",
        "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
        "    N, C, T = X.shape\n",
        "    transition_gt = np.zeros((N, T), dtype=np.float32)\n",
        "\n",
        "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
        "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
        "    mix_pts = int(T * mix)\n",
        "\n",
        "    # A → B 전이\n",
        "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
        "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
        "    for t, s in zip(targets_A, sources_B):\n",
        "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
        "        transition_gt[t, -mix_pts:] = 1.0\n",
        "\n",
        "    # B → A 전이\n",
        "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
        "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
        "    for t, s in zip(targets_B, sources_A):\n",
        "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
        "        transition_gt[t, -mix_pts:] = 1.0\n",
        "\n",
        "    mod_dataset = UCIHARInertial(\n",
        "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
        "        preloaded_data=(X, y)\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'modified_samples': len(targets_A) + len(targets_B),\n",
        "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
        "    }\n",
        "    return mod_dataset, transition_gt, info"
      ],
      "metadata": {
        "id": "Kq3pB-0lATpM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_transition_detection(\n",
        "    model: SSLHARModel, dataset: UCIHARInertial, transition_gt: np.ndarray, cfg: Config\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"전이 탐지 성능 평가\"\"\"\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x = x.to(cfg.device)\n",
        "        _, transition_logits = model(x)\n",
        "        transition_probs = torch.sigmoid(transition_logits).cpu().numpy()  # (B, T_down)\n",
        "\n",
        "        # Ground truth downsampling\n",
        "        batch_start = i * cfg.batch_size\n",
        "        batch_end = min(batch_start + cfg.batch_size, len(dataset))\n",
        "        gt_batch = transition_gt[batch_start:batch_end]\n",
        "\n",
        "        T_down = transition_probs.shape[1]\n",
        "        stride = gt_batch.shape[1] // T_down\n",
        "        gt_down = gt_batch.reshape(gt_batch.shape[0], T_down, stride).max(axis=2)\n",
        "\n",
        "        all_probs.append(transition_probs)\n",
        "        all_labels.append(gt_down)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs).flatten()\n",
        "    all_labels = np.concatenate(all_labels).flatten()\n",
        "\n",
        "    # AUC-ROC 계산\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "    # Binary accuracy (threshold=0.5)\n",
        "    pred_binary = (all_probs > cfg.transition_threshold).astype(int)\n",
        "    acc = accuracy_score(all_labels, pred_binary)\n",
        "\n",
        "    # Precision, Recall\n",
        "    tp = ((pred_binary == 1) & (all_labels == 1)).sum()\n",
        "    fp = ((pred_binary == 1) & (all_labels == 0)).sum()\n",
        "    fn = ((pred_binary == 0) & (all_labels == 1)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'auc': auc,\n",
        "        'acc': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "kr65olrtAV9_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return super(NumpyEncoder, self).default(obj)"
      ],
      "metadata": {
        "id": "VLc66TQAAXru"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ssl_experiment(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # 데이터셋 로드\n",
        "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
        "\n",
        "    # 전이 테스트 시나리오\n",
        "    scenarios = [\n",
        "        (\"STANDING\", \"SITTING\", 0.60, 0.50),\n",
        "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
        "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
        "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    🔬 TRANSITIONAL TEST SETS 생성 (Ground Truth 포함)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    transition_test_data = []\n",
        "    for clsA, clsB, p, mix in scenarios:\n",
        "        test_set_mod, transition_gt, info = create_transitional_test_set_with_gt(\n",
        "            test_set_orig, clsA, clsB, p=p, mix=mix\n",
        "        )\n",
        "        transition_test_data.append((test_set_mod, transition_gt, info))\n",
        "        print(f\"   - {clsA}↔{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}개 샘플 변형\")\n",
        "\n",
        "    # 비교 모델\n",
        "    ssl_configs = [\n",
        "        {\"name\": \"SSL_Euclidean\", \"use_hyperbolic\": False},\n",
        "        {\"name\": \"SSL_Hyperbolic\", \"use_hyperbolic\": True},\n",
        "    ]\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    for ssl_cfg in ssl_configs:\n",
        "        print(f\"\\n{'='*70}\\n   실험: {ssl_cfg['name']}\\n{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        cfg.use_hyperbolic = ssl_cfg[\"use_hyperbolic\"]\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "        test_loader_orig = DataLoader(test_set_orig, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "        model = SSLHARModel(\n",
        "            d_model=cfg.d_model, use_hyperbolic=cfg.use_hyperbolic, hyperbolic_c=cfg.hyperbolic_c\n",
        "        ).to(cfg.device)\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        print(f\"Training {ssl_cfg['name']} for {cfg.epochs} epochs...\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            stats = train_one_epoch_ssl(model, train_loader, opt, cfg)\n",
        "            te_acc, te_f1, te_trans_conf = evaluate_ssl(model, test_loader_orig, cfg)\n",
        "\n",
        "            if te_acc > best_acc:\n",
        "                best_acc = te_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if epoch % 5 == 0 or epoch == 1:\n",
        "                print(f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} (Cls:{stats['cls_loss']:.4f} \"\n",
        "                      f\"SSL:{stats['ssl_loss']:.4f} Cons:{stats['cons_loss']:.4f}) A:{stats['acc']:.4f} | \"\n",
        "                      f\"Test A:{te_acc:.4f} F1:{te_f1:.4f} TransConf:{te_trans_conf:.4f}\")\n",
        "\n",
        "        if best_wts:\n",
        "            model.load_state_dict(best_wts)\n",
        "        print(f\"✅ Best Original Test Acc: {best_acc:.4f}\")\n",
        "\n",
        "        # 원본 테스트셋 평가\n",
        "        acc_orig, f1_orig, _ = evaluate_ssl(model, test_loader_orig, cfg)\n",
        "\n",
        "        # 전이 테스트셋 평가\n",
        "        transition_results = []\n",
        "        print(\"\\n   🔍 전이 테스트셋 평가 (Self-Supervised Detection)...\")\n",
        "\n",
        "        for i, (test_set_mod, transition_gt, info) in enumerate(transition_test_data):\n",
        "            test_loader_mod = DataLoader(test_set_mod, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "            # 분류 성능\n",
        "            acc_trans, f1_trans, _ = evaluate_ssl(model, test_loader_mod, cfg)\n",
        "            drop = acc_orig - acc_trans\n",
        "\n",
        "            # 전이 탐지 성능\n",
        "            detection_metrics = evaluate_transition_detection(model, test_set_mod, transition_gt, cfg)\n",
        "\n",
        "            transition_results.append({\n",
        "                'scenario': i+1,\n",
        "                'class_acc': acc_trans,\n",
        "                'class_drop': drop,\n",
        "                'detection_auc': detection_metrics['auc'],\n",
        "                'detection_acc': detection_metrics['acc'],\n",
        "                'detection_f1': detection_metrics['f1'],\n",
        "                'detection_precision': detection_metrics['precision'],\n",
        "                'detection_recall': detection_metrics['recall']\n",
        "            })\n",
        "\n",
        "            print(f\"     - Scenario {i+1}: ClassAcc={acc_trans:.4f} (Drop={drop:.4f}) | \"\n",
        "                  f\"DetAUC={detection_metrics['auc']:.4f} DetF1={detection_metrics['f1']:.4f}\")\n",
        "\n",
        "        # 평균 성능 계산\n",
        "        avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
        "        avg_drop = acc_orig - avg_trans_acc\n",
        "        retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
        "        avg_det_auc = np.mean([r['detection_auc'] for r in transition_results])\n",
        "        avg_det_f1 = np.mean([r['detection_f1'] for r in transition_results])\n",
        "\n",
        "        results_table.append({\n",
        "            \"config\": ssl_cfg[\"name\"],\n",
        "            \"orig_acc\": acc_orig,\n",
        "            \"orig_f1\": f1_orig,\n",
        "            \"avg_trans_acc\": avg_trans_acc,\n",
        "            \"avg_drop\": avg_drop,\n",
        "            \"retention\": retention,\n",
        "            \"avg_detection_auc\": avg_det_auc,\n",
        "            \"avg_detection_f1\": avg_det_f1,\n",
        "            \"transition_results\": transition_results\n",
        "        })\n",
        "\n",
        "    # 최종 결과 요약\n",
        "    print(f\"\\n{'='*70}\\n   SELF-SUPERVISED LEARNING 실험 결과\\n{'='*70}\")\n",
        "    print(f\"{'Config':<20} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<12} {'Det AUC':<10} {'Det F1':<10}\")\n",
        "    print(\"-\" * 95)\n",
        "\n",
        "    for r in results_table:\n",
        "        print(f\"{r['config']:<20} {r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
        "              f\"{r['avg_drop']:.4f}   {r['retention']:.2f}%      {r['avg_detection_auc']:.4f}    {r['avg_detection_f1']:.4f}\")\n",
        "\n",
        "    # 상세 비교 분석\n",
        "    if len(results_table) > 1:\n",
        "        euclidean, hyperbolic = results_table[0], results_table[1]\n",
        "\n",
        "        print(\"\\n\" + \"-\"*95)\n",
        "        print(\"📊 상세 비교 분석\")\n",
        "        print(\"-\" * 95)\n",
        "\n",
        "        # 분류 성능 비교\n",
        "        drop_improve = ((euclidean['avg_drop'] - hyperbolic['avg_drop']) / euclidean['avg_drop'] * 100) \\\n",
        "                       if euclidean['avg_drop'] > 0 else 0\n",
        "        print(f\"\\n1️⃣  분류 강건성:\")\n",
        "        print(f\"   - Drop 감소율: {drop_improve:+.2f}%\")\n",
        "        print(f\"   - Retention 이득: {hyperbolic['retention'] - euclidean['retention']:+.2f}pp\")\n",
        "\n",
        "        # 전이 탐지 성능 비교\n",
        "        det_auc_improve = (hyperbolic['avg_detection_auc'] - euclidean['avg_detection_auc']) * 100\n",
        "        det_f1_improve = (hyperbolic['avg_detection_f1'] - euclidean['avg_detection_f1']) * 100\n",
        "        print(f\"\\n2️⃣  전이 탐지 성능:\")\n",
        "        print(f\"   - AUC 개선: {det_auc_improve:+.2f}pp\")\n",
        "        print(f\"   - F1 개선: {det_f1_improve:+.2f}pp\")\n",
        "\n",
        "        # 시나리오별 상세 비교\n",
        "        print(f\"\\n3️⃣  시나리오별 전이 탐지 성능:\")\n",
        "        print(f\"   {'Scenario':<12} {'Euclidean AUC':<16} {'Hyperbolic AUC':<16} {'개선':<10}\")\n",
        "        print(\"   \" + \"-\"*60)\n",
        "        for i in range(len(euclidean['transition_results'])):\n",
        "            euc_auc = euclidean['transition_results'][i]['detection_auc']\n",
        "            hyp_auc = hyperbolic['transition_results'][i]['detection_auc']\n",
        "            improve = (hyp_auc - euc_auc) * 100\n",
        "            print(f\"   Scenario {i+1:<4} {euc_auc:.4f}           {hyp_auc:.4f}           {improve:+.2f}pp\")\n",
        "\n",
        "        # 해석\n",
        "        print(\"\\n\" + \"-\"*95)\n",
        "        print(\"🧠 결과 해석:\")\n",
        "        print(\"-\" * 95)\n",
        "\n",
        "        if drop_improve > 20 and det_auc_improve > 5:\n",
        "            print(\"✅ 쌍곡 공간이 Self-Supervised Learning에서 탁월한 성능 발휘:\")\n",
        "            print(\"   1. 전이 구간을 더 정확하게 탐지 (높은 AUC)\")\n",
        "            print(\"   2. 탐지된 정보를 활용해 분류 강건성 크게 향상\")\n",
        "            print(\"   3. 쌍곡 기하학이 implicit + explicit robustness 모두 제공\")\n",
        "        elif drop_improve > 10 or det_auc_improve > 3:\n",
        "            print(\"⚠️  쌍곡 공간이 적당한 개선 제공:\")\n",
        "            print(\"   - 전이 탐지 또는 분류 강건성 중 하나에서 우수\")\n",
        "            print(\"   - SSL 파라미터 튜닝으로 추가 개선 가능\")\n",
        "        else:\n",
        "            print(\"❌ 제한적 개선:\")\n",
        "            print(\"   - Euclidean 공간도 SSL로 충분한 강건성 확보\")\n",
        "            print(\"   - 쌍곡 공간의 이점이 SSL 설정에서 두드러지지 않음\")\n",
        "\n",
        "        print(\"\\n💡 핵심 인사이트:\")\n",
        "        if hyperbolic['avg_detection_auc'] > 0.75:\n",
        "            print(f\"   ✓ 쌍곡 모델의 전이 탐지 AUC {hyperbolic['avg_detection_auc']:.3f}로 신뢰할만한 수준\")\n",
        "            print(\"   ✓ Self-Supervised Head가 전이 패턴을 효과적으로 학습\")\n",
        "        if hyperbolic['retention'] > 85:\n",
        "            print(f\"   ✓ Retention {hyperbolic['retention']:.1f}%로 실용적 수준의 강건성 달성\")\n",
        "\n",
        "        print(\"\\n📈 추가 실험 제안:\")\n",
        "        print(\"   1. SSL 가중치(ssl_weight, consistency_weight) 최적화\")\n",
        "        print(\"   2. 더 다양한 전이 패턴(점진적 전이, 다중 전이 등) 테스트\")\n",
        "        print(\"   3. Attention 메커니즘 추가로 전이 구간 집중 학습\")\n",
        "        print(\"   4. 준지도 학습(Semi-supervised) 설정 실험\")\n",
        "\n",
        "    # 결과 저장\n",
        "    with open(os.path.join(cfg.save_dir, \"ssl_results.json\"), \"w\") as f:\n",
        "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
        "    print(f\"\\n✅ Results saved to '{cfg.save_dir}/ssl_results.json'\")\n",
        "\n",
        "    # 시각화를 위한 추가 정보 저장\n",
        "    visualization_data = {\n",
        "        'configs': [r['config'] for r in results_table],\n",
        "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
        "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
        "        'detection_auc': [r['avg_detection_auc'] for r in results_table],\n",
        "        'detection_f1': [r['avg_detection_f1'] for r in results_table],\n",
        "        'retention': [r['retention'] for r in results_table]\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(cfg.save_dir, \"ssl_visualization_data.json\"), \"w\") as f:\n",
        "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)"
      ],
      "metadata": {
        "id": "zlBUe-diAYLb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"   🧪 UCI-HAR Self-Supervised Learning with Transition Detection\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"   핵심 아이디어:\")\n",
        "    print(\"   1. 전이 구간을 mask로 표시하지 않음\")\n",
        "    print(\"   2. Multipath CNN이 자기주도학습으로 전이 구간 탐지\")\n",
        "    print(\"   3. Main Task: 행동 분류 / Pretext Task: 전이 탐지\")\n",
        "    print(\"   4. Temporal Consistency Loss로 암시적 강건성 학습\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}, LR: {config.lr}\")\n",
        "    print(f\"SSL Weight: {config.ssl_weight}, Consistency Weight: {config.consistency_weight}\")\n",
        "    print(f\"Transition Augment Prob: {config.train_augment_prob}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    run_ssl_experiment(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnElPggfAgtm",
        "outputId": "5a3734a7-3147-41aa-a99b-6d3d70bc78d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "   🧪 UCI-HAR Self-Supervised Learning with Transition Detection\n",
            "======================================================================\n",
            "   핵심 아이디어:\n",
            "   1. 전이 구간을 mask로 표시하지 않음\n",
            "   2. Multipath CNN이 자기주도학습으로 전이 구간 탐지\n",
            "   3. Main Task: 행동 분류 / Pretext Task: 전이 탐지\n",
            "   4. Temporal Consistency Loss로 암시적 강건성 학습\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 30, LR: 0.0003\n",
            "SSL Weight: 0.3, Consistency Weight: 0.2\n",
            "Transition Augment Prob: 0.4\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "    🔬 TRANSITIONAL TEST SETS 생성 (Ground Truth 포함)\n",
            "======================================================================\n",
            "   - STANDING↔SITTING (p=0.60, mix=0.50): 613개 샘플 변형\n",
            "   - STANDING↔SITTING (p=0.70, mix=0.55): 715개 샘플 변형\n",
            "   - WALKING↔WALKING_UPSTAIRS (p=0.65, mix=0.52): 628개 샘플 변형\n",
            "   - SITTING↔LAYING (p=0.75, mix=0.58): 770개 샘플 변형\n",
            "\n",
            "======================================================================\n",
            "   실험: SSL_Euclidean\n",
            "======================================================================\n",
            "Training SSL_Euclidean for 30 epochs...\n",
            "[01/30] Train L:0.9212 (Cls:0.6860 SSL:0.5784 Cons:0.3082) A:0.8723 | Test A:0.8860 F1:0.8861 TransConf:0.3167\n",
            "[05/30] Train L:0.4172 (Cls:0.3394 SSL:0.1730 Cons:0.1296) A:0.9539 | Test A:0.9209 F1:0.9218 TransConf:0.1326\n",
            "[10/30] Train L:0.3694 (Cls:0.3273 SSL:0.0742 Cons:0.0995) A:0.9574 | Test A:0.9260 F1:0.9266 TransConf:0.0486\n",
            "[15/30] Train L:0.3478 (Cls:0.3166 SSL:0.0454 Cons:0.0878) A:0.9607 | Test A:0.9294 F1:0.9305 TransConf:0.0297\n",
            "[20/30] Train L:0.3385 (Cls:0.3143 SSL:0.0293 Cons:0.0768) A:0.9626 | Test A:0.9203 F1:0.9213 TransConf:0.0278\n",
            "[25/30] Train L:0.3321 (Cls:0.3123 SSL:0.0215 Cons:0.0667) A:0.9631 | Test A:0.9237 F1:0.9245 TransConf:0.0233\n",
            "[30/30] Train L:0.3324 (Cls:0.3122 SSL:0.0236 Cons:0.0657) A:0.9630 | Test A:0.9192 F1:0.9203 TransConf:0.0226\n",
            "✅ Best Original Test Acc: 0.9328\n",
            "\n",
            "   🔍 전이 테스트셋 평가 (Self-Supervised Detection)...\n",
            "     - Scenario 1: ClassAcc=0.9237 (Drop=0.0092) | DetAUC=0.9437 DetF1=0.7311\n",
            "     - Scenario 2: ClassAcc=0.9172 (Drop=0.0156) | DetAUC=0.9456 DetF1=0.7479\n",
            "     - Scenario 3: ClassAcc=0.9223 (Drop=0.0105) | DetAUC=0.9166 DetF1=0.7566\n",
            "     - Scenario 4: ClassAcc=0.9294 (Drop=0.0034) | DetAUC=0.9926 DetF1=0.9266\n",
            "\n",
            "======================================================================\n",
            "   실험: SSL_Hyperbolic\n",
            "======================================================================\n",
            "Training SSL_Hyperbolic for 30 epochs...\n",
            "[01/30] Train L:1.6396 (Cls:1.4110 SSL:0.6204 Cons:0.2121) A:0.8239 | Test A:0.8907 F1:0.8912 TransConf:0.3604\n",
            "[05/30] Train L:0.9255 (Cls:0.8604 SSL:0.1737 Cons:0.0650) A:0.9538 | Test A:0.9172 F1:0.9183 TransConf:0.1118\n",
            "[10/30] Train L:0.5828 (Cls:0.5507 SSL:0.0755 Cons:0.0469) A:0.9601 | Test A:0.9182 F1:0.9193 TransConf:0.0571\n",
            "[15/30] Train L:0.4424 (Cls:0.4224 SSL:0.0412 Cons:0.0382) A:0.9603 | Test A:0.9209 F1:0.9215 TransConf:0.0484\n",
            "[20/30] Train L:0.3802 (Cls:0.3642 SSL:0.0310 Cons:0.0338) A:0.9638 | Test A:0.9260 F1:0.9271 TransConf:0.0289\n",
            "[25/30] Train L:0.3440 (Cls:0.3306 SSL:0.0252 Cons:0.0290) A:0.9669 | Test A:0.9304 F1:0.9313 TransConf:0.0118\n",
            "[30/30] Train L:0.3320 (Cls:0.3205 SSL:0.0199 Cons:0.0276) A:0.9672 | Test A:0.9226 F1:0.9229 TransConf:0.0116\n",
            "✅ Best Original Test Acc: 0.9342\n",
            "\n",
            "   🔍 전이 테스트셋 평가 (Self-Supervised Detection)...\n",
            "     - Scenario 1: ClassAcc=0.9267 (Drop=0.0075) | DetAUC=0.9047 DetF1=0.7139\n",
            "     - Scenario 2: ClassAcc=0.9206 (Drop=0.0136) | DetAUC=0.8950 DetF1=0.7413\n",
            "     - Scenario 3: ClassAcc=0.9189 (Drop=0.0153) | DetAUC=0.9802 DetF1=0.8031\n",
            "     - Scenario 4: ClassAcc=0.9332 (Drop=0.0010) | DetAUC=0.9957 DetF1=0.9531\n",
            "\n",
            "======================================================================\n",
            "   SELF-SUPERVISED LEARNING 실험 결과\n",
            "======================================================================\n",
            "Config               Orig Acc   Trans Acc   Drop       Retention    Det AUC    Det F1    \n",
            "-----------------------------------------------------------------------------------------------\n",
            "SSL_Euclidean        0.9328     0.9231      0.0097   98.96%      0.9496    0.7906\n",
            "SSL_Hyperbolic       0.9342     0.9248      0.0093   99.00%      0.9439    0.8029\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "📊 상세 비교 분석\n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "1️⃣  분류 강건성:\n",
            "   - Drop 감소율: +3.51%\n",
            "   - Retention 이득: +0.04pp\n",
            "\n",
            "2️⃣  전이 탐지 성능:\n",
            "   - AUC 개선: -0.58pp\n",
            "   - F1 개선: +1.23pp\n",
            "\n",
            "3️⃣  시나리오별 전이 탐지 성능:\n",
            "   Scenario     Euclidean AUC    Hyperbolic AUC   개선        \n",
            "   ------------------------------------------------------------\n",
            "   Scenario 1    0.9437           0.9047           -3.90pp\n",
            "   Scenario 2    0.9456           0.8950           -5.07pp\n",
            "   Scenario 3    0.9166           0.9802           +6.36pp\n",
            "   Scenario 4    0.9926           0.9957           +0.30pp\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "🧠 결과 해석:\n",
            "-----------------------------------------------------------------------------------------------\n",
            "❌ 제한적 개선:\n",
            "   - Euclidean 공간도 SSL로 충분한 강건성 확보\n",
            "   - 쌍곡 공간의 이점이 SSL 설정에서 두드러지지 않음\n",
            "\n",
            "💡 핵심 인사이트:\n",
            "   ✓ 쌍곡 모델의 전이 탐지 AUC 0.944로 신뢰할만한 수준\n",
            "   ✓ Self-Supervised Head가 전이 패턴을 효과적으로 학습\n",
            "   ✓ Retention 99.0%로 실용적 수준의 강건성 달성\n",
            "\n",
            "📈 추가 실험 제안:\n",
            "   1. SSL 가중치(ssl_weight, consistency_weight) 최적화\n",
            "   2. 더 다양한 전이 패턴(점진적 전이, 다중 전이 등) 테스트\n",
            "   3. Attention 메커니즘 추가로 전이 구간 집중 학습\n",
            "   4. 준지도 학습(Semi-supervised) 설정 실험\n",
            "\n",
            "✅ Results saved to '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/ssl_transition/ssl_results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLfqNlqnC_pQ"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}