{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad7451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Random Seed ========================\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f78ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Ïã§Ìóò ÏÑ§Ï†ï\"\"\"\n",
    "    data_dir: str = \"C://Users/park9/HAR/SSL_HAR/data\"\n",
    "    save_dir: str = \"C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\"\n",
    "\n",
    "    # SSL Pretrain ÌååÎùºÎØ∏ÌÑ∞\n",
    "    pretrain_epochs: int = 100\n",
    "    pretrain_batch_size: int = 512  # ‚úÖ InfoNCE ÏÑ±Îä• Ìñ•ÏÉÅ\n",
    "    pretrain_lr: float = 1e-3\n",
    "    pretrain_warmup_epochs: int = 0  # ‚úÖ Warmup\n",
    "\n",
    "    # Supervised / Linear Eval / Fine-tune ÌååÎùºÎØ∏ÌÑ∞\n",
    "    finetune_epochs: int = 50\n",
    "    finetune_batch_size: int = 128\n",
    "    finetune_lr: float = 3e-4\n",
    "    finetune_warmup_epochs: int = 0  # ‚úÖ Warmup\n",
    "    finetune_backbone_lr_ratio: float = 0.1  # ‚úÖ Î∞±Î≥∏ LR ÎπÑÏú®\n",
    "\n",
    "    # Í≥µÌÜµ ÌååÎùºÎØ∏ÌÑ∞\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.05\n",
    "    use_ema: bool = False  # ‚úÖ EMA ÏÇ¨Ïö©\n",
    "    ema_decay: float = 0.9995  # ‚úÖ EMA decay\n",
    "    consistency_weight: float = 0.0  # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ Í∞ÄÏ§ëÏπò\n",
    "\n",
    "    # Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    hyperbolic_c_init: float = 1.0  # ‚úÖ Ï¥àÍ∏∞Í∞í (ÌïôÏäµ Í∞ÄÎä•)\n",
    "\n",
    "    # SSL ÌååÎùºÎØ∏ÌÑ∞\n",
    "    temperature: float = 0.07\n",
    "    projection_dim: int = 128\n",
    "\n",
    "    # Augmentation ÌååÎùºÎØ∏ÌÑ∞ (‚úÖ Ï†ÑÏù¥ Í∞ïÍ±¥ÏÑ± ÏµúÏ†ÅÌôî)\n",
    "    aug_jitter_scale: float = 0.05\n",
    "    aug_scale_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    aug_channel_drop_prob: float = 0.2\n",
    "    aug_time_warp_prob: float = 0.10  # ‚úÖ 0.3 ‚Üí 0.10\n",
    "    aug_cutout_prob: float = 0.20     # ‚úÖ 0.3 ‚Üí 0.20\n",
    "    aug_cutout_ratio: float = 0.10    # ‚úÖ 0.2 ‚Üí 0.10\n",
    "\n",
    "    # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c29fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Dataset Configuration ========================\n",
    "\n",
    "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
    "RAW_CHANNELS = [\n",
    "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
    "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
    "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
    "]\n",
    "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
    "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
    "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
    "\n",
    "def load_split_raw(root: str, split: str):\n",
    "    assert split in (\"train\", \"test\")\n",
    "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
    "\n",
    "    # Ï°¥Ïû¨ ÌôïÏù∏(Î¨∏Ï†ú ÏûàÏúºÎ©¥ Î∞îÎ°ú Ïñ¥ÎîîÍ∞Ä ÏóÜÎäîÏßÄ ÏïåÎ†§Ï§å)\n",
    "    if not os.path.isdir(inertial_path):\n",
    "        raise FileNotFoundError(f\"[Missing dir] {inertial_path}\")\n",
    "\n",
    "    X_list = []\n",
    "    for p, e in RAW_CHANNELS:\n",
    "        fpath = os.path.join(inertial_path, f\"{p}{split}.{e}\")  # ex) body_acc_x_train.txt\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise FileNotFoundError(f\"[Missing file] {fpath}\")\n",
    "        # URL Ïò§Ïù∏ Î∞©ÏßÄ: ÌååÏùº Ìï∏Îì§Î°ú Ï†ÑÎã¨\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            arr = np.loadtxt(f)       # (N, 128)\n",
    "        X_list.append(arr[..., None]) # (N, 128, 1)\n",
    "\n",
    "    # Ï±ÑÎÑê Î™®Îëê Í∞ôÏùÄ ÏÉòÌîå ÏàòÏù∏ÏßÄ Ï≤¥ÌÅ¨(ÏïàÏ†ÑÏû•Ïπò)\n",
    "    n_samples = {x.shape[0] for x in X_list}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(f\"Ï±ÑÎÑêÎ≥Ñ ÏÉòÌîå Ïàò Î∂àÏùºÏπò: {n_samples}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)  # (N, 9, 128)\n",
    "\n",
    "    y_path = os.path.join(root, split, f\"y_{split}.txt\")\n",
    "    if not os.path.isfile(y_path):\n",
    "        raise FileNotFoundError(f\"[Missing file] {y_path}\")\n",
    "    with open(y_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        y = np.loadtxt(f).astype(int) - 1  # 0-based\n",
    "\n",
    "    print(f\"[OK] {split}: X{X.shape}, y{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "class UCIHARInertial(Dataset):\n",
    "    \"\"\"UCI-HAR Dataset (‚úÖ Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï)\"\"\"\n",
    "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
    "                 preloaded_data: Tuple[np.ndarray, np.ndarray] = None):\n",
    "        super().__init__()\n",
    "        if preloaded_data is not None:\n",
    "            X, y = preloaded_data\n",
    "        else:\n",
    "            X, y = load_split_raw(root, split)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
    "\n",
    "        # mean/std ÏÑ∏ÌåÖ\n",
    "        if mean is not None and std is not None:\n",
    "            self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
    "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
    "\n",
    "        # ‚úÖ preloaded_data Ïó¨Î∂ÄÏôÄ Î¨¥Í¥ÄÌïòÍ≤å Ìï≠ÏÉÅ train ÌÜµÍ≥ÑÎ°ú Ï†ïÍ∑úÌôî\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c846a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Label-Independent Augmentations ========================\n",
    "\n",
    "def random_jitter(x: torch.Tensor, scale: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = torch.randn_like(x) * scale\n",
    "    return x + noise\n",
    "\n",
    "def random_scaling(x: torch.Tensor, scale_range: Tuple[float, float] = (0.8, 1.2)) -> torch.Tensor:\n",
    "    \"\"\"Random scaling of amplitudes\"\"\"\n",
    "    scale = torch.empty(x.size(0), x.size(1), 1, device=x.device).uniform_(*scale_range)\n",
    "    return x * scale\n",
    "\n",
    "def random_channel_drop(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly drop channels (set to zero)\"\"\"\n",
    "    B, C, T = x.shape\n",
    "    mask = torch.rand(B, C, 1, device=x.device) > drop_prob\n",
    "    return x * mask.float()\n",
    "\n",
    "def random_time_warp(x: torch.Tensor, warp_prob: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Simple time warping by random interpolation\"\"\"\n",
    "    if random.random() > warp_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    warp_factor = random.uniform(0.8, 1.2)\n",
    "    new_T = int(T * warp_factor)\n",
    "\n",
    "    x_warped = F.interpolate(x, size=new_T, mode='linear', align_corners=False)\n",
    "\n",
    "    if new_T > T:\n",
    "        start = random.randint(0, new_T - T)\n",
    "        x_warped = x_warped[:, :, start:start+T]\n",
    "    elif new_T < T:\n",
    "        pad_total = T - new_T\n",
    "        pad_left = random.randint(0, pad_total)\n",
    "        pad_right = pad_total - pad_left\n",
    "        x_warped = F.pad(x_warped, (pad_left, pad_right), mode='replicate')\n",
    "\n",
    "    return x_warped\n",
    "\n",
    "def random_cutout(x: torch.Tensor, cutout_prob: float = 0.20, cutout_ratio: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Randomly mask out a temporal segment\"\"\"\n",
    "    if random.random() > cutout_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    cutout_len = int(T * cutout_ratio)\n",
    "    start = random.randint(0, T - cutout_len)\n",
    "    x_cut = x.clone()\n",
    "    x_cut[:, :, start:start+cutout_len] = 0\n",
    "    return x_cut\n",
    "\n",
    "def augment_time_series(x: torch.Tensor, cfg: Config) -> torch.Tensor:\n",
    "    \"\"\"Label-independent augmentation pipeline\"\"\"\n",
    "    x_aug = x.clone()\n",
    "    x_aug = random_jitter(x_aug, scale=cfg.aug_jitter_scale)\n",
    "    x_aug = random_scaling(x_aug, scale_range=cfg.aug_scale_range)\n",
    "    x_aug = random_channel_drop(x_aug, drop_prob=cfg.aug_channel_drop_prob)\n",
    "    x_aug = random_time_warp(x_aug, warp_prob=cfg.aug_time_warp_prob)\n",
    "    x_aug = random_cutout(x_aug, cutout_prob=cfg.aug_cutout_prob, cutout_ratio=cfg.aug_cutout_ratio)\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Tail-Head Stitch (Ï†ÑÏù¥ Ïú†ÏÇ¨ ÌòºÌï©) ========================\n",
    "\n",
    "def tail_head_stitch(x_a: torch.Tensor, x_b: torch.Tensor, mix: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tail-Head Stitch: x_aÏùò ÏïûÎ∂ÄÎ∂Ñ + x_bÏùò Îí∑Î∂ÄÎ∂Ñ\n",
    "    Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖãÍ≥º Ïú†ÏÇ¨Ìïú Í≤ΩÍ≥Ñ ÌòºÌï© ÏÉùÏÑ±\n",
    "    \"\"\"\n",
    "    B, C, T = x_a.shape\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    x_mix = x_a.clone()\n",
    "    x_mix[:, :, -mix_pts:] = x_b[:, :, :mix_pts]\n",
    "\n",
    "    return x_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2a6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== ResNet Building Blocks ========================\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"1D Residual Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_blocks=[2, 2, 2]):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(d_model, num_blocks[2], stride=2)\n",
    "\n",
    "        self.stride = 16\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResBlock1D(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock1D(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6ca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transformer Encoder ========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Module\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a59e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Backbone ========================\n",
    "\n",
    "class ResNetTransformerBackbone(nn.Module):\n",
    "    \"\"\"ResNet + Transformer Encoder Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet1D(in_channels=in_channels, d_model=d_model)\n",
    "        self.transformer = TransformerEncoder(d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.stride = self.resnet.stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = self.resnet(x)\n",
    "        fmap = self.transformer(fmap)\n",
    "        return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b872092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Projection Head ========================\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"MLP projection head for contrastive learning\"\"\"\n",
    "    def __init__(self, d_model, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06574fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Classification Heads ========================\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Linear Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "class HyperbolicProjection(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Projection (‚úÖ ÌïôÏäµ Í∞ÄÎä•Ìïú c)\"\"\"\n",
    "    def __init__(self, c_init=1.0):\n",
    "        super().__init__()\n",
    "        self.c = nn.Parameter(torch.tensor(c_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ‚úÖ Feature norm clipping\n",
    "        x = torch.clamp(x, -5.0, 5.0)\n",
    "\n",
    "        c = torch.clamp(self.c, min=0.1, max=10.0)\n",
    "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
    "        max_norm = (1.0 / math.sqrt(c)) - 1e-4\n",
    "        scale = torch.clamp(norm, max=max_norm) / norm\n",
    "        return x * scale\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int, c_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pre_proj = nn.Linear(d_model, d_model)\n",
    "        self.hyperbolic_proj = HyperbolicProjection(c_init=c_init)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        h = self.pre_proj(pooled)\n",
    "        h_hyp = self.hyperbolic_proj(h)\n",
    "        logits = self.fc(h_hyp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249fa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SSL Model ========================\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    \"\"\"Self-Supervised Learning Model\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetTransformerBackbone(\n",
    "            in_channels=9, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(d_model, projection_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns normalized projection\"\"\"\n",
    "        fmap = self.backbone(x)\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        z = self.projection_head(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42036e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== EMA Utility ========================\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model parameters\"\"\"\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: param.clone().detach()\n",
    "                       for name, param in model.named_parameters() if param.requires_grad}\n",
    "        self.backup = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        \"\"\"Update EMA parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                self.shadow[name].mul_(self.decay).add_(param.data, alpha=1 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        \"\"\"Apply EMA parameters to model\"\"\"\n",
    "        self.backup = {name: param.data.clone() for name, param in model.named_parameters() if param.requires_grad}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        \"\"\"Restore original parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f86599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Learning Rate Scheduler ========================\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Cosine annealing with linear warmup\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48ebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Contrastive Loss ========================\n",
    "\n",
    "def contrastive_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"NT-Xent Loss (InfoNCE)\"\"\"\n",
    "    B = z1.shape[0]\n",
    "    device = z1.device\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
    "\n",
    "    labels = torch.arange(B, device=device)\n",
    "    labels = torch.cat([labels + B, labels], dim=0)\n",
    "\n",
    "    mask = torch.eye(2 * B, device=device, dtype=torch.bool)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc243dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Consistency Loss ========================\n",
    "\n",
    "def consistency_loss(model: nn.Module, head: nn.Module, x_a: torch.Tensor, x_b: torch.Tensor,\n",
    "                     mix: float = 0.5, device: str = \"cuda\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§\n",
    "    Tail-Head Stitch ÌõÑ ÏòàÏ∏° Î∂ÑÌè¨Ïùò KL divergence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        fmap_a = model.backbone(x_a) if hasattr(model, 'backbone') else model(x_a)\n",
    "        fmap_b = model.backbone(x_b) if hasattr(model, 'backbone') else model(x_b)\n",
    "\n",
    "        logits_a = head(fmap_a)\n",
    "        logits_b = head(fmap_b)\n",
    "\n",
    "        p_a = F.softmax(logits_a, dim=-1)\n",
    "        p_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    x_mix = tail_head_stitch(x_a, x_b, mix=mix)\n",
    "\n",
    "    fmap_mix = model.backbone(x_mix) if hasattr(model, 'backbone') else model(x_mix)\n",
    "    logits_mix = head(fmap_mix)\n",
    "    p_mix = F.log_softmax(logits_mix, dim=-1)\n",
    "\n",
    "    # Soft target: 0.5*p_a + 0.5*p_b\n",
    "    p_target = 0.5 * p_a + 0.5 * p_b\n",
    "\n",
    "    loss = F.kl_div(p_mix, p_target, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Training Functions ========================\n",
    "\n",
    "def pretrain_one_epoch(model: SSLModel, loader: DataLoader, opt: torch.optim.Optimizer,\n",
    "                       scheduler, ema: Optional[EMA], cfg: Config):\n",
    "    \"\"\"SSL Pretrain: No labels, only contrastive loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        x1 = augment_time_series(x, cfg)\n",
    "        x2 = augment_time_series(x, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = contrastive_loss(z1, z2, temperature=cfg.temperature)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "    return {\"ssl_loss\": total_loss / total_samples}\n",
    "\n",
    "def linear_eval_epoch(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                      opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Linear evaluation: Freeze backbone, train head only\"\"\"\n",
    "    if train:\n",
    "        backbone.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fmap = backbone(x)\n",
    "\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "def finetune_epoch(model: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   opt: torch.optim.Optimizer, scheduler, ema: Optional[EMA],\n",
    "                   cfg: Config, train: bool = True):\n",
    "    \"\"\"Fine-tuning: Train both backbone and head (‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ Ï∂îÍ∞Ä)\"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "        head.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "    total_correct, total_samples = 0, 0\n",
    "\n",
    "    data_iter = iter(loader)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        # Forward pass\n",
    "        fmap = model.backbone(x) if hasattr(model, 'backbone') else model(x)\n",
    "        logits = head(fmap)\n",
    "        loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (ÌïôÏäµ ÏãúÏóêÎßå)\n",
    "        loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "        if train and cfg.consistency_weight > 0:\n",
    "            try:\n",
    "                x_b, _ = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(loader)\n",
    "                x_b, _ = next(data_iter)\n",
    "\n",
    "            x_b = x_b.to(cfg.device)\n",
    "            if x_b.size(0) == x.size(0):\n",
    "                loss_cons = consistency_loss(model, head, x, x_b, mix=0.5, device=cfg.device)\n",
    "\n",
    "        loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(head.parameters()), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_ce_loss += loss_ce.item() * y.size(0)\n",
    "        total_cons_loss += loss_cons.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"ce_loss\": total_ce_loss / total_samples,\n",
    "        \"cons_loss\": total_cons_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   ema: Optional[EMA], cfg: Config, use_ema: bool = False):\n",
    "    \"\"\"Evaluate model (‚úÖ EMA ÏßÄÏõê)\"\"\"\n",
    "    if use_ema and ema is not None:\n",
    "        ema.apply_shadow(backbone)\n",
    "\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        fmap = backbone(x)\n",
    "        logits = head(fmap)\n",
    "        y_pred.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    if use_ema and ema is not None:\n",
    "        ema.restore(backbone)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8275be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transitional Test Set ========================\n",
    "\n",
    "def create_transitional_test_set(\n",
    "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
    ") -> Tuple[UCIHARInertial, dict]:\n",
    "    \"\"\"Create transitional test set (‚úÖ Ï†ïÍ∑úÌôî Î≥¥Ïû•)\"\"\"\n",
    "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
    "    N, C, T = X.shape\n",
    "\n",
    "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
    "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
    "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
    "    for t, s in zip(targets_A, sources_B):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
    "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
    "    for t, s in zip(targets_B, sources_A):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    # (ÏàòÏ†ï) Ïù¥Ï§ë Ï†ïÍ∑úÌôî Î∞©ÏßÄ: ÏõêÎ≥∏ Ïä§ÏºÄÏùºÎ°ú Î≥µÏõê\n",
    "    X_restored = (X * orig_dataset.std) + orig_dataset.mean\n",
    "\n",
    "    # ‚úÖ train ÌÜµÍ≥ÑÎ°ú Ï†ïÍ∑úÌôîÌïòÎèÑÎ°ù mean/std Ï†ÑÎã¨\n",
    "    mod_dataset = UCIHARInertial(\n",
    "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
    "        preloaded_data=(X_restored, y)\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        'class_A': class_A,\n",
    "        'class_B': class_B,\n",
    "        'p': p,\n",
    "        'mix': mix,\n",
    "        'modified_samples': len(targets_A) + len(targets_B),\n",
    "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
    "    }\n",
    "    return mod_dataset, info\n",
    "\n",
    "# ======================== JSON Encoder ========================\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON Encoder for NumPy types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# ======================== Main Experiment Function ========================\n",
    "\n",
    "def run_full_comparison(cfg: Config):\n",
    "    \"\"\"Run complete supervised vs SSL comparison\"\"\"\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\nüì¶ Loading UCI-HAR Dataset...\")\n",
    "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
    "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
    "    print(f\"   - Train samples: {len(train_set)}\")\n",
    "    print(f\"   - Test samples: {len(test_set_orig)}\")\n",
    "\n",
    "    # Create transitional test sets (‚úÖ 2Î†àÎ≤® Í∞ïÎèÑ)\n",
    "    scenarios = [\n",
    "        # Level 1: Moderate (Ï§ëÍ∞Ñ Í∞ïÎèÑ)\n",
    "        (\"STANDING\", \"SITTING\", 0.50, 0.40),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.55, 0.42),\n",
    "        # Level 2: Strong (Í∞ïÌïú Í∞ïÎèÑ)\n",
    "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
    "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"    üî¨ TRANSITIONAL TEST SETS ÏÉùÏÑ± (2Î†àÎ≤® Í∞ïÎèÑ)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    transition_test_data = []\n",
    "    for i, (clsA, clsB, p, mix) in enumerate(scenarios):\n",
    "        test_set_mod, info = create_transitional_test_set(test_set_orig, clsA, clsB, p=p, mix=mix)\n",
    "        transition_test_data.append((test_set_mod, info))\n",
    "        level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "        print(f\"   - [{level}] {clsA}‚Üî{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}Í∞ú ÏÉòÌîå Î≥ÄÌòï\")\n",
    "\n",
    "    # Experiment configurations\n",
    "    experiment_configs = [\n",
    "        {\"name\": \"Supervised_Linear\", \"method\": \"supervised\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"Supervised_Hyperbolic\", \"method\": \"supervised\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_LinearEval_Linear\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_LinearEval_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_FineTune_Linear\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_FineTune_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": True},\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    for exp_cfg in experiment_configs:\n",
    "        print(f\"\\n{'='*80}\\n   Ïã§Ìóò: {exp_cfg['name']}\\n{'='*80}\")\n",
    "\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "        if exp_cfg['method'] == 'supervised':\n",
    "            # Supervised Learning\n",
    "            print(\"\\nüìö Supervised Learning (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            backbone = ResNetTransformerBackbone(\n",
    "                in_channels=9, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "                n_layers=cfg.n_layers, dropout=cfg.dropout\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            params = list(backbone.parameters()) + list(head.parameters())\n",
    "            opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ‚úÖ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "            warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "            # scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "            scheduler = None\n",
    "\n",
    "            # ‚úÖ EMA\n",
    "            ema = EMA(backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            def train_supervised_epoch(backbone, head, loader, opt, scheduler, ema, cfg, train=True):\n",
    "                if train:\n",
    "                    backbone.train()\n",
    "                    head.train()\n",
    "                else:\n",
    "                    backbone.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "                total_correct, total_samples = 0, 0\n",
    "\n",
    "                data_iter = iter(loader)\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "                    fmap = backbone(x)\n",
    "                    logits = head(fmap)\n",
    "                    loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "                    # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§\n",
    "                    loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "                    if train and cfg.consistency_weight > 0:\n",
    "                        try:\n",
    "                            x_b, _ = next(data_iter)\n",
    "                        except StopIteration:\n",
    "                            data_iter = iter(loader)\n",
    "                            x_b, _ = next(data_iter)\n",
    "\n",
    "                        x_b = x_b.to(cfg.device)\n",
    "                        if x_b.size(0) == x.size(0):\n",
    "                            loss_cons = consistency_loss(\n",
    "                                type('Model', (), {'backbone': backbone})(),\n",
    "                                head, x, x_b, mix=0.5, device=cfg.device\n",
    "                            )\n",
    "\n",
    "                    loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "                    if train:\n",
    "                        opt.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "                        opt.step()\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                        if ema is not None:\n",
    "                            ema.update(backbone)\n",
    "\n",
    "                    pred = logits.argmax(dim=-1)\n",
    "                    total_correct += (pred == y).sum().item()\n",
    "                    total_loss += loss.item() * y.size(0)\n",
    "                    total_ce_loss += loss_ce.item() * y.size(0)\n",
    "                    total_cons_loss += loss_cons.item() * y.size(0)\n",
    "                    total_samples += y.size(0)\n",
    "\n",
    "                return {\n",
    "                    \"loss\": total_loss / total_samples,\n",
    "                    \"ce_loss\": total_ce_loss / total_samples,\n",
    "                    \"cons_loss\": total_cons_loss / total_samples,\n",
    "                    \"acc\": total_correct / total_samples\n",
    "                }\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"Training for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_supervised_epoch(backbone, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "                te_acc, te_f1 = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'backbone': copy.deepcopy(backbone.state_dict()),\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if ema is not None:\n",
    "                        best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Supervised {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                backbone.load_state_dict(best_wts['backbone'])\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if ema is not None and 'ema_shadow' in best_wts:\n",
    "                    ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"‚úÖ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(backbone, head, test_loader_mod, ema, cfg, use_ema=cfg.use_ema)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}‚Üî{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ‚úÖ Î†àÎ≤®Î≥Ñ Î∂ÑÏÑù\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"supervised\",\n",
    "                \"mode\": \"supervised\",\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "        else:  # SSL\n",
    "            # Stage 1: SSL Pretrain\n",
    "            print(\"\\nüìö Stage 1: Self-Supervised Pretraining (No Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            ssl_model = SSLModel(\n",
    "                d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers,\n",
    "                dropout=cfg.dropout, projection_dim=cfg.projection_dim\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            pretrain_loader = DataLoader(train_set, cfg.pretrain_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            ssl_opt = torch.optim.AdamW(ssl_model.parameters(), lr=cfg.pretrain_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ‚úÖ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.pretrain_epochs * len(pretrain_loader)\n",
    "            warmup_steps = cfg.pretrain_warmup_epochs * len(pretrain_loader)\n",
    "            # ssl_scheduler = get_cosine_schedule_with_warmup(ssl_opt, warmup_steps, total_steps)\n",
    "            ssl_scheduler = None\n",
    "\n",
    "            # ‚úÖ EMA\n",
    "            ssl_ema = EMA(ssl_model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            print(f\"Pretraining for {cfg.pretrain_epochs} epochs...\")\n",
    "            for epoch in range(1, cfg.pretrain_epochs + 1):\n",
    "                stats = pretrain_one_epoch(ssl_model, pretrain_loader, ssl_opt, ssl_scheduler, ssl_ema, cfg)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Pretrain {epoch:03d}/{cfg.pretrain_epochs}] SSL Loss: {stats['ssl_loss']:.4f}\")\n",
    "\n",
    "            # ‚úÖ EMA Ï†ÅÏö©\n",
    "            if ssl_ema is not None:\n",
    "                ssl_ema.apply_shadow(ssl_model)\n",
    "\n",
    "            print(\"‚úÖ Pretraining Complete!\")\n",
    "\n",
    "            # Stage 2: Linear Eval or Fine-tune\n",
    "            print(f\"\\nüìö Stage 2: {exp_cfg['mode'].upper()} (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            if exp_cfg['mode'] == 'linear_eval':\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                opt = torch.optim.AdamW(head.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                ema = None  # Linear evalÏóêÏÑúÎäî Î∞±Î≥∏Ïù¥ frozenÏù¥ÎØÄÎ°ú EMA Î∂àÌïÑÏöî\n",
    "\n",
    "                train_fn = lambda: linear_eval_epoch(ssl_model.backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            else:  # finetune\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                # ‚úÖ Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•†\n",
    "                backbone_params = list(ssl_model.backbone.parameters())\n",
    "                head_params = list(head.parameters())\n",
    "\n",
    "                opt = torch.optim.AdamW([\n",
    "                    {'params': backbone_params, 'lr': cfg.finetune_lr * cfg.finetune_backbone_lr_ratio},\n",
    "                    {'params': head_params, 'lr': cfg.finetune_lr},\n",
    "                ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                # ‚úÖ EMA (Î∞±Î≥∏Îßå)\n",
    "                ema = EMA(ssl_model.backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "                train_fn = lambda: finetune_epoch(ssl_model, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"{exp_cfg['mode']} for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_fn()\n",
    "                te_acc, te_f1 = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if exp_cfg['mode'] == 'finetune':\n",
    "                        best_wts['backbone'] = copy.deepcopy(ssl_model.backbone.state_dict())\n",
    "                        if ema is not None:\n",
    "                            best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    if 'cons_loss' in stats:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if exp_cfg['mode'] == 'finetune':\n",
    "                    ssl_model.backbone.load_state_dict(best_wts['backbone'])\n",
    "                    if ema is not None and 'ema_shadow' in best_wts:\n",
    "                        ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"‚úÖ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(ssl_model.backbone, head, test_loader_mod, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}‚Üî{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ‚úÖ Î†àÎ≤®Î≥Ñ Î∂ÑÏÑù\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"ssl\",\n",
    "                \"mode\": exp_cfg['mode'],\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"   üìä SUPERVISED vs TRUE SSL Ïã§Ìóò Í≤∞Í≥º (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Method':<12} {'Mode':<12} {'Classifier':<12} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<10}\")\n",
    "    print(\"-\" * 115)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['method']:<12} {r['mode']:<12} {r['classifier']:<12} \"\n",
    "              f\"{r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
    "              f\"{r['avg_drop']:.4f}  {r['retention']:.2f}%\")\n",
    "\n",
    "    # ‚úÖ Î†àÎ≤®Î≥Ñ Í≤∞Í≥º Ï∂îÍ∞Ä\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Î†àÎ≤®Î≥Ñ Retention Î∂ÑÏÑù\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Overall':<12} {'Moderate':<12} {'Strong':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['retention']:>6.2f}%      {r['retention_moderate']:>6.2f}%       {r['retention_strong']:>6.2f}%\")\n",
    "\n",
    "    # Detailed analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä ÏÉÅÏÑ∏ ÎπÑÍµê Î∂ÑÏÑù\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Final ranking\n",
    "    sorted_results = sorted(results_table, key=lambda x: x['retention'], reverse=True)\n",
    "    print(\"\\nüèÜ ÏµúÏ¢Ö ÏÑ±Îä• Îû≠ÌÇπ (Overall Retention Í∏∞Ï§Ä)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for rank, r in enumerate(sorted_results, 1):\n",
    "        method_mode = f\"{r['method']}-{r['mode']}\" if r['method'] == 'ssl' else r['method']\n",
    "        print(f\"   {rank}. {r['config']:<35} ({method_mode:<20}) \"\n",
    "              f\"Retention: {r['retention']:.2f}% (Mod: {r['retention_moderate']:.2f}% | Str: {r['retention_strong']:.2f}%)\")\n",
    "\n",
    "    best_config = sorted_results[0]\n",
    "    best_ssl = max([r for r in results_table if r['method'] == 'ssl'], key=lambda x: x['retention'])\n",
    "    best_sup = max([r for r in results_table if r['method'] == 'supervised'], key=lambda x: x['retention'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Í≤∞Î°†\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - ÏµúÍ≥† ÏÑ±Îä•: {best_config['config']} (Retention: {best_config['retention']:.2f}%)\")\n",
    "    print(f\"   - Supervised baseline: {best_sup['retention']:.2f}% (Mod: {best_sup['retention_moderate']:.2f}% | Str: {best_sup['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - SSL best: {best_ssl['retention']:.2f}% (Mod: {best_ssl['retention_moderate']:.2f}% | Str: {best_ssl['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - Performance gap: {abs(best_ssl['retention'] - best_sup['retention']):.2f}pp\")\n",
    "\n",
    "    # ‚úÖ Í∞úÏÑ† Ìö®Í≥º Î∂ÑÏÑù\n",
    "    print(\"\\n   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\")\n",
    "    print(\"   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï ‚úÖ\")\n",
    "    print(\"   - Cosine + Warmup Ïä§ÏºÄÏ§ÑÎü¨ ‚úÖ\")\n",
    "    print(\"   - EMA (Exponential Moving Average) ‚úÖ\")\n",
    "    print(\"   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune) ‚úÖ\")\n",
    "    print(\"   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch) ‚úÖ\")\n",
    "    print(f\"   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (warp: 0.10, cutout: 0.10) ‚úÖ\")\n",
    "    print(\"   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c ‚úÖ\")\n",
    "    print(\"   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong) ‚úÖ\")\n",
    "\n",
    "    # Save results\n",
    "    save_path = os.path.join(cfg.save_dir, \"supervised_vs_ssl_results_optimized.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    visualization_data = {\n",
    "        'configs': [r['config'] for r in results_table],\n",
    "        'methods': [r['method'] for r in results_table],\n",
    "        'modes': [r['mode'] for r in results_table],\n",
    "        'classifiers': [r['classifier'] for r in results_table],\n",
    "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
    "        'orig_f1': [r['orig_f1'] for r in results_table],\n",
    "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
    "        'trans_f1': [r['avg_trans_f1'] for r in results_table],\n",
    "        'retention': [r['retention'] for r in results_table],\n",
    "        'retention_moderate': [r['retention_moderate'] for r in results_table],\n",
    "        'retention_strong': [r['retention_strong'] for r in results_table],\n",
    "        'avg_drop': [r['avg_drop'] for r in results_table]\n",
    "    }\n",
    "\n",
    "    viz_path = os.path.join(cfg.save_dir, \"visualization_data_optimized.json\")\n",
    "    with open(viz_path, \"w\") as f:\n",
    "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n‚úÖ Results saved to:\")\n",
    "    print(f\"   - {save_path}\")\n",
    "    print(f\"   - {viz_path}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c68009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   üß™ UCI-HAR Comprehensive Comparison (‚ú® OPTIMIZED)\n",
      "   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\n",
      "   Architecture: ResNet + Transformer Encoder\n",
      "================================================================================\n",
      "\n",
      "   üìã Ïã§Ìóò ÏÑ§Í≥Ñ:\n",
      "   1. ÎèôÏùºÌïú Î∞±Î≥∏ (ResNet + Transformer)\n",
      "   2. 2Î†àÎ≤® Ï†ÑÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖã (Moderate/Strong)\n",
      "   3. 6Í∞ÄÏßÄ ÏÑ§Ï†ï ÎπÑÍµê:\n",
      "      ‚îú‚îÄ Supervised √ó (Linear, Hyperbolic)\n",
      "      ‚îú‚îÄ SSL Linear Eval √ó (Linear, Hyperbolic)\n",
      "      ‚îî‚îÄ SSL Fine-tune √ó (Linear, Hyperbolic)\n",
      "================================================================================\n",
      "\n",
      "   ‚öôÔ∏è  Supervised ÏÑ§Ï†ï:\n",
      "   - Epochs: 50\n",
      "   - Batch size: 128\n",
      "   - Learning rate: 0.0003\n",
      "   - Warmup: 0 epochs\n",
      "   - EMA decay: Disabled\n",
      "   - Consistency weight: 0.0\n",
      "   - Training: End-to-end with labels + consistency loss\n",
      "\n",
      "   ‚öôÔ∏è  SSL ÏÑ§Ï†ï:\n",
      "   - Stage 1 (Pretrain): 100 epochs, batch=512, lr=0.001\n",
      "     ‚Üí Contrastive learning only (NO LABELS)\n",
      "     ‚Üí Label-independent augmentation\n",
      "     ‚Üí Warmup: 0 epochs\n",
      "     ‚Üí EMA decay: Disabled\n",
      "   - Stage 2 (Eval/FT): 50 epochs, batch=128, lr=0.0003\n",
      "     ‚Üí Linear Eval: Freeze backbone\n",
      "     ‚Üí Fine-tune: Train all (backbone LR √ó 0.1)\n",
      "     ‚Üí Consistency weight: 0.0\n",
      "\n",
      "   üîß Augmentations (SSL - Optimized):\n",
      "   - Jitter (scale=0.05)\n",
      "   - Scaling (range=(0.8, 1.2))\n",
      "   - Channel Drop (prob=0.2)\n",
      "   - Time Warp (prob=0.1) ‚úÖ 0.3‚Üí0.10\n",
      "   - Cutout (prob=0.2, ratio=0.1) ‚úÖ 0.2‚Üí0.10\n",
      "   - ALL label-independent!\n",
      "\n",
      "   üèóÔ∏è  Architecture:\n",
      "   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads=4, layers=2)\n",
      "   - d_model: 128, dropout: 0.1\n",
      "   - Classifier: Linear vs Hyperbolic (c_init=1.0, learnable ‚úÖ)\n",
      "   - Projection dim (SSL): 128\n",
      "\n",
      "   üî¨ SSL Contrastive Learning:\n",
      "   - Loss: NT-Xent (InfoNCE)\n",
      "   - Temperature: 0.07\n",
      "   - Negative samples: 2*batch_size - 2\n",
      "\n",
      "   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\n",
      "   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï (Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã)\n",
      "   - Cosine Annealing + Warmup\n",
      "   - EMA (Exponential Moving Average)\n",
      "   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune)\n",
      "   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch)\n",
      "   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (Í≤ΩÍ≥Ñ Ï†ïÎ≥¥ Î≥¥Ï°¥)\n",
      "   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c\n",
      "   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üì¶ Loading UCI-HAR Dataset...\n",
      "[OK] train: X(7352, 9, 128), y(7352,)\n",
      "[OK] test: X(2947, 9, 128), y(2947,)\n",
      "   - Train samples: 7352\n",
      "   - Test samples: 2947\n",
      "\n",
      "================================================================================\n",
      "    üî¨ TRANSITIONAL TEST SETS ÏÉùÏÑ± (2Î†àÎ≤® Í∞ïÎèÑ)\n",
      "================================================================================\n",
      "   - [Moderate] STANDING‚ÜîSITTING (p=0.50, mix=0.40): 511Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Moderate] WALKING‚ÜîWALKING_UPSTAIRS (p=0.55, mix=0.42): 531Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] STANDING‚ÜîSITTING (p=0.70, mix=0.55): 715Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] WALKING‚ÜîWALKING_UPSTAIRS (p=0.65, mix=0.52): 628Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] SITTING‚ÜîLAYING (p=0.75, mix=0.58): 770Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: Supervised_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:0.5001 CE:0.5001 Cons:0.0000 A:0.8864 | Test A:0.9260 F1:0.9269\n",
      "[Supervised 10/50] Train L:0.3215 CE:0.3215 Cons:0.0000 A:0.9597 | Test A:0.9328 F1:0.9328\n",
      "[Supervised 20/50] Train L:0.3040 CE:0.3040 Cons:0.0000 A:0.9642 | Test A:0.9284 F1:0.9290\n",
      "[Supervised 30/50] Train L:0.2881 CE:0.2881 Cons:0.0000 A:0.9748 | Test A:0.9413 F1:0.9416\n",
      "[Supervised 40/50] Train L:0.2874 CE:0.2874 Cons:0.0000 A:0.9769 | Test A:0.9525 F1:0.9532\n",
      "[Supervised 50/50] Train L:0.2775 CE:0.2775 Cons:0.0000 A:0.9826 | Test A:0.9454 F1:0.9457\n",
      "‚úÖ Best Test Acc: 0.9545\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.9111 F1=0.9128 (Drop=0.0434)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.9091 F1=0.9085 (Drop=0.0455)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8585 F1=0.8620 (Drop=0.0960)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8388 F1=0.8344 (Drop=0.1157)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.8120 F1=0.8124 (Drop=0.1425)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: Supervised_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.4078 CE:1.4078 Cons:0.0000 A:0.8394 | Test A:0.9209 F1:0.9208\n",
      "[Supervised 10/50] Train L:0.5614 CE:0.5614 Cons:0.0000 A:0.9589 | Test A:0.9002 F1:0.9013\n",
      "[Supervised 20/50] Train L:0.3679 CE:0.3679 Cons:0.0000 A:0.9640 | Test A:0.9291 F1:0.9309\n",
      "[Supervised 30/50] Train L:0.3104 CE:0.3104 Cons:0.0000 A:0.9763 | Test A:0.9253 F1:0.9262\n",
      "[Supervised 40/50] Train L:0.2853 CE:0.2853 Cons:0.0000 A:0.9791 | Test A:0.9403 F1:0.9419\n",
      "[Supervised 50/50] Train L:0.2724 CE:0.2724 Cons:0.0000 A:0.9879 | Test A:0.9505 F1:0.9505\n",
      "‚úÖ Best Test Acc: 0.9505\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.9036 F1=0.9022 (Drop=0.0468)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8907 F1=0.8892 (Drop=0.0597)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8510 F1=0.8466 (Drop=0.0994)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8334 F1=0.8282 (Drop=0.1171)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.8327 F1=0.8129 (Drop=0.1177)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_LinearEval_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.8074 A:0.2393 | Test A:0.4581 F1:0.4450\n",
      "[linear_eval 10/50] Train L:0.5546 A:0.9125 | Test A:0.8955 F1:0.8941\n",
      "[linear_eval 20/50] Train L:0.4666 A:0.9264 | Test A:0.9063 F1:0.9056\n",
      "[linear_eval 30/50] Train L:0.4429 A:0.9342 | Test A:0.9108 F1:0.9101\n",
      "[linear_eval 40/50] Train L:0.4306 A:0.9380 | Test A:0.9121 F1:0.9115\n",
      "[linear_eval 50/50] Train L:0.4221 A:0.9418 | Test A:0.9128 F1:0.9123\n",
      "‚úÖ Best Test Acc: 0.9138\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.8890 F1=0.8891 (Drop=0.0248)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8616 F1=0.8579 (Drop=0.0523)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8293 F1=0.8314 (Drop=0.0845)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8008 F1=0.7923 (Drop=0.1130)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.7760 F1=0.7806 (Drop=0.1378)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_LinearEval_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.6645 A:0.6007 | Test A:0.8392 F1:0.8281\n",
      "[linear_eval 10/50] Train L:0.6622 A:0.9298 | Test A:0.9094 F1:0.9086\n",
      "[linear_eval 20/50] Train L:0.4351 A:0.9411 | Test A:0.9192 F1:0.9186\n",
      "[linear_eval 30/50] Train L:0.3778 A:0.9497 | Test A:0.9189 F1:0.9182\n",
      "[linear_eval 40/50] Train L:0.3584 A:0.9524 | Test A:0.9237 F1:0.9231\n",
      "[linear_eval 50/50] Train L:0.3506 A:0.9559 | Test A:0.9233 F1:0.9227\n",
      "‚úÖ Best Test Acc: 0.9277\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.8999 F1=0.9006 (Drop=0.0278)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8775 F1=0.8744 (Drop=0.0502)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8303 F1=0.8335 (Drop=0.0974)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8059 F1=0.7974 (Drop=0.1218)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.7679 F1=0.7724 (Drop=0.1598)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_FineTune_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.4352 CE:1.4352 Cons:0.0000 A:0.5197 | Test A:0.8514 F1:0.8502\n",
      "[finetune 10/50] Train L:0.3074 CE:0.3074 Cons:0.0000 A:0.9693 | Test A:0.9566 F1:0.9566\n",
      "[finetune 20/50] Train L:0.2734 CE:0.2734 Cons:0.0000 A:0.9861 | Test A:0.9674 F1:0.9674\n",
      "[finetune 30/50] Train L:0.2644 CE:0.2644 Cons:0.0000 A:0.9913 | Test A:0.9701 F1:0.9701\n",
      "[finetune 40/50] Train L:0.2603 CE:0.2603 Cons:0.0000 A:0.9931 | Test A:0.9684 F1:0.9684\n",
      "[finetune 50/50] Train L:0.2585 CE:0.2585 Cons:0.0000 A:0.9932 | Test A:0.9695 F1:0.9695\n",
      "‚úÖ Best Test Acc: 0.9708\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.9213 F1=0.9224 (Drop=0.0495)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.9135 F1=0.9112 (Drop=0.0573)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8381 F1=0.8413 (Drop=0.1327)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8588 F1=0.8518 (Drop=0.1120)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.8178 F1=0.8124 (Drop=0.1530)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_FineTune_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.6141 CE:1.6141 Cons:0.0000 A:0.6625 | Test A:0.8884 F1:0.8839\n",
      "[finetune 10/50] Train L:0.5404 CE:0.5404 Cons:0.0000 A:0.9699 | Test A:0.9535 F1:0.9537\n",
      "[finetune 20/50] Train L:0.3409 CE:0.3409 Cons:0.0000 A:0.9879 | Test A:0.9688 F1:0.9688\n",
      "[finetune 30/50] Train L:0.2995 CE:0.2995 Cons:0.0000 A:0.9890 | Test A:0.9698 F1:0.9699\n",
      "[finetune 40/50] Train L:0.2814 CE:0.2814 Cons:0.0000 A:0.9935 | Test A:0.9701 F1:0.9702\n",
      "[finetune 50/50] Train L:0.2773 CE:0.2773 Cons:0.0000 A:0.9950 | Test A:0.9701 F1:0.9702\n",
      "‚úÖ Best Test Acc: 0.9718\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.9209 F1=0.9217 (Drop=0.0509)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.9063 F1=0.9032 (Drop=0.0655)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8409 F1=0.8426 (Drop=0.1310)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8683 F1=0.8596 (Drop=0.1035)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.7879 F1=0.7935 (Drop=0.1839)\n",
      "\n",
      "================================================================================\n",
      "   üìä SUPERVISED vs TRUE SSL Ïã§Ìóò Í≤∞Í≥º (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)\n",
      "================================================================================\n",
      "Config                              Method       Mode         Classifier   Orig Acc   Trans Acc   Drop       Retention \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Supervised_Linear                   supervised   supervised   Linear       0.9545     0.8659      0.0886  90.71%\n",
      "Supervised_Hyperbolic               supervised   supervised   Hyperbolic   0.9505     0.8623      0.0882  90.72%\n",
      "SSL_LinearEval_Linear               ssl          linear_eval  Linear       0.9138     0.8314      0.0825  90.98%\n",
      "SSL_LinearEval_Hyperbolic           ssl          linear_eval  Hyperbolic   0.9277     0.8363      0.0914  90.15%\n",
      "SSL_FineTune_Linear                 ssl          finetune     Linear       0.9708     0.8699      0.1009  89.61%\n",
      "SSL_FineTune_Hyperbolic             ssl          finetune     Hyperbolic   0.9718     0.8649      0.1070  88.99%\n",
      "\n",
      "================================================================================\n",
      "üìä Î†àÎ≤®Î≥Ñ Retention Î∂ÑÏÑù\n",
      "================================================================================\n",
      "Config                              Overall      Moderate     Strong      \n",
      "--------------------------------------------------------------------------------\n",
      "Supervised_Linear                    90.71%       95.34%        87.63%\n",
      "Supervised_Hyperbolic                90.72%       94.39%        88.28%\n",
      "SSL_LinearEval_Linear                90.98%       95.79%        87.77%\n",
      "SSL_LinearEval_Hyperbolic            90.15%       95.79%        86.38%\n",
      "SSL_FineTune_Linear                  89.61%       94.49%        86.35%\n",
      "SSL_FineTune_Hyperbolic              88.99%       94.01%        85.65%\n",
      "\n",
      "================================================================================\n",
      "üìä ÏÉÅÏÑ∏ ÎπÑÍµê Î∂ÑÏÑù\n",
      "================================================================================\n",
      "\n",
      "üèÜ ÏµúÏ¢Ö ÏÑ±Îä• Îû≠ÌÇπ (Overall Retention Í∏∞Ï§Ä)\n",
      "--------------------------------------------------------------------------------\n",
      "   1. SSL_LinearEval_Linear               (ssl-linear_eval     ) Retention: 90.98% (Mod: 95.79% | Str: 87.77%)\n",
      "   2. Supervised_Hyperbolic               (supervised          ) Retention: 90.72% (Mod: 94.39% | Str: 88.28%)\n",
      "   3. Supervised_Linear                   (supervised          ) Retention: 90.71% (Mod: 95.34% | Str: 87.63%)\n",
      "   4. SSL_LinearEval_Hyperbolic           (ssl-linear_eval     ) Retention: 90.15% (Mod: 95.79% | Str: 86.38%)\n",
      "   5. SSL_FineTune_Linear                 (ssl-finetune        ) Retention: 89.61% (Mod: 94.49% | Str: 86.35%)\n",
      "   6. SSL_FineTune_Hyperbolic             (ssl-finetune        ) Retention: 88.99% (Mod: 94.01% | Str: 85.65%)\n",
      "\n",
      "================================================================================\n",
      "üéØ Í≤∞Î°†\n",
      "================================================================================\n",
      "   - ÏµúÍ≥† ÏÑ±Îä•: SSL_LinearEval_Linear (Retention: 90.98%)\n",
      "   - Supervised baseline: 90.72% (Mod: 94.39% | Str: 88.28%)\n",
      "   - SSL best: 90.98% (Mod: 95.79% | Str: 87.77%)\n",
      "   - Performance gap: 0.25pp\n",
      "\n",
      "   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\n",
      "   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï ‚úÖ\n",
      "   - Cosine + Warmup Ïä§ÏºÄÏ§ÑÎü¨ ‚úÖ\n",
      "   - EMA (Exponential Moving Average) ‚úÖ\n",
      "   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune) ‚úÖ\n",
      "   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch) ‚úÖ\n",
      "   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (warp: 0.10, cutout: 0.10) ‚úÖ\n",
      "   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c ‚úÖ\n",
      "   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong) ‚úÖ\n",
      "\n",
      "‚úÖ Results saved to:\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\\supervised_vs_ssl_results_optimized.json\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\\visualization_data_optimized.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================== Main Entry Point ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"   üß™ UCI-HAR Comprehensive Comparison (‚ú® OPTIMIZED)\")\n",
    "    print(\"   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\")\n",
    "    print(\"   Architecture: ResNet + Transformer Encoder\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   üìã Ïã§Ìóò ÏÑ§Í≥Ñ:\")\n",
    "    print(\"   1. ÎèôÏùºÌïú Î∞±Î≥∏ (ResNet + Transformer)\")\n",
    "    print(\"   2. 2Î†àÎ≤® Ï†ÑÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖã (Moderate/Strong)\")\n",
    "    print(\"   3. 6Í∞ÄÏßÄ ÏÑ§Ï†ï ÎπÑÍµê:\")\n",
    "    print(\"      ‚îú‚îÄ Supervised √ó (Linear, Hyperbolic)\")\n",
    "    print(\"      ‚îú‚îÄ SSL Linear Eval √ó (Linear, Hyperbolic)\")\n",
    "    print(\"      ‚îî‚îÄ SSL Fine-tune √ó (Linear, Hyperbolic)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   ‚öôÔ∏è  Supervised ÏÑ§Ï†ï:\")\n",
    "    print(f\"   - Epochs: {config.finetune_epochs}\")\n",
    "    print(f\"   - Batch size: {config.finetune_batch_size}\")\n",
    "    print(f\"   - Learning rate: {config.finetune_lr}\")\n",
    "    print(f\"   - Warmup: {config.finetune_warmup_epochs} epochs\")\n",
    "    print(f\"   - EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"   - Training: End-to-end with labels + consistency loss\")\n",
    "    print(f\"\\n   ‚öôÔ∏è  SSL ÏÑ§Ï†ï:\")\n",
    "    print(f\"   - Stage 1 (Pretrain): {config.pretrain_epochs} epochs, batch={config.pretrain_batch_size}, lr={config.pretrain_lr}\")\n",
    "    print(f\"     ‚Üí Contrastive learning only (NO LABELS)\")\n",
    "    print(f\"     ‚Üí Label-independent augmentation\")\n",
    "    print(f\"     ‚Üí Warmup: {config.pretrain_warmup_epochs} epochs\")\n",
    "    print(f\"     ‚Üí EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Stage 2 (Eval/FT): {config.finetune_epochs} epochs, batch={config.finetune_batch_size}, lr={config.finetune_lr}\")\n",
    "    print(f\"     ‚Üí Linear Eval: Freeze backbone\")\n",
    "    print(f\"     ‚Üí Fine-tune: Train all (backbone LR √ó {config.finetune_backbone_lr_ratio})\")\n",
    "    print(f\"     ‚Üí Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"\\n   üîß Augmentations (SSL - Optimized):\")\n",
    "    print(f\"   - Jitter (scale={config.aug_jitter_scale})\")\n",
    "    print(f\"   - Scaling (range={config.aug_scale_range})\")\n",
    "    print(f\"   - Channel Drop (prob={config.aug_channel_drop_prob})\")\n",
    "    print(f\"   - Time Warp (prob={config.aug_time_warp_prob}) ‚úÖ 0.3‚Üí0.10\")\n",
    "    print(f\"   - Cutout (prob={config.aug_cutout_prob}, ratio={config.aug_cutout_ratio}) ‚úÖ 0.2‚Üí0.10\")\n",
    "    print(\"   - ALL label-independent!\")\n",
    "    print(f\"\\n   üèóÔ∏è  Architecture:\")\n",
    "    print(f\"   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads={config.n_heads}, layers={config.n_layers})\")\n",
    "    print(f\"   - d_model: {config.d_model}, dropout: {config.dropout}\")\n",
    "    print(f\"   - Classifier: Linear vs Hyperbolic (c_init={config.hyperbolic_c_init}, learnable ‚úÖ)\")\n",
    "    print(f\"   - Projection dim (SSL): {config.projection_dim}\")\n",
    "    print(f\"\\n   üî¨ SSL Contrastive Learning:\")\n",
    "    print(f\"   - Loss: NT-Xent (InfoNCE)\")\n",
    "    print(f\"   - Temperature: {config.temperature}\")\n",
    "    print(f\"   - Negative samples: 2*batch_size - 2\")\n",
    "    print(f\"\\n   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\")\n",
    "    print(\"   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï (Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã)\")\n",
    "    print(\"   - Cosine Annealing + Warmup\")\n",
    "    print(\"   - EMA (Exponential Moving Average)\")\n",
    "    print(\"   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune)\")\n",
    "    print(\"   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch)\")\n",
    "    print(\"   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (Í≤ΩÍ≥Ñ Ï†ïÎ≥¥ Î≥¥Ï°¥)\")\n",
    "    print(\"   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c\")\n",
    "    print(\"   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    run_full_comparison(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee20d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
