{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b488a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23430866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Random Seed ========================\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769b9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Configuration ========================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"실험 설정\"\"\"\n",
    "    data_dir: str = \"C://Users/park9/HAR/SSL_HAR/data\"\n",
    "    save_dir: str = \"C://Users/park9/HAR/SSL_HAR/RESULTS/BASIC/low\"\n",
    "\n",
    "    # SSL Pretrain 파라미터\n",
    "    pretrain_epochs: int = 100\n",
    "    pretrain_batch_size: int = 256\n",
    "    pretrain_lr: float = 1e-3\n",
    "\n",
    "    # Supervised / Linear Eval / Fine-tune 파라미터\n",
    "    finetune_epochs: int = 50\n",
    "    finetune_batch_size: int = 128\n",
    "    finetune_lr: float = 3e-4\n",
    "\n",
    "    # 공통 파라미터\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.05\n",
    "\n",
    "    # 모델 파라미터\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    hyperbolic_c: float = 1.0\n",
    "\n",
    "    # SSL 파라미터\n",
    "    temperature: float = 0.07\n",
    "    projection_dim: int = 128\n",
    "\n",
    "    # Augmentation 파라미터 (라벨 독립)\n",
    "    aug_jitter_scale: float = 0.05\n",
    "    aug_scale_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    aug_channel_drop_prob: float = 0.2\n",
    "    aug_time_warp_prob: float = 0.10  # 0.3\n",
    "    aug_cutout_prob: float = 0.20  # 0.3\n",
    "    aug_cutout_ratio: float = 0.10  # 0.2\n",
    "\n",
    "    # 시스템 파라미터\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 0  # ⚠️ multiprocessing issue 방지를 위해 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52370685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Dataset Configuration ========================\n",
    "\n",
    "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
    "RAW_CHANNELS = [\n",
    "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
    "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
    "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
    "]\n",
    "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
    "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
    "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
    "\n",
    "def load_split_raw(root: str, split: str):\n",
    "    assert split in (\"train\", \"test\")\n",
    "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
    "\n",
    "    # 존재 확인(문제 있으면 바로 어디가 없는지 알려줌)\n",
    "    if not os.path.isdir(inertial_path):\n",
    "        raise FileNotFoundError(f\"[Missing dir] {inertial_path}\")\n",
    "\n",
    "    X_list = []\n",
    "    for p, e in RAW_CHANNELS:\n",
    "        fpath = os.path.join(inertial_path, f\"{p}{split}.{e}\")  # ex) body_acc_x_train.txt\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise FileNotFoundError(f\"[Missing file] {fpath}\")\n",
    "        # URL 오인 방지: 파일 핸들로 전달\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            arr = np.loadtxt(f)       # (N, 128)\n",
    "        X_list.append(arr[..., None]) # (N, 128, 1)\n",
    "\n",
    "    # 채널 모두 같은 샘플 수인지 체크(안전장치)\n",
    "    n_samples = {x.shape[0] for x in X_list}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(f\"채널별 샘플 수 불일치: {n_samples}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)  # (N, 9, 128)\n",
    "\n",
    "    y_path = os.path.join(root, split, f\"y_{split}.txt\")\n",
    "    if not os.path.isfile(y_path):\n",
    "        raise FileNotFoundError(f\"[Missing file] {y_path}\")\n",
    "    with open(y_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        y = np.loadtxt(f).astype(int) - 1  # 0-based\n",
    "\n",
    "    print(f\"[OK] {split}: X{X.shape}, y{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "class UCIHARInertial(Dataset):\n",
    "    \"\"\"UCI-HAR Dataset (✅ 정규화 버그 수정)\"\"\"\n",
    "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
    "                 preloaded_data: Tuple[np.ndarray, np.ndarray] = None):\n",
    "        super().__init__()\n",
    "        if preloaded_data is not None:\n",
    "            X, y = preloaded_data\n",
    "        else:\n",
    "            X, y = load_split_raw(root, split)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
    "\n",
    "        # mean/std 세팅\n",
    "        if mean is not None and std is not None:\n",
    "            self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
    "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
    "\n",
    "        # ✅ preloaded_data 여부와 무관하게 항상 train 통계로 정규화\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67d2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Label-Independent Augmentations ========================\n",
    "\n",
    "def random_jitter(x: torch.Tensor, scale: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = torch.randn_like(x) * scale\n",
    "    return x + noise\n",
    "\n",
    "def random_scaling(x: torch.Tensor, scale_range: Tuple[float, float] = (0.8, 1.2)) -> torch.Tensor:\n",
    "    \"\"\"Random scaling of amplitudes\"\"\"\n",
    "    scale = torch.empty(x.size(0), x.size(1), 1, device=x.device).uniform_(*scale_range)\n",
    "    return x * scale\n",
    "\n",
    "def random_channel_drop(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly drop channels (set to zero)\"\"\"\n",
    "    B, C, T = x.shape\n",
    "    mask = torch.rand(B, C, 1, device=x.device) > drop_prob\n",
    "    return x * mask.float()\n",
    "\n",
    "def random_time_warp(x: torch.Tensor, warp_prob: float = 0.3) -> torch.Tensor:\n",
    "    \"\"\"Simple time warping by random interpolation\"\"\"\n",
    "    if random.random() > warp_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    warp_factor = random.uniform(0.8, 1.2)\n",
    "    new_T = int(T * warp_factor)\n",
    "\n",
    "    x_warped = F.interpolate(x, size=new_T, mode='linear', align_corners=False)\n",
    "\n",
    "    if new_T > T:\n",
    "        start = random.randint(0, new_T - T)\n",
    "        x_warped = x_warped[:, :, start:start+T]\n",
    "    elif new_T < T:\n",
    "        pad_total = T - new_T\n",
    "        pad_left = random.randint(0, pad_total)\n",
    "        pad_right = pad_total - pad_left\n",
    "        x_warped = F.pad(x_warped, (pad_left, pad_right), mode='replicate')\n",
    "\n",
    "    return x_warped\n",
    "\n",
    "def random_cutout(x: torch.Tensor, cutout_prob: float = 0.3, cutout_ratio: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly mask out a temporal segment\"\"\"\n",
    "    if random.random() > cutout_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    cutout_len = int(T * cutout_ratio)\n",
    "    start = random.randint(0, T - cutout_len)\n",
    "    x_cut = x.clone()\n",
    "    x_cut[:, :, start:start+cutout_len] = 0\n",
    "    return x_cut\n",
    "\n",
    "def augment_time_series(x: torch.Tensor, cfg: Config) -> torch.Tensor:\n",
    "    \"\"\"Label-independent augmentation pipeline\"\"\"\n",
    "    x_aug = x.clone()\n",
    "    x_aug = random_jitter(x_aug, scale=cfg.aug_jitter_scale)\n",
    "    x_aug = random_scaling(x_aug, scale_range=cfg.aug_scale_range)\n",
    "    x_aug = random_channel_drop(x_aug, drop_prob=cfg.aug_channel_drop_prob)\n",
    "    x_aug = random_time_warp(x_aug, warp_prob=cfg.aug_time_warp_prob)\n",
    "    x_aug = random_cutout(x_aug, cutout_prob=cfg.aug_cutout_prob, cutout_ratio=cfg.aug_cutout_ratio)\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59516c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== ResNet Building Blocks ========================\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"1D Residual Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_blocks=[2, 2, 2]):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(d_model, num_blocks[2], stride=2)\n",
    "\n",
    "        self.stride = 16\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResBlock1D(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock1D(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fb68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transformer Encoder ========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Module\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, C, T) -> (B, T, C)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)  # (B, T, C) -> (B, C, T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e506fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Backbone ========================\n",
    "\n",
    "class ResNetTransformerBackbone(nn.Module):\n",
    "    \"\"\"ResNet + Transformer Encoder Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet1D(in_channels=in_channels, d_model=d_model)\n",
    "        self.transformer = TransformerEncoder(d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.stride = self.resnet.stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = self.resnet(x)\n",
    "        fmap = self.transformer(fmap)\n",
    "        return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7afe926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Projection Head ========================\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"MLP projection head for contrastive learning\"\"\"\n",
    "    def __init__(self, d_model, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715537c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Classification Heads ========================\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Linear Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "class HyperbolicProjection(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Projection\"\"\"\n",
    "    def __init__(self, c=1.0):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
    "        max_norm = (1.0 / math.sqrt(self.c)) - 1e-4\n",
    "        scale = torch.clamp(norm, max=max_norm) / norm\n",
    "        return x * scale\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int, c: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pre_proj = nn.Linear(d_model, d_model)\n",
    "        self.hyperbolic_proj = HyperbolicProjection(c=c)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        h = self.pre_proj(pooled)\n",
    "        h_hyp = self.hyperbolic_proj(h)\n",
    "        logits = self.fc(h_hyp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9637ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SSL Model ========================\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    \"\"\"Self-Supervised Learning Model\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetTransformerBackbone(\n",
    "            in_channels=9, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(d_model, projection_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns normalized projection\"\"\"\n",
    "        fmap = self.backbone(x)\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        z = self.projection_head(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287cc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Contrastive Loss ========================\n",
    "\n",
    "def contrastive_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"NT-Xent Loss (InfoNCE)\"\"\"\n",
    "    B = z1.shape[0]\n",
    "    device = z1.device\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)  # (2B, D)\n",
    "    sim_matrix = torch.mm(z, z.t()) / temperature  # (2B, 2B)\n",
    "\n",
    "    # Positive pairs: (i, i+B) and (i+B, i)\n",
    "    labels = torch.arange(B, device=device)\n",
    "    labels = torch.cat([labels + B, labels], dim=0)\n",
    "\n",
    "    # Mask out self-similarity\n",
    "    mask = torch.eye(2 * B, device=device, dtype=torch.bool)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16df0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Training Functions ========================\n",
    "\n",
    "def pretrain_one_epoch(model: SSLModel, loader: DataLoader, opt: torch.optim.Optimizer, cfg: Config):\n",
    "    \"\"\"SSL Pretrain: No labels, only contrastive loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for x, _ in loader:  # Ignore labels!\n",
    "        x = x.to(cfg.device)\n",
    "        x1 = augment_time_series(x, cfg)\n",
    "        x2 = augment_time_series(x, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = contrastive_loss(z1, z2, temperature=cfg.temperature)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "    return {\"ssl_loss\": total_loss / total_samples}\n",
    "\n",
    "def linear_eval_epoch(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                     opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Linear evaluation: Freeze backbone, train head only\"\"\"\n",
    "    if train:\n",
    "        backbone.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fmap = backbone(x)\n",
    "\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "def finetune_epoch(model: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                  opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Fine-tuning: Train both backbone and head\"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "        head.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        fmap = model.backbone(x)\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(head.parameters()), cfg.grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(backbone: nn.Module, head: nn.Module, loader: DataLoader, cfg: Config):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        fmap = backbone(x)\n",
    "        logits = head(fmap)\n",
    "        y_pred.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cbcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transitional Test Set ========================\n",
    "\n",
    "def create_transitional_test_set(\n",
    "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
    ") -> Tuple[UCIHARInertial, dict]:\n",
    "    \"\"\"Create transitional test set\"\"\"\n",
    "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
    "    N, C, T = X.shape\n",
    "\n",
    "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
    "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
    "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
    "    for t, s in zip(targets_A, sources_B):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
    "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
    "    for t, s in zip(targets_B, sources_A):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    mod_dataset = UCIHARInertial(\n",
    "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
    "        preloaded_data=(X, y)\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        'modified_samples': len(targets_A) + len(targets_B),\n",
    "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
    "    }\n",
    "    return mod_dataset, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dfa358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== JSON Encoder ========================\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON Encoder for NumPy types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f65fe498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Main Experiment Function ========================\n",
    "\n",
    "def run_full_comparison(cfg: Config):\n",
    "    \"\"\"Run complete supervised vs SSL comparison\"\"\"\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\n📦 Loading UCI-HAR Dataset...\")\n",
    "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
    "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
    "    print(f\"   - Train samples: {len(train_set)}\")\n",
    "    print(f\"   - Test samples: {len(test_set_orig)}\")\n",
    "\n",
    "    # Create transitional test sets\n",
    "    scenarios = [\n",
    "        (\"STANDING\", \"SITTING\", 0.60, 0.50),\n",
    "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
    "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"    🔬 TRANSITIONAL TEST SETS 생성\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    transition_test_data = []\n",
    "    for clsA, clsB, p, mix in scenarios:\n",
    "        test_set_mod, info = create_transitional_test_set(test_set_orig, clsA, clsB, p=p, mix=mix)\n",
    "        transition_test_data.append((test_set_mod, info))\n",
    "        print(f\"   - {clsA}↔{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}개 샘플 변형\")\n",
    "\n",
    "    # Experiment configurations\n",
    "    experiment_configs = [\n",
    "        {\"name\": \"Supervised_Linear\", \"method\": \"supervised\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"Supervised_Hyperbolic\", \"method\": \"supervised\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_LinearEval_Linear\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_LinearEval_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_FineTune_Linear\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_FineTune_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": True},\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    for exp_cfg in experiment_configs:\n",
    "        print(f\"\\n{'='*80}\\n   실험: {exp_cfg['name']}\\n{'='*80}\")\n",
    "\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "        if exp_cfg['method'] == 'supervised':\n",
    "            # Supervised Learning\n",
    "            print(\"\\n📚 Supervised Learning (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            backbone = ResNetTransformerBackbone(\n",
    "                in_channels=9, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "                n_layers=cfg.n_layers, dropout=cfg.dropout\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c=cfg.hyperbolic_c).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            params = list(backbone.parameters()) + list(head.parameters())\n",
    "            opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            def train_supervised_epoch(backbone, head, loader, opt, cfg, train=True):\n",
    "                if train:\n",
    "                    backbone.train()\n",
    "                    head.train()\n",
    "                else:\n",
    "                    backbone.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "                    fmap = backbone(x)\n",
    "                    logits = head(fmap)\n",
    "                    loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "                    if train:\n",
    "                        opt.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "                        opt.step()\n",
    "\n",
    "                    pred = logits.argmax(dim=-1)\n",
    "                    total_correct += (pred == y).sum().item()\n",
    "                    total_loss += loss.item() * y.size(0)\n",
    "                    total_samples += y.size(0)\n",
    "\n",
    "                return {\n",
    "                    \"loss\": total_loss / total_samples,\n",
    "                    \"acc\": total_correct / total_samples\n",
    "                }\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"Training for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_supervised_epoch(backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "                te_acc, te_f1 = evaluate_model(backbone, head, test_loader_orig, cfg)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'backbone': copy.deepcopy(backbone.state_dict()),\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Supervised {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                backbone.load_state_dict(best_wts['backbone'])\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(backbone, head, test_loader_orig, cfg)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(backbone, head, test_loader_mod, cfg)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - Scenario {i+1}: Acc={acc_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"supervised\",\n",
    "                \"mode\": \"supervised\",\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "        else:  # SSL\n",
    "            # Stage 1: SSL Pretrain\n",
    "            print(\"\\n📚 Stage 1: Self-Supervised Pretraining (No Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            ssl_model = SSLModel(\n",
    "                d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers,\n",
    "                dropout=cfg.dropout, projection_dim=cfg.projection_dim\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            pretrain_loader = DataLoader(train_set, cfg.pretrain_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            ssl_opt = torch.optim.AdamW(ssl_model.parameters(), lr=cfg.pretrain_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            print(f\"Pretraining for {cfg.pretrain_epochs} epochs...\")\n",
    "            for epoch in range(1, cfg.pretrain_epochs + 1):\n",
    "                stats = pretrain_one_epoch(ssl_model, pretrain_loader, ssl_opt, cfg)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Pretrain {epoch:03d}/{cfg.pretrain_epochs}] SSL Loss: {stats['ssl_loss']:.4f}\")\n",
    "\n",
    "            print(\"✅ Pretraining Complete!\")\n",
    "\n",
    "            # Stage 2: Linear Eval or Fine-tune\n",
    "            print(f\"\\n📚 Stage 2: {exp_cfg['mode'].upper()} (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c=cfg.hyperbolic_c).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            if exp_cfg['mode'] == 'linear_eval':\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                opt = torch.optim.AdamW(head.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "                train_fn = lambda: linear_eval_epoch(ssl_model.backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            else:  # finetune\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                params = list(ssl_model.backbone.parameters()) + list(head.parameters())\n",
    "                opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "                train_fn = lambda: finetune_epoch(ssl_model, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"{exp_cfg['mode']} for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_fn()\n",
    "                te_acc, te_f1 = evaluate_model(ssl_model.backbone, head, test_loader_orig, cfg)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if exp_cfg['mode'] == 'finetune':\n",
    "                        best_wts['backbone'] = copy.deepcopy(ssl_model.backbone.state_dict())\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if exp_cfg['mode'] == 'finetune':\n",
    "                    ssl_model.backbone.load_state_dict(best_wts['backbone'])\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(ssl_model.backbone, head, test_loader_orig, cfg)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(ssl_model.backbone, head, test_loader_mod, cfg)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - Scenario {i+1}: Acc={acc_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"ssl\",\n",
    "                \"mode\": exp_cfg['mode'],\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"   📊 SUPERVISED vs TRUE SSL 실험 결과\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Method':<12} {'Mode':<12} {'Classifier':<12} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<10}\")\n",
    "    print(\"-\" * 115)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['method']:<12} {r['mode']:<12} {r['classifier']:<12} \"\n",
    "              f\"{r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
    "              f\"{r['avg_drop']:.4f}   {r['retention']:.2f}%\")\n",
    "\n",
    "    # Detailed analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 상세 비교 분석\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sup_lin = next(r for r in results_table if r['config'] == 'Supervised_Linear')\n",
    "    sup_hyp = next(r for r in results_table if r['config'] == 'Supervised_Hyperbolic')\n",
    "    linear_eval_lin = next(r for r in results_table if r['config'] == 'SSL_LinearEval_Linear')\n",
    "    finetune_lin = next(r for r in results_table if r['config'] == 'SSL_FineTune_Linear')\n",
    "    linear_eval_hyp = next(r for r in results_table if r['config'] == 'SSL_LinearEval_Hyperbolic')\n",
    "    finetune_hyp = next(r for r in results_table if r['config'] == 'SSL_FineTune_Hyperbolic')\n",
    "\n",
    "    # Final ranking\n",
    "    sorted_results = sorted(results_table, key=lambda x: x['retention'], reverse=True)\n",
    "    print(\"\\n🏆 최종 성능 랭킹 (Retention 기준)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for rank, r in enumerate(sorted_results, 1):\n",
    "        method_mode = f\"{r['method']}-{r['mode']}\" if r['method'] == 'ssl' else r['method']\n",
    "        print(f\"   {rank}. {r['config']:<35} ({method_mode:<20}) Retention: {r['retention']:.2f}%\")\n",
    "\n",
    "    best_config = sorted_results[0]\n",
    "    best_ssl = max([r for r in results_table if r['method'] == 'ssl'], key=lambda x: x['retention'])\n",
    "    best_sup = max([r for r in results_table if r['method'] == 'supervised'], key=lambda x: x['retention'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 결론\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - 최고 성능: {best_config['config']} (Retention: {best_config['retention']:.2f}%)\")\n",
    "    print(f\"   - Supervised baseline: {best_sup['retention']:.2f}%\")\n",
    "    print(f\"   - SSL best: {best_ssl['retention']:.2f}%\")\n",
    "    print(f\"   - Performance gap: {abs(best_ssl['retention'] - best_sup['retention']):.2f}pp\")\n",
    "\n",
    "    # Save results\n",
    "    save_path = os.path.join(cfg.save_dir, \"supervised_vs_ssl_results.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    visualization_data = {\n",
    "        'configs': [r['config'] for r in results_table],\n",
    "        'methods': [r['method'] for r in results_table],\n",
    "        'modes': [r['mode'] for r in results_table],\n",
    "        'classifiers': [r['classifier'] for r in results_table],\n",
    "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
    "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
    "        'retention': [r['retention'] for r in results_table],\n",
    "        'avg_drop': [r['avg_drop'] for r in results_table]\n",
    "    }\n",
    "\n",
    "    viz_path = os.path.join(cfg.save_dir, \"visualization_data.json\")\n",
    "    with open(viz_path, \"w\") as f:\n",
    "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n✅ Results saved to:\")\n",
    "    print(f\"   - {save_path}\")\n",
    "    print(f\"   - {viz_path}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53693212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   🧪 UCI-HAR Comprehensive Comparison\n",
      "   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\n",
      "   Architecture: ResNet + Transformer Encoder\n",
      "================================================================================\n",
      "\n",
      "   📋 실험 설계:\n",
      "   1. 동일한 백본 (ResNet + Transformer)\n",
      "   2. 동일한 전이 데이터셋 (4가지 시나리오)\n",
      "   3. 6가지 설정 비교:\n",
      "      ├─ Supervised × (Linear, Hyperbolic)\n",
      "      ├─ SSL Linear Eval × (Linear, Hyperbolic)\n",
      "      └─ SSL Fine-tune × (Linear, Hyperbolic)\n",
      "================================================================================\n",
      "\n",
      "   ⚙️  Supervised 설정:\n",
      "   - Epochs: 50\n",
      "   - Batch size: 128\n",
      "   - Learning rate: 0.0003\n",
      "   - Training: End-to-end with labels\n",
      "\n",
      "   ⚙️  SSL 설정:\n",
      "   - Stage 1 (Pretrain): 100 epochs, batch=256, lr=0.001\n",
      "     → Contrastive learning only (NO LABELS)\n",
      "     → Label-independent augmentation\n",
      "   - Stage 2 (Eval/FT): 50 epochs, batch=128, lr=0.0003\n",
      "     → Linear Eval: Freeze backbone\n",
      "     → Fine-tune: Train all\n",
      "\n",
      "   🔧 Augmentations (SSL):\n",
      "   - Jitter (scale=0.05)\n",
      "   - Scaling (range=(0.8, 1.2))\n",
      "   - Channel Drop (prob=0.2)\n",
      "   - Time Warp (prob=0.1)\n",
      "   - Cutout (prob=0.2, ratio=0.1)\n",
      "   - ALL label-independent!\n",
      "\n",
      "   🏗️  Architecture:\n",
      "   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads=4, layers=2)\n",
      "   - d_model: 128, dropout: 0.1\n",
      "   - Classifier: Linear vs Hyperbolic (c=1.0)\n",
      "   - Projection dim (SSL): 128\n",
      "\n",
      "   🔬 SSL Contrastive Learning:\n",
      "   - Loss: NT-Xent (InfoNCE)\n",
      "   - Temperature: 0.07\n",
      "   - Negative samples: 2*batch_size - 2\n",
      "================================================================================\n",
      "\n",
      "\n",
      "📦 Loading UCI-HAR Dataset...\n",
      "[OK] train: X(7352, 9, 128), y(7352,)\n",
      "[OK] test: X(2947, 9, 128), y(2947,)\n",
      "   - Train samples: 7352\n",
      "   - Test samples: 2947\n",
      "\n",
      "================================================================================\n",
      "    🔬 TRANSITIONAL TEST SETS 생성\n",
      "================================================================================\n",
      "   - STANDING↔SITTING (p=0.60, mix=0.50): 613개 샘플 변형\n",
      "   - STANDING↔SITTING (p=0.70, mix=0.55): 715개 샘플 변형\n",
      "   - WALKING↔WALKING_UPSTAIRS (p=0.65, mix=0.52): 628개 샘플 변형\n",
      "   - SITTING↔LAYING (p=0.75, mix=0.58): 770개 샘플 변형\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:0.5038 A:0.8893 | Test A:0.9308 F1:0.9305\n",
      "[Supervised 10/50] Train L:0.3114 A:0.9635 | Test A:0.9196 F1:0.9200\n",
      "[Supervised 20/50] Train L:0.3061 A:0.9655 | Test A:0.9216 F1:0.9215\n",
      "[Supervised 30/50] Train L:0.2893 A:0.9754 | Test A:0.9423 F1:0.9421\n",
      "[Supervised 40/50] Train L:0.2761 A:0.9837 | Test A:0.9444 F1:0.9456\n",
      "[Supervised 50/50] Train L:0.2714 A:0.9878 | Test A:0.9430 F1:0.9426\n",
      "✅ Best Test Acc: 0.9583\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.5029 (Drop=0.4554)\n",
      "     - Scenario 2: Acc=0.4930 (Drop=0.4652)\n",
      "     - Scenario 3: Acc=0.5202 (Drop=0.4381)\n",
      "     - Scenario 4: Acc=0.5148 (Drop=0.4435)\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.4057 A:0.8456 | Test A:0.9355 F1:0.9366\n",
      "[Supervised 10/50] Train L:0.5620 A:0.9561 | Test A:0.9274 F1:0.9297\n",
      "[Supervised 20/50] Train L:0.3646 A:0.9672 | Test A:0.9192 F1:0.9205\n",
      "[Supervised 30/50] Train L:0.3068 A:0.9767 | Test A:0.9281 F1:0.9299\n",
      "[Supervised 40/50] Train L:0.2953 A:0.9773 | Test A:0.9352 F1:0.9367\n",
      "[Supervised 50/50] Train L:0.2748 A:0.9856 | Test A:0.9437 F1:0.9435\n",
      "✅ Best Test Acc: 0.9555\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.5931 (Drop=0.3624)\n",
      "     - Scenario 2: Acc=0.5789 (Drop=0.3767)\n",
      "     - Scenario 3: Acc=0.5942 (Drop=0.3614)\n",
      "     - Scenario 4: Acc=0.6230 (Drop=0.3325)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 3.9468\n",
      "[Pretrain 010/100] SSL Loss: 1.3064\n",
      "[Pretrain 020/100] SSL Loss: 0.9671\n",
      "[Pretrain 030/100] SSL Loss: 0.7293\n",
      "[Pretrain 040/100] SSL Loss: 0.6786\n",
      "[Pretrain 050/100] SSL Loss: 0.6011\n",
      "[Pretrain 060/100] SSL Loss: 0.5339\n",
      "[Pretrain 070/100] SSL Loss: 0.5040\n",
      "[Pretrain 080/100] SSL Loss: 0.5093\n",
      "[Pretrain 090/100] SSL Loss: 0.4279\n",
      "[Pretrain 100/100] SSL Loss: 0.4222\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.7679 A:0.2737 | Test A:0.4428 F1:0.4251\n",
      "[linear_eval 10/50] Train L:0.6034 A:0.9003 | Test A:0.8996 F1:0.8966\n",
      "[linear_eval 20/50] Train L:0.4981 A:0.9191 | Test A:0.9091 F1:0.9073\n",
      "[linear_eval 30/50] Train L:0.4691 A:0.9244 | Test A:0.9138 F1:0.9126\n",
      "[linear_eval 40/50] Train L:0.4549 A:0.9276 | Test A:0.9165 F1:0.9155\n",
      "[linear_eval 50/50] Train L:0.4456 A:0.9319 | Test A:0.9165 F1:0.9156\n",
      "✅ Best Test Acc: 0.9179\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.5022 (Drop=0.4157)\n",
      "     - Scenario 2: Acc=0.5042 (Drop=0.4136)\n",
      "     - Scenario 3: Acc=0.4676 (Drop=0.4503)\n",
      "     - Scenario 4: Acc=0.4893 (Drop=0.4286)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 3.9468\n",
      "[Pretrain 010/100] SSL Loss: 1.3064\n",
      "[Pretrain 020/100] SSL Loss: 0.9671\n",
      "[Pretrain 030/100] SSL Loss: 0.7293\n",
      "[Pretrain 040/100] SSL Loss: 0.6786\n",
      "[Pretrain 050/100] SSL Loss: 0.6011\n",
      "[Pretrain 060/100] SSL Loss: 0.5339\n",
      "[Pretrain 070/100] SSL Loss: 0.5040\n",
      "[Pretrain 080/100] SSL Loss: 0.5093\n",
      "[Pretrain 090/100] SSL Loss: 0.4279\n",
      "[Pretrain 100/100] SSL Loss: 0.4222\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.6727 A:0.5427 | Test A:0.8073 F1:0.7953\n",
      "[linear_eval 10/50] Train L:0.6990 A:0.9204 | Test A:0.9223 F1:0.9218\n",
      "[linear_eval 20/50] Train L:0.4565 A:0.9381 | Test A:0.9298 F1:0.9300\n",
      "[linear_eval 30/50] Train L:0.3941 A:0.9459 | Test A:0.9281 F1:0.9281\n",
      "[linear_eval 40/50] Train L:0.3735 A:0.9483 | Test A:0.9352 F1:0.9357\n",
      "[linear_eval 50/50] Train L:0.3647 A:0.9513 | Test A:0.9311 F1:0.9316\n",
      "✅ Best Test Acc: 0.9352\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.4320 (Drop=0.5032)\n",
      "     - Scenario 2: Acc=0.4279 (Drop=0.5073)\n",
      "     - Scenario 3: Acc=0.4337 (Drop=0.5015)\n",
      "     - Scenario 4: Acc=0.4140 (Drop=0.5212)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 3.9468\n",
      "[Pretrain 010/100] SSL Loss: 1.3064\n",
      "[Pretrain 020/100] SSL Loss: 0.9671\n",
      "[Pretrain 030/100] SSL Loss: 0.7293\n",
      "[Pretrain 040/100] SSL Loss: 0.6786\n",
      "[Pretrain 050/100] SSL Loss: 0.6011\n",
      "[Pretrain 060/100] SSL Loss: 0.5339\n",
      "[Pretrain 070/100] SSL Loss: 0.5040\n",
      "[Pretrain 080/100] SSL Loss: 0.5093\n",
      "[Pretrain 090/100] SSL Loss: 0.4279\n",
      "[Pretrain 100/100] SSL Loss: 0.4222\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:0.6930 A:0.8262 | Test A:0.9423 F1:0.9440\n",
      "[finetune 10/50] Train L:0.2638 A:0.9898 | Test A:0.9786 F1:0.9791\n",
      "[finetune 20/50] Train L:0.2499 A:0.9967 | Test A:0.9698 F1:0.9704\n",
      "[finetune 30/50] Train L:0.2484 A:0.9967 | Test A:0.9695 F1:0.9701\n",
      "[finetune 40/50] Train L:0.2464 A:0.9971 | Test A:0.9762 F1:0.9767\n",
      "[finetune 50/50] Train L:0.2430 A:0.9990 | Test A:0.9722 F1:0.9727\n",
      "✅ Best Test Acc: 0.9796\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.5188 (Drop=0.4608)\n",
      "     - Scenario 2: Acc=0.5148 (Drop=0.4649)\n",
      "     - Scenario 3: Acc=0.5358 (Drop=0.4438)\n",
      "     - Scenario 4: Acc=0.5304 (Drop=0.4493)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 3.9468\n",
      "[Pretrain 010/100] SSL Loss: 1.3064\n",
      "[Pretrain 020/100] SSL Loss: 0.9671\n",
      "[Pretrain 030/100] SSL Loss: 0.7293\n",
      "[Pretrain 040/100] SSL Loss: 0.6786\n",
      "[Pretrain 050/100] SSL Loss: 0.6011\n",
      "[Pretrain 060/100] SSL Loss: 0.5339\n",
      "[Pretrain 070/100] SSL Loss: 0.5040\n",
      "[Pretrain 080/100] SSL Loss: 0.5093\n",
      "[Pretrain 090/100] SSL Loss: 0.4279\n",
      "[Pretrain 100/100] SSL Loss: 0.4222\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.4559 A:0.8113 | Test A:0.9430 F1:0.9447\n",
      "[finetune 10/50] Train L:0.5195 A:0.9808 | Test A:0.9715 F1:0.9720\n",
      "[finetune 20/50] Train L:0.3059 A:0.9939 | Test A:0.9756 F1:0.9760\n",
      "[finetune 30/50] Train L:0.2579 A:0.9978 | Test A:0.9742 F1:0.9746\n",
      "[finetune 40/50] Train L:0.2506 A:0.9967 | Test A:0.9691 F1:0.9694\n",
      "[finetune 50/50] Train L:0.2452 A:0.9980 | Test A:0.9681 F1:0.9677\n",
      "✅ Best Test Acc: 0.9776\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - Scenario 1: Acc=0.4900 (Drop=0.4876)\n",
      "     - Scenario 2: Acc=0.4876 (Drop=0.4900)\n",
      "     - Scenario 3: Acc=0.4958 (Drop=0.4818)\n",
      "     - Scenario 4: Acc=0.4985 (Drop=0.4791)\n",
      "\n",
      "================================================================================\n",
      "   📊 SUPERVISED vs TRUE SSL 실험 결과\n",
      "================================================================================\n",
      "Config                              Method       Mode         Classifier   Orig Acc   Trans Acc   Drop       Retention \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Supervised_Linear                   supervised   supervised   Linear       0.9583     0.5077      0.4505   52.98%\n",
      "Supervised_Hyperbolic               supervised   supervised   Hyperbolic   0.9555     0.5973      0.3582   62.51%\n",
      "SSL_LinearEval_Linear               ssl          linear_eval  Linear       0.9179     0.4908      0.4270   53.48%\n",
      "SSL_LinearEval_Hyperbolic           ssl          linear_eval  Hyperbolic   0.9352     0.4269      0.5083   45.65%\n",
      "SSL_FineTune_Linear                 ssl          finetune     Linear       0.9796     0.5249      0.4547   53.59%\n",
      "SSL_FineTune_Hyperbolic             ssl          finetune     Hyperbolic   0.9776     0.4930      0.4846   50.43%\n",
      "\n",
      "================================================================================\n",
      "📊 상세 비교 분석\n",
      "================================================================================\n",
      "\n",
      "🏆 최종 성능 랭킹 (Retention 기준)\n",
      "--------------------------------------------------------------------------------\n",
      "   1. Supervised_Hyperbolic               (supervised          ) Retention: 62.51%\n",
      "   2. SSL_FineTune_Linear                 (ssl-finetune        ) Retention: 53.59%\n",
      "   3. SSL_LinearEval_Linear               (ssl-linear_eval     ) Retention: 53.48%\n",
      "   4. Supervised_Linear                   (supervised          ) Retention: 52.98%\n",
      "   5. SSL_FineTune_Hyperbolic             (ssl-finetune        ) Retention: 50.43%\n",
      "   6. SSL_LinearEval_Hyperbolic           (ssl-linear_eval     ) Retention: 45.65%\n",
      "\n",
      "================================================================================\n",
      "🎯 결론\n",
      "================================================================================\n",
      "   - 최고 성능: Supervised_Hyperbolic (Retention: 62.51%)\n",
      "   - Supervised baseline: 62.51%\n",
      "   - SSL best: 53.59%\n",
      "   - Performance gap: 8.92pp\n",
      "\n",
      "✅ Results saved to:\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/BASIC/low\\supervised_vs_ssl_results.json\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/BASIC/low\\visualization_data.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================== Main Entry Point ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"   🧪 UCI-HAR Comprehensive Comparison\")\n",
    "    print(\"   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\")\n",
    "    print(\"   Architecture: ResNet + Transformer Encoder\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   📋 실험 설계:\")\n",
    "    print(\"   1. 동일한 백본 (ResNet + Transformer)\")\n",
    "    print(\"   2. 동일한 전이 데이터셋 (4가지 시나리오)\")\n",
    "    print(\"   3. 6가지 설정 비교:\")\n",
    "    print(\"      ├─ Supervised × (Linear, Hyperbolic)\")\n",
    "    print(\"      ├─ SSL Linear Eval × (Linear, Hyperbolic)\")\n",
    "    print(\"      └─ SSL Fine-tune × (Linear, Hyperbolic)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   ⚙️  Supervised 설정:\")\n",
    "    print(f\"   - Epochs: {config.finetune_epochs}\")\n",
    "    print(f\"   - Batch size: {config.finetune_batch_size}\")\n",
    "    print(f\"   - Learning rate: {config.finetune_lr}\")\n",
    "    print(f\"   - Training: End-to-end with labels\")\n",
    "    print(f\"\\n   ⚙️  SSL 설정:\")\n",
    "    print(f\"   - Stage 1 (Pretrain): {config.pretrain_epochs} epochs, batch={config.pretrain_batch_size}, lr={config.pretrain_lr}\")\n",
    "    print(\"     → Contrastive learning only (NO LABELS)\")\n",
    "    print(\"     → Label-independent augmentation\")\n",
    "    print(f\"   - Stage 2 (Eval/FT): {config.finetune_epochs} epochs, batch={config.finetune_batch_size}, lr={config.finetune_lr}\")\n",
    "    print(\"     → Linear Eval: Freeze backbone\")\n",
    "    print(\"     → Fine-tune: Train all\")\n",
    "    print(f\"\\n   🔧 Augmentations (SSL):\")\n",
    "    print(f\"   - Jitter (scale={config.aug_jitter_scale})\")\n",
    "    print(f\"   - Scaling (range={config.aug_scale_range})\")\n",
    "    print(f\"   - Channel Drop (prob={config.aug_channel_drop_prob})\")\n",
    "    print(f\"   - Time Warp (prob={config.aug_time_warp_prob})\")\n",
    "    print(f\"   - Cutout (prob={config.aug_cutout_prob}, ratio={config.aug_cutout_ratio})\")\n",
    "    print(\"   - ALL label-independent!\")\n",
    "    print(f\"\\n   🏗️  Architecture:\")\n",
    "    print(f\"   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads={config.n_heads}, layers={config.n_layers})\")\n",
    "    print(f\"   - d_model: {config.d_model}, dropout: {config.dropout}\")\n",
    "    print(f\"   - Classifier: Linear vs Hyperbolic (c={config.hyperbolic_c})\")\n",
    "    print(f\"   - Projection dim (SSL): {config.projection_dim}\")\n",
    "    print(f\"\\n   🔬 SSL Contrastive Learning:\")\n",
    "    print(f\"   - Loss: NT-Xent (InfoNCE)\")\n",
    "    print(f\"   - Temperature: {config.temperature}\")\n",
    "    print(f\"   - Negative samples: 2*batch_size - 2\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    run_full_comparison(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
