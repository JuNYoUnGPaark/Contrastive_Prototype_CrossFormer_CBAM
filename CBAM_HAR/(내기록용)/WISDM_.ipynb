{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7340188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# 0. 재현성 / 유틸\n",
    "# =================================================================================\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    재현성 확보: Python, NumPy, PyTorch 모두 시드 고정\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    DataLoader의 worker마다 난수 고정\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae708afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 데이터 로드 / 기본 라벨 매핑\n",
    "# -------------------------------------------\n",
    "def load_full_dataframe(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    WISDM raw txt를 pandas DataFrame으로 로드.\n",
    "    user, activity, timestamp, x, y, z\n",
    "    z는 'xxxx;' 형태라 세미콜론 제거.\n",
    "    \"\"\"\n",
    "    col_names = ['user', 'activity', 'timestamp', 'x', 'y', 'z']\n",
    "    df = pd.read_csv(filepath, header=None, names=col_names, on_bad_lines='skip')\n",
    "    df['z'] = pd.to_numeric(df['z'].astype(str).str.rstrip(';'), errors='coerce')\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    return df\n",
    "\n",
    "class WISDMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한 사용자/한 activity 구간씩 슬라이딩 윈도우를 뽑는다.\n",
    "    우리는 4종 라벨을 다 저장해둔다:\n",
    "      - y_full   : 최종 6-class {Walk,Jog,Up,Down,Sit,Stand} -> {0..5}\n",
    "      - y_branch : branch(2-class) locomotion=0 / static=1\n",
    "      - y_static : static 내부 2-class Sitting=0 / Standing=1 / else=-1\n",
    "      - y_loco   : locomotion 내부 4-class Walk=0 Jog=1 Up=2 Down=3 / else=-1\n",
    "    기본 __getitem__은 (x, y_full)를 반환하지만\n",
    "    branch/static/loco 학습엔 아래 SubsetForTask 래퍼를 쓴다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, window_size=200, step_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # 원래 6-class 인덱스 맵\n",
    "        self.full_mapping = {\n",
    "            'Walking': 0,\n",
    "            'Jogging': 1,\n",
    "            'Upstairs': 2,\n",
    "            'Downstairs': 3,\n",
    "            'Sitting': 4,\n",
    "            'Standing': 5\n",
    "        }\n",
    "\n",
    "        def to_branch(a):\n",
    "            # locomotion(걷기/뛰기/계단)=0, static(앉기/서기)=1\n",
    "            if a in ['Walking','Jogging','Upstairs','Downstairs']:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        def to_static(a):\n",
    "            # Sitting=0, Standing=1, else=-1 (해당없음)\n",
    "            if a == 'Sitting':\n",
    "                return 0\n",
    "            elif a == 'Standing':\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        def to_loco(a):\n",
    "            # Walking=0, Jogging=1, Upstairs=2, Downstairs=3, else=-1\n",
    "            if a == 'Walking':\n",
    "                return 0\n",
    "            elif a == 'Jogging':\n",
    "                return 1\n",
    "            elif a == 'Upstairs':\n",
    "                return 2\n",
    "            elif a == 'Downstairs':\n",
    "                return 3\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        X_list = []\n",
    "        y_full_list = []\n",
    "        y_branch_list = []\n",
    "        y_static_list = []\n",
    "        y_loco_list = []\n",
    "\n",
    "        #  subject별 / activity별로 끊어서 -> sliding window\n",
    "        for (user, activity), group in dataframe.groupby(['user', 'activity']):\n",
    "            # 슬라이딩 윈도우\n",
    "            for i in range(0, len(group) - self.window_size, self.step_size):\n",
    "                window = group.iloc[i:i+self.window_size]\n",
    "\n",
    "                # 입력 시그널 (3, T)\n",
    "                sig = window[['x','y','z']].values.T.astype(np.float32)\n",
    "                X_list.append(sig)\n",
    "\n",
    "                # 라벨들\n",
    "                y_full_list.append(self.full_mapping[activity])\n",
    "                y_branch_list.append(to_branch(activity))\n",
    "                y_static_list.append(to_static(activity))\n",
    "                y_loco_list.append(to_loco(activity))\n",
    "\n",
    "        # numpy 배열화\n",
    "        self.X = np.stack(X_list)  # (N, 3, window_size)\n",
    "        self.y_full = np.array(y_full_list, dtype=np.int64)\n",
    "        self.y_branch = np.array(y_branch_list, dtype=np.int64)\n",
    "        self.y_static = np.array(y_static_list, dtype=np.int64)\n",
    "        self.y_loco = np.array(y_loco_list, dtype=np.int64)\n",
    "\n",
    "        print(f\"[WISDMDataset] X={self.X.shape} full={self.y_full.shape} \"\n",
    "              f\"branch={self.y_branch.shape} static={self.y_static.shape} \"\n",
    "              f\"loco={self.y_loco.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 기본은 full 6-class 라벨을 반환 \n",
    "        x = torch.from_numpy(self.X[idx])          # (3, window)\n",
    "        y = torch.tensor(self.y_full[idx])         # scalar\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b16992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetForTask(Dataset):\n",
    "    \"\"\"\n",
    "    기존 WISDMDataset에서 특정 인덱스들만 골라 쓰고,\n",
    "    어떤 라벨(y_branch / y_static / y_loco)을 target으로 쓸지 선택하는 래퍼.\n",
    "    - base_dataset: WISDMDataset 인스턴스\n",
    "    - indices: 사용할 샘플 인덱스 리스트/array\n",
    "    - task: 'branch' | 'static' | 'loco'\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, indices, task='branch'):\n",
    "        self.base = base_dataset\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "        assert task in ['branch', 'static', 'loco'], \"task must be one of ['branch','static','loco']\"\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "\n",
    "        x = torch.from_numpy(self.base.X[idx])  # (3, window)\n",
    "        if self.task == 'branch':\n",
    "            y = self.base.y_branch[idx]\n",
    "        elif self.task == 'static':\n",
    "            y = self.base.y_static[idx]\n",
    "        elif self.task == 'loco':\n",
    "            y = self.base.y_loco[idx]\n",
    "\n",
    "        return x.float(), torch.tensor(int(y), dtype=torch.long)\n",
    "\n",
    "# 추론 시 라벨 없이 X만 뽑고, 원래 인덱스를 기억하고 싶을 때:\n",
    "class IndexOnlyXDataset(Dataset):\n",
    "    def __init__(self, base_dataset: WISDMDataset, indices):\n",
    "        self.base = base_dataset\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        x = torch.from_numpy(self.base.X[idx])  # (3,T)\n",
    "        return x.float(), int(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c9182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2. CBAM (1D 버전)\n",
    "# =================================================================================\n",
    "class ChannelAttention1D(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, C, T)\n",
    "        avg_out = self.avg_pool(x).squeeze(-1)  # (B, C)\n",
    "        max_out = self.max_pool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        avg_out = self.fc(avg_out)\n",
    "        max_out = self.fc(max_out)\n",
    "\n",
    "        out = (avg_out + max_out).unsqueeze(-1)  # (B, C, 1)\n",
    "        scale = self.sigmoid(out)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class TemporalAttention1D(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, C, T)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "\n",
    "        out = torch.cat([avg_out, max_out], dim=1)  # (B, 2, T)\n",
    "        out = self.conv(out)                        # (B, 1, T)\n",
    "        out = self.sigmoid(out)\n",
    "        return x * out\n",
    "\n",
    "\n",
    "class CBAM1D(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention1D(channels, reduction)\n",
    "        self.temporal_att = TemporalAttention1D(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, C, T)\n",
    "        x = self.channel_att(x)\n",
    "        x = self.temporal_att(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f56fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 3. Contrastive Prototype Loss\n",
    "# =================================================================================\n",
    "class ContrastivePrototypeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    각 클래스의 prototype과 feature를 InfoNCE 방식 loss\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, prototypes, labels):\n",
    "        \"\"\"\n",
    "        Contrastive Loss between features and prototypes\n",
    "\n",
    "        Args:\n",
    "            features: (B, D) - 샘플 특징\n",
    "            prototypes: (N_class, D) - 클래스별 프로토타입\n",
    "            labels: (B,) - 레이블\n",
    "\n",
    "        Returns:\n",
    "            loss: contrastive loss\n",
    "        \"\"\"\n",
    "        # L2 normalize\n",
    "        features = F.normalize(features, dim=1)\n",
    "        prototypes = F.normalize(prototypes, dim=1)\n",
    "\n",
    "        # cosine similarity\n",
    "        logits = torch.matmul(features, prototypes.t()) / self.temperature  # (B, num_classes)\n",
    "\n",
    "        # InfoNCE Loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7715cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 4. CrossFormer Block (Cross-Attn between tokens and learnable prototypes)\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 n_prototypes=6,\n",
    "                 n_heads=4,\n",
    "                 mlp_ratio=2.0, \n",
    "                 dropout=0.1,\n",
    "                 initial_prototypes=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.prototypes = nn.Parameter(torch.randn(n_prototypes, dim))\n",
    "        if initial_prototypes is not None:\n",
    "            assert initial_prototypes.shape == self.prototypes.shape, \\\n",
    "                f\"Shape mismatch: initial_prototypes {initial_prototypes.shape} vs self.prototypes {self.prototypes.shape}\"\n",
    "            self.prototypes.data.copy_(initial_prototypes)\n",
    "            print(\">>> [Main Model] Prototypes initialized with calculated mean features.\")\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.prototypes)\n",
    "            print(\">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\")\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "                                 nn.Linear(hidden_dim, dim), nn.Dropout(dropout))\n",
    "        self.proto_proj = nn.Sequential(nn.Linear(dim, dim), nn.GELU(), nn.Linear(dim, dim))\n",
    "\n",
    "    def forward(self, x, return_proto_features=False, skip_cross_attention=False):\n",
    "        B, T, C = x.shape\n",
    "        attn_weights = None\n",
    "\n",
    "        if not skip_cross_attention:\n",
    "            normalized_prototypes = F.normalize(self.prototypes, dim=1)\n",
    "            prototypes = normalized_prototypes.unsqueeze(0).expand(B, -1, -1)\n",
    "            x_norm = self.norm1(x)\n",
    "            cross_out, attn_weights = self.cross_attn(x_norm, prototypes, prototypes)\n",
    "            x = x + cross_out\n",
    "\n",
    "        x_norm = self.norm2(x)\n",
    "        self_out, _ = self.self_attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self_out\n",
    "        x = x + self.mlp(self.norm3(x))\n",
    "\n",
    "        if return_proto_features:\n",
    "            proto_features = x.mean(dim=1)\n",
    "            proto_features = self.proto_proj(proto_features)\n",
    "            return x, proto_features, attn_weights\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d911c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 5. 최종 HAR 모델: embedding + (CBAM) + CrossFormer + classifier\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerCBAM_HAR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=9, \n",
    "                 seq_len=200,\n",
    "                 embed_dim=64, \n",
    "                 reduced_dim=32,\n",
    "                 n_classes=6, \n",
    "                 n_prototypes=6, \n",
    "                 n_heads=8,\n",
    "                 kernel_size=7,\n",
    "                 dropout=0.1,\n",
    "                 temperature=0.07, \n",
    "                 initial_prototypes=None,\n",
    "                 use_cbam=True,\n",
    "                 use_crossformer=True,\n",
    "                 use_contrast=True,\n",
    "                 use_dim_reduction=False):\n",
    "        super().__init__()\n",
    "        self.use_cbam = use_cbam\n",
    "        self.use_crossformer = use_crossformer\n",
    "        self.use_contrast = use_contrast\n",
    "        self.use_dim_reduction = use_dim_reduction\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, embed_dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2),\n",
    "            nn.BatchNorm1d(embed_dim), nn.GELU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        if self.use_cbam:\n",
    "            self.cbam = CBAM1D(embed_dim, reduction=8, kernel_size=kernel_size)\n",
    "\n",
    "        working_dim = reduced_dim if use_dim_reduction else embed_dim\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_reduce = nn.Linear(embed_dim, reduced_dim)\n",
    "\n",
    "        if self.use_crossformer:\n",
    "            self.crossformer = ContrastCrossFormerBlock(\n",
    "                dim=working_dim, n_prototypes=n_prototypes, n_heads=n_heads,\n",
    "                mlp_ratio=2.0, dropout=dropout, initial_prototypes=initial_prototypes\n",
    "            )\n",
    "        else:\n",
    "            self.self_attn = nn.TransformerEncoderLayer(\n",
    "                d_model=working_dim, nhead=n_heads, dim_feedforward=int(working_dim * 2),\n",
    "                dropout=dropout, batch_first=True\n",
    "            )\n",
    "\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_restore = nn.Linear(reduced_dim, embed_dim)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "        if self.use_contrast and self.use_crossformer:\n",
    "            self.contrast_loss = ContrastivePrototypeLoss(temperature=temperature)\n",
    "\n",
    "    def forward(self, x, labels=None, return_contrast_loss=False):\n",
    "        x = self.embedding(x)\n",
    "        if self.use_cbam:\n",
    "            x = self.cbam(x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_reduce(x)\n",
    "        \n",
    "        proto_features = None\n",
    "        if self.use_crossformer:\n",
    "            if return_contrast_loss and self.use_contrast:\n",
    "                x, proto_features, _ = self.crossformer(x, return_proto_features=True)\n",
    "            else:\n",
    "                x = self.crossformer(x, return_proto_features=False)\n",
    "        else:\n",
    "            x = self.self_attn(x)\n",
    "            \n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_restore(x)\n",
    "            \n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_contrast_loss and self.use_contrast and proto_features is not None and labels is not None:\n",
    "            contrast_loss = self.contrast_loss(proto_features, self.crossformer.prototypes, labels)\n",
    "            return logits, contrast_loss\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2cada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 6. 프로토타입 초기화: train data 평균 feature로 클래스별 prototype 만들기\n",
    "# =================================================================================\n",
    "def get_mean_prototypes(train_full_dataset, device, config):\n",
    "    print(\"Calculating initial prototypes from mean features...\")\n",
    "\n",
    "    temp_model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=config['in_channels'],\n",
    "        seq_len=config['seq_len'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        reduced_dim=config['reduced_dim'], \n",
    "        n_heads=config['n_heads'],\n",
    "        kernel_size=config['kernel_size'],\n",
    "        dropout=config['dropout'],\n",
    "        use_cbam=True,\n",
    "        use_crossformer=True, \n",
    "        use_contrast=False,\n",
    "        use_dim_reduction=config['use_dim_reduction']\n",
    "    ).to(device)\n",
    "\n",
    "    temp_model.eval()\n",
    "\n",
    "    temp_loader = DataLoader(train_full_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    all_features, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(temp_loader, desc=\"Prototype Init\"):\n",
    "            batch_x = batch_x.to(device)\n",
    "            x = temp_model.embedding(batch_x)\n",
    "            if temp_model.use_cbam:\n",
    "                x = temp_model.cbam(x)\n",
    "            x = x.transpose(1, 2).contiguous()\n",
    "            if temp_model.use_dim_reduction:\n",
    "                x = temp_model.dim_reduce(x)\n",
    "            x = temp_model.crossformer(x, skip_cross_attention=True)\n",
    "            x = x.transpose(1, 2).contiguous()\n",
    "            pooled_features = temp_model.pool(x).squeeze(-1)\n",
    "            all_features.append(pooled_features.cpu())\n",
    "            all_labels.append(batch_y.cpu())\n",
    "\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    working_dim = config['reduced_dim'] if config['use_dim_reduction'] else config['embed_dim']\n",
    "    n_cls        = 6\n",
    "    mean_proto   = torch.zeros(n_cls, working_dim)\n",
    "\n",
    "    for c in range(n_cls):\n",
    "        feats_c = all_features[all_labels == c]\n",
    "        mean_proto[c] = feats_c.mean(dim=0) if len(feats_c)>0 else torch.randn(working_dim)\n",
    "\n",
    "    return mean_proto.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada2ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 7. 학습/평가 루프\n",
    "# =================================================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, use_contrast=True, contrast_weight=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_contrast_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        if use_contrast and model.use_contrast and model.use_crossformer:\n",
    "            logits, contrast_loss = model(batch_x, batch_y, return_contrast_loss=True)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            loss = ce_loss + contrast_weight * contrast_loss\n",
    "            total_contrast_loss += contrast_loss.item()\n",
    "        else:\n",
    "            logits = model(batch_x)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            loss = ce_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.synchronize() # 한 에폭 끝에서 동기화\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "    avg_contrast_loss = total_contrast_loss / len(dataloader) if total_contrast_loss > 0 else 0\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_loss, avg_ce_loss, avg_contrast_loss, acc, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, acc, f1, np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e2ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 8. Oversampling loader\n",
    "# =================================================================================\n",
    "def make_class_weights_for_dataset(dataset: SubsetForTask):\n",
    "    ys = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, y = dataset[i]\n",
    "        ys.append(int(y.item()))\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    unique, counts = np.unique(ys, return_counts=True)\n",
    "    invfreq = {c:1.0/counts[j] for j,c in enumerate(unique)}\n",
    "\n",
    "    sample_weights = np.array([invfreq[int(label)] for label in ys], dtype=np.float32)\n",
    "    return torch.from_numpy(sample_weights)\n",
    "\n",
    "def make_loader(dataset, batch_size, oversample=True, shuffle=True):\n",
    "    if oversample:\n",
    "        sw = make_class_weights_for_dataset(dataset)\n",
    "        sampler = WeightedRandomSampler(weights=sw,\n",
    "                                        num_samples=len(sw),\n",
    "                                        replacement=True)\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=sampler,\n",
    "                            drop_last=False)\n",
    "    else:\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=False)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0822d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 9. 모델 생성/학습 헬퍼\n",
    "# =================================================================================\n",
    "def build_model_for_task(num_classes, config, device, initial_prototypes=None):\n",
    "    model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=config['in_channels'],\n",
    "        seq_len=config['seq_len'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        reduced_dim=config['reduced_dim'],\n",
    "        n_classes=num_classes,\n",
    "        n_prototypes=num_classes,\n",
    "        n_heads=config['n_heads'],\n",
    "        kernel_size=config['kernel_size'],\n",
    "        dropout=config['dropout'],\n",
    "        temperature=config['temperature'],\n",
    "        initial_prototypes=initial_prototypes,\n",
    "        use_cbam=config['use_cbam'],\n",
    "        use_crossformer=config['use_crossformer'],\n",
    "        use_contrast=config['use_contrast'],\n",
    "        use_dim_reduction=config['use_dim_reduction'],\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def train_one_task(\n",
    "    task_name,\n",
    "    num_classes,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    config,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    한 task(branch/static/loco)에 대해:\n",
    "    - 모델 초기화\n",
    "    - optimizer/scheduler\n",
    "    - 에폭 반복하며 best ckpt 추적\n",
    "    - best ckpt로 test 평가\n",
    "    \"\"\"\n",
    "    model = build_model_for_task(\n",
    "        num_classes=num_classes,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        initial_prototypes=None\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['LEARNING_RATE'],\n",
    "        weight_decay=config['WEIGHT_DECAY']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['EPOCHS']\n",
    "    )\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        train_loss, train_ce, train_ctr, train_acc, train_f1 = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            use_contrast=config['use_contrast'],\n",
    "            contrast_weight=config['contrast_weight']\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"[{task_name}] Epoch {epoch+1:03d}/{config['EPOCHS']:03d} \"\n",
    "                  f\"TrainLoss={train_loss:.4f} \"\n",
    "                  f\"TrainAcc={train_acc:.4f} \"\n",
    "                  f\"ValAcc={val_acc:.4f}\")\n",
    "\n",
    "    # best ckpt 로드 후 test\n",
    "    assert best_state is not None, \"No best state saved.\"\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "        model,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[{task_name}] Done!\")\n",
    "    print(f\"  best val acc: {best_val_acc:.4f} @ epoch {best_epoch}\")\n",
    "    print(f\"  final test   : acc={test_acc:.4f} f1={test_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'task': task_name,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_preds': test_preds,\n",
    "        'test_labels': test_labels,\n",
    "        'best_state': best_state,\n",
    "    }\n",
    "\n",
    "def init_trained_model(num_classes, config, device, state_dict):\n",
    "    model = build_model_for_task(\n",
    "        num_classes=num_classes,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        initial_prototypes=None\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_on_indices(model, base_dataset, indices, device, batch_size=256):\n",
    "    \"\"\"\n",
    "    indices에 해당하는 window들만 추론.\n",
    "    return: dict[idx] = pred_label(int)\n",
    "    \"\"\"\n",
    "    ds = IndexOnlyXDataset(base_dataset, indices)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    out = {}\n",
    "    for xb, idx_original in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        for i, raw_idx in enumerate(idx_original.numpy()):\n",
    "            out[int(raw_idx)] = int(preds[i])\n",
    "    return out\n",
    "\n",
    "def majority_smooth(labels, k=5):\n",
    "    \"\"\"\n",
    "    단순 majority smoothing (슬라이딩 윈도우 mode filter)\n",
    "    labels: (N,)\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    N = len(labels)\n",
    "    r = k // 2\n",
    "    smoothed = np.empty_like(labels)\n",
    "    for i in range(N):\n",
    "        s = max(0, i-r)\n",
    "        e = min(N, i+r+1)\n",
    "        win = labels[s:e]\n",
    "        binc = np.bincount(win)\n",
    "        smoothed[i] = np.argmax(binc)\n",
    "    return smoothed\n",
    "\n",
    "def assemble_and_evaluate_final(\n",
    "    test_dataset: WISDMDataset,\n",
    "    branch_results,\n",
    "    static_results,\n",
    "    loco_results,\n",
    "    config,\n",
    "    device,\n",
    "    smooth_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    2레벨 파이프라인으로 최종 6-class 예측:\n",
    "    1) branch model이 전 윈도우를 static vs loco로 나눔\n",
    "    2) static idx는 static model로 Sitting/Standing\n",
    "       loco   idx는 loco   model로 Walk/Jog/Up/Down\n",
    "    3) 합쳐서 최종 [0..5] 라벨로 복원\n",
    "    4) smoothing 후 성능 보고\n",
    "    \"\"\"\n",
    "    # 1. best state 로드\n",
    "    branch_model = init_trained_model(\n",
    "        num_classes=2,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        state_dict=branch_results['best_state']\n",
    "    )\n",
    "    static_model = init_trained_model(\n",
    "        num_classes=2,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        state_dict=static_results['best_state']\n",
    "    )\n",
    "    loco_model = init_trained_model(\n",
    "        num_classes=4,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        state_dict=loco_results['best_state']\n",
    "    )\n",
    "\n",
    "    N = len(test_dataset)\n",
    "    all_idx = np.arange(N)\n",
    "\n",
    "    # branch 예측: 0=locomotion, 1=static\n",
    "    branch_pred_map = predict_on_indices(branch_model, test_dataset,\n",
    "                                         all_idx, device,\n",
    "                                         batch_size=config['BATCH_SIZE'])\n",
    "    branch_pred_arr = np.array([branch_pred_map[i] for i in range(N)], dtype=int)\n",
    "\n",
    "    pred_static_idx = np.where(branch_pred_arr == 1)[0]\n",
    "    pred_loco_idx   = np.where(branch_pred_arr == 0)[0]\n",
    "\n",
    "    # static 예측: 0 -> Sitting(4), 1 -> Standing(5)\n",
    "    static_pred_map = predict_on_indices(static_model, test_dataset,\n",
    "                                         pred_static_idx, device,\n",
    "                                         batch_size=config['BATCH_SIZE'])\n",
    "    # loco 예측:   0->Walking(0),1->Jogging(1),2->Upstairs(2),3->Downstairs(3)\n",
    "    loco_pred_map   = predict_on_indices(loco_model, test_dataset,\n",
    "                                         pred_loco_idx, device,\n",
    "                                         batch_size=config['BATCH_SIZE'])\n",
    "\n",
    "    final_pred = np.full(N, fill_value=-1, dtype=int)\n",
    "    for idx in pred_static_idx:\n",
    "        sp = static_pred_map[idx]  # 0 or 1\n",
    "        final_pred[idx] = 4 if sp == 0 else 5\n",
    "    for idx in pred_loco_idx:\n",
    "        lp = loco_pred_map[idx]    # 0..3 already match 0..3\n",
    "        final_pred[idx] = lp\n",
    "\n",
    "    y_true = test_dataset.y_full.copy()\n",
    "\n",
    "    final_pred_smooth = majority_smooth(final_pred, k=smooth_k)\n",
    "\n",
    "    acc_raw = accuracy_score(y_true, final_pred)\n",
    "    f1_raw  = f1_score(y_true, final_pred, average='weighted')\n",
    "    acc_sm  = accuracy_score(y_true, final_pred_smooth)\n",
    "    f1_sm   = f1_score(y_true, final_pred_smooth, average='weighted')\n",
    "\n",
    "    target_names = ['Walking','Jogging','Upstairs','Downstairs','Sitting','Standing']\n",
    "\n",
    "    print(\"\\n================ FINAL (6-class) ================\")\n",
    "    print(f\"Raw     : Acc={acc_raw:.4f}, F1(w)={f1_raw:.4f}\")\n",
    "    print(f\"Smoothed: Acc={acc_sm:.4f}, F1(w)={f1_sm:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Smoothed):\")\n",
    "    print(classification_report(y_true, final_pred_smooth,\n",
    "                                target_names=target_names, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, final_pred_smooth, labels=[0,1,2,3,4,5])\n",
    "    plt.figure(figsize=(6,5))\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.colorbar(im)\n",
    "    plt.xticks(range(6), target_names, rotation=45, ha='right')\n",
    "    plt.yticks(range(6), target_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Smoothed)')\n",
    "    # annotate\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix_final.png\", dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved confusion_matrix_final.png\")\n",
    "\n",
    "    return {\n",
    "        'acc_raw': acc_raw,\n",
    "        'f1_raw': f1_raw,\n",
    "        'acc_smooth': acc_sm,\n",
    "        'f1_smooth': f1_sm,\n",
    "        'y_true': y_true,\n",
    "        'pred_raw': final_pred,\n",
    "        'pred_smooth': final_pred_smooth,\n",
    "        'cm': cm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d680cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading WISDM Dataset from: C://Users/park9/CBAM_HAR/WISDM\n",
      "Users per split: 22 6 8\n",
      "[WISDMDataset] X=(12792, 3, 200) full=(12792,) branch=(12792,) static=(12792,) loco=(12792,)\n",
      "[WISDMDataset] X=(3791, 3, 200) full=(3791,) branch=(3791,) static=(3791,) loco=(3791,)\n",
      "[WISDMDataset] X=(4514, 3, 200) full=(4514,) branch=(4514,) static=(4514,) loco=(4514,)\n",
      "Windows per split: train=12792, val=3791, test=4514\n",
      "[branch] train/val/test = 12792, 3791, 4514\n",
      "[static] train/val/test = 1497, 216, 288\n",
      "[loco]   train/val/test = 11295, 3575, 4226\n",
      "[Subset sizes] branch_train=12792, static_train=1497, loco_train=11295\n",
      "✔ Dataloaders prepared (branch/static/loco).\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 010/100 TrainLoss=0.0001 TrainAcc=1.0000 ValAcc=0.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 020/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 030/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 040/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 050/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 060/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 070/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 080/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 090/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch] Epoch 100/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.9926\n",
      "\n",
      "[branch] Done!\n",
      "  best val acc: 0.9987 @ epoch 69\n",
      "  final test   : acc=0.9967 f1=0.9967\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 010/100 TrainLoss=0.0426 TrainAcc=0.9947 ValAcc=0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 020/100 TrainLoss=0.0068 TrainAcc=0.9987 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 030/100 TrainLoss=0.0073 TrainAcc=0.9980 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 040/100 TrainLoss=0.0056 TrainAcc=0.9980 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 050/100 TrainLoss=0.0026 TrainAcc=0.9993 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 060/100 TrainLoss=0.0013 TrainAcc=0.9993 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 070/100 TrainLoss=0.0048 TrainAcc=0.9987 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 080/100 TrainLoss=0.0014 TrainAcc=1.0000 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 090/100 TrainLoss=0.0012 TrainAcc=0.9993 ValAcc=0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[static] Epoch 100/100 TrainLoss=0.0007 TrainAcc=1.0000 ValAcc=0.8241\n",
      "\n",
      "[static] Done!\n",
      "  best val acc: 0.8241 @ epoch 1\n",
      "  final test   : acc=0.9757 f1=0.9757\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 010/100 TrainLoss=0.0110 TrainAcc=0.9979 ValAcc=0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 020/100 TrainLoss=0.0021 TrainAcc=0.9995 ValAcc=0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 030/100 TrainLoss=0.0004 TrainAcc=1.0000 ValAcc=0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 040/100 TrainLoss=0.0019 TrainAcc=0.9996 ValAcc=0.8629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 050/100 TrainLoss=0.0001 TrainAcc=1.0000 ValAcc=0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 060/100 TrainLoss=0.0001 TrainAcc=1.0000 ValAcc=0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 070/100 TrainLoss=0.0002 TrainAcc=1.0000 ValAcc=0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 080/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.8674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 090/100 TrainLoss=0.0001 TrainAcc=1.0000 ValAcc=0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loco] Epoch 100/100 TrainLoss=0.0000 TrainAcc=1.0000 ValAcc=0.8671\n",
      "\n",
      "[loco] Done!\n",
      "  best val acc: 0.8769 @ epoch 45\n",
      "  final test   : acc=0.8157 f1=0.8261\n",
      "\n",
      "All three tasks trained!\n",
      "branch: 0.9966770048737262 0.9967162209141439\n",
      "static: 0.9756944444444444 0.9756882900263258\n",
      "loco  : 0.8156649313771889 0.8261417879991512\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n",
      ">>> [Temporary Model or No Init Provided] Prototypes initialized with Xavier Uniform.\n",
      "\n",
      "================ FINAL (6-class) ================\n",
      "Raw     : Acc=0.8252, F1(w)=0.8344\n",
      "Smoothed: Acc=0.8299, F1(w)=0.8383\n",
      "\n",
      "Classification Report (Smoothed):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Walking     0.9636    0.7498    0.8434      2086\n",
      "     Jogging     0.9295    1.0000    0.9635      1306\n",
      "    Upstairs     0.5007    0.7351    0.5957       487\n",
      "  Downstairs     0.5021    0.6801    0.5777       347\n",
      "     Sitting     0.9595    1.0000    0.9793       142\n",
      "    Standing     0.9150    0.9589    0.9365       146\n",
      "\n",
      "    accuracy                         0.8299      4514\n",
      "   macro avg     0.7951    0.8540    0.8160      4514\n",
      "weighted avg     0.8667    0.8299    0.8383      4514\n",
      "\n",
      "Saved confusion_matrix_final.png\n",
      "\n",
      "Final (6-class) Weighted F1 after smoothing: 0.8383\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    config = {\n",
    "        'DATA_DIR': 'C://Users/park9/CBAM_HAR/WISDM',\n",
    "        'BATCH_SIZE': 256,\n",
    "        'EPOCHS': 100,\n",
    "        'SEED': 42,\n",
    "        'LEARNING_RATE': 5e-4,\n",
    "        'WEIGHT_DECAY': 1e-2,\n",
    "\n",
    "        'in_channels': 3,\n",
    "        'seq_len': 200,\n",
    "        'step_size': 50,\n",
    "\n",
    "        'embed_dim': 64,\n",
    "        'reduced_dim': 32,\n",
    "        'n_heads': 8,\n",
    "        'kernel_size': 13,\n",
    "        'dropout': 0.1,\n",
    "\n",
    "        'use_cbam': True,\n",
    "        'use_crossformer': True,\n",
    "        'use_contrast': True,\n",
    "        'use_dim_reduction': False,\n",
    "\n",
    "        'temperature': 0.05,\n",
    "        'contrast_weight': 0.35,\n",
    "    }\n",
    "\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    seed_everything(config['SEED'])\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Loading WISDM Dataset from: {config['DATA_DIR']}\")\n",
    "\n",
    "    # 1. raw 로드\n",
    "    full_df = load_full_dataframe(\n",
    "        os.path.join(config['DATA_DIR'], 'WISDM_ar_v1.1_raw.txt')\n",
    "    )\n",
    "\n",
    "    # 2. subject-wise split\n",
    "    all_users = full_df['user'].unique()\n",
    "    train_val_users, test_users = train_test_split(\n",
    "        all_users, test_size=0.2, random_state=config['SEED']\n",
    "    )\n",
    "    train_users, val_users = train_test_split(\n",
    "        train_val_users, test_size=0.2, random_state=config['SEED']\n",
    "    )\n",
    "\n",
    "    train_df = full_df[full_df['user'].isin(train_users)].copy()\n",
    "    val_df   = full_df[full_df['user'].isin(val_users)].copy()\n",
    "    test_df  = full_df[full_df['user'].isin(test_users)].copy()\n",
    "    print(\"Users per split:\",\n",
    "          len(train_users), len(val_users), len(test_users))\n",
    "\n",
    "    # 3. scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[['x','y','z']])\n",
    "    for df_ in [train_df, val_df, test_df]:\n",
    "        df_[['x','y','z']] = scaler.transform(df_[['x','y','z']])\n",
    "\n",
    "    # 4. window → base dataset\n",
    "    train_dataset = WISDMDataset(train_df,\n",
    "                                 window_size=config['seq_len'],\n",
    "                                 step_size=config['step_size'])\n",
    "    val_dataset   = WISDMDataset(val_df,\n",
    "                                 window_size=config['seq_len'],\n",
    "                                 step_size=config['step_size'])\n",
    "    test_dataset  = WISDMDataset(test_df,\n",
    "                                 window_size=config['seq_len'],\n",
    "                                 step_size=config['step_size'])\n",
    "    print(f\"Windows per split: train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}\")\n",
    "\n",
    "    # 5. 인덱스 분리\n",
    "    train_idx_branch = np.arange(len(train_dataset))\n",
    "    val_idx_branch   = np.arange(len(val_dataset))\n",
    "    test_idx_branch  = np.arange(len(test_dataset))\n",
    "\n",
    "    train_idx_static = np.where(train_dataset.y_static != -1)[0]\n",
    "    val_idx_static   = np.where(val_dataset.y_static != -1)[0]\n",
    "    test_idx_static  = np.where(test_dataset.y_static != -1)[0]\n",
    "\n",
    "    train_idx_loco = np.where(train_dataset.y_loco != -1)[0]\n",
    "    val_idx_loco   = np.where(val_dataset.y_loco != -1)[0]\n",
    "    test_idx_loco  = np.where(test_dataset.y_loco != -1)[0]\n",
    "\n",
    "    print(f\"[branch] train/val/test = {len(train_idx_branch)}, {len(val_idx_branch)}, {len(test_idx_branch)}\")\n",
    "    print(f\"[static] train/val/test = {len(train_idx_static)}, {len(val_idx_static)}, {len(test_idx_static)}\")\n",
    "    print(f\"[loco]   train/val/test = {len(train_idx_loco)}, {len(val_idx_loco)}, {len(test_idx_loco)}\")\n",
    "\n",
    "    # 6. SubsetForTask\n",
    "    train_branch_ds = SubsetForTask(train_dataset, train_idx_branch, task='branch')\n",
    "    val_branch_ds   = SubsetForTask(val_dataset,   val_idx_branch,   task='branch')\n",
    "    test_branch_ds  = SubsetForTask(test_dataset,  test_idx_branch,  task='branch')\n",
    "\n",
    "    train_static_ds = SubsetForTask(train_dataset, train_idx_static, task='static')\n",
    "    val_static_ds   = SubsetForTask(val_dataset,   val_idx_static,   task='static')\n",
    "    test_static_ds  = SubsetForTask(test_dataset,  test_idx_static,  task='static')\n",
    "\n",
    "    train_loco_ds   = SubsetForTask(train_dataset, train_idx_loco,   task='loco')\n",
    "    val_loco_ds     = SubsetForTask(val_dataset,   val_idx_loco,     task='loco')\n",
    "    test_loco_ds    = SubsetForTask(test_dataset,  test_idx_loco,    task='loco')\n",
    "\n",
    "    print(f\"[Subset sizes] branch_train={len(train_branch_ds)}, static_train={len(train_static_ds)}, loco_train={len(train_loco_ds)}\")\n",
    "\n",
    "    # 7. DataLoaders\n",
    "    BATCH = config['BATCH_SIZE']\n",
    "    train_branch_loader = make_loader(train_branch_ds, BATCH, oversample=True)\n",
    "    val_branch_loader   = make_loader(val_branch_ds,   BATCH, oversample=False, shuffle=False)\n",
    "    test_branch_loader  = make_loader(test_branch_ds,  BATCH, oversample=False, shuffle=False)\n",
    "\n",
    "    train_static_loader = make_loader(train_static_ds, BATCH, oversample=True)\n",
    "    val_static_loader   = make_loader(val_static_ds,   BATCH, oversample=False, shuffle=False)\n",
    "    test_static_loader  = make_loader(test_static_ds,  BATCH, oversample=False, shuffle=False)\n",
    "\n",
    "    train_loco_loader   = make_loader(train_loco_ds,   BATCH, oversample=True)\n",
    "    val_loco_loader     = make_loader(val_loco_ds,     BATCH, oversample=False, shuffle=False)\n",
    "    test_loco_loader    = make_loader(test_loco_ds,    BATCH, oversample=False, shuffle=False)\n",
    "\n",
    "    print(\"✔ Dataloaders prepared (branch/static/loco).\")\n",
    "\n",
    "    # 8. task별 학습\n",
    "    branch_results = train_one_task(\n",
    "        task_name=\"branch\",\n",
    "        num_classes=2,\n",
    "        train_loader=train_branch_loader,\n",
    "        val_loader=val_branch_loader,\n",
    "        test_loader=test_branch_loader,\n",
    "        config=config,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    static_results = train_one_task(\n",
    "        task_name=\"static\",\n",
    "        num_classes=2,\n",
    "        train_loader=train_static_loader,\n",
    "        val_loader=val_static_loader,\n",
    "        test_loader=test_static_loader,\n",
    "        config=config,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    loco_results = train_one_task(\n",
    "        task_name=\"loco\",\n",
    "        num_classes=4,\n",
    "        train_loader=train_loco_loader,\n",
    "        val_loader=val_loco_loader,\n",
    "        test_loader=test_loco_loader,\n",
    "        config=config,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    print(\"\\nAll three tasks trained!\")\n",
    "    print(\"branch:\", branch_results['test_acc'], branch_results['test_f1'])\n",
    "    print(\"static:\", static_results['test_acc'], static_results['test_f1'])\n",
    "    print(\"loco  :\", loco_results['test_acc'],   loco_results['test_f1'])\n",
    "\n",
    "    # 9. 최종 6-class 조립 + smoothing 평가\n",
    "    final_pack = assemble_and_evaluate_final(\n",
    "        test_dataset=test_dataset,\n",
    "        branch_results=branch_results,\n",
    "        static_results=static_results,\n",
    "        loco_results=loco_results,\n",
    "        config=config,\n",
    "        device=DEVICE,\n",
    "        smooth_k=5\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal (6-class) Weighted F1 after smoothing:\",\n",
    "            f\"{final_pack['f1_smooth']:.4f}\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
