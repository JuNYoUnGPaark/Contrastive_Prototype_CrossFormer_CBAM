{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5f6ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Class counts (before): {'Walking': 267687, 'Jogging': 215184, 'Upstairs': 78685, 'Downstairs': 64413, 'Sitting': 38298, 'Standing': 31070}\n",
      "Class counts (after): {'Walking': 267687, 'Downstairs': 267687, 'Upstairs': 267687, 'Standing': 267687, 'Sitting': 267687, 'Jogging': 267687}\n",
      "Shapes: (15795, 3, 200) (15795,) (1472, 3, 200) (1472,) (1901, 3, 200) (1901,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010/080] TrainAcc=0.9314 ValAcc=0.9355 ValF1=0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[020/080] TrainAcc=0.9744 ValAcc=0.9694 ValF1=0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[030/080] TrainAcc=0.9837 ValAcc=0.9796 ValF1=0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[040/080] TrainAcc=0.9918 ValAcc=0.9721 ValF1=0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[050/080] TrainAcc=0.9948 ValAcc=0.9864 ValF1=0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[060/080] TrainAcc=0.9947 ValAcc=0.9891 ValF1=0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[070/080] TrainAcc=0.9975 ValAcc=0.9878 ValF1=0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[080/080] TrainAcc=0.9955 ValAcc=0.9857 ValF1=0.9858\n",
      "\n",
      "================= RESULT =================\n",
      "Best Val Acc: 0.9946 @ epoch 68\n",
      "Test Acc    : 0.9963\n",
      "Test F1(w)  : 0.9963\n",
      "Classif Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Walking     0.9974    0.9962    0.9968       783\n",
      "     Jogging     1.0000    1.0000    1.0000       625\n",
      "    Upstairs     0.9799    1.0000    0.9898       195\n",
      "  Downstairs     0.9932    0.9735    0.9833       151\n",
      "     Sitting     1.0000    1.0000    1.0000        86\n",
      "    Standing     1.0000    1.0000    1.0000        61\n",
      "\n",
      "    accuracy                         0.9963      1901\n",
      "   macro avg     0.9951    0.9949    0.9950      1901\n",
      "weighted avg     0.9963    0.9963    0.9963      1901\n",
      "\n",
      "Saved cm_simple_balanced.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# 0. 재현성\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =========================================================\n",
    "# 1. 데이터 로드 & 전처리\n",
    "# =========================================================\n",
    "ACTIVITY_TO_ID = {\n",
    "    'Walking': 0,\n",
    "    'Jogging': 1,\n",
    "    'Upstairs': 2,\n",
    "    'Downstairs': 3,\n",
    "    'Sitting': 4,\n",
    "    'Standing': 5,\n",
    "}\n",
    "ID_TO_ACTIVITY = {v:k for k,v in ACTIVITY_TO_ID.items()}\n",
    "\n",
    "def load_wisdm_raw(path_txt: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    WISDM_ar_v1.1_raw.txt 를 DataFrame으로 로드\n",
    "    columns: user, activity, timestamp, x, y, z\n",
    "    \"\"\"\n",
    "    cols = [\"user\",\"activity\",\"timestamp\",\"x\",\"y\",\"z\"]\n",
    "    df = pd.read_csv(path_txt, header=None, names=cols, on_bad_lines='skip')\n",
    "    # z 컬럼에 붙은 ; 제거\n",
    "    df[\"z\"] = pd.to_numeric(df[\"z\"].astype(str).str.replace(\";\",\"\", regex=False),\n",
    "                            errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def balance_by_oversampling(df: pd.DataFrame,\n",
    "                            per_class_target: Dict[str,int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    간단 수동 오버샘플링/언더샘플링:\n",
    "      - 각 activity별로 원하는 개수(per_class_target[act])만큼 행을 뽑는다.\n",
    "      - 만약 원본이 적으면 반복 복제해서 늘린다.\n",
    "      - 많으면 앞에서 자른다 (언더샘플링 느낌).\n",
    "    결과적으로 클래스별 개수를 비슷하게 맞춘 balanced df 리턴.\n",
    "\n",
    "    per_class_target 예:\n",
    "      {'Walking':20000, 'Jogging':20000, 'Upstairs':20000,\n",
    "       'Downstairs':20000, 'Sitting':20000, 'Standing':20000}\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for act, target_n in per_class_target.items():\n",
    "        sub = df[df['activity']==act]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        if len(sub) >= target_n:\n",
    "            dfs.append(sub.sample(n=target_n, random_state=42))\n",
    "        else:\n",
    "            # 부족하면 반복 replicate\n",
    "            reps = target_n // len(sub) + 1\n",
    "            sub_rep = pd.concat([sub]*reps, ignore_index=True)\n",
    "            dfs.append(sub_rep.sample(n=target_n, random_state=42))\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    out = out.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def make_sliding_windows(df: pd.DataFrame,\n",
    "                         window_size=200,\n",
    "                         step_size=20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    사용자/액티비티별로 구간을 끊고\n",
    "    (x,y,z)를 window_size 길이로 슬라이싱, step_size stride로 밀어가며 윈도우를 만든다.\n",
    "    X_out: (N, 3, T)\n",
    "    y_out: (N,)\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for (user, act), g in df.groupby(['user','activity']):\n",
    "        sig = g[['x','y','z']].values.astype(np.float32)  # (L,3)\n",
    "        label = ACTIVITY_TO_ID[act]\n",
    "\n",
    "        L = len(sig)\n",
    "        for start in range(0, L - window_size, step_size):\n",
    "            chunk = sig[start:start+window_size]  # (T,3)\n",
    "            X_list.append(chunk.T)               # (3,T)\n",
    "            y_list.append(label)\n",
    "\n",
    "    X_out = np.stack(X_list)                    # (N,3,T)\n",
    "    y_out = np.array(y_list, dtype=np.int64)    # (N,)\n",
    "    return X_out, y_out\n",
    "\n",
    "# =========================================================\n",
    "# 2. Dataset / Dataloader\n",
    "# =========================================================\n",
    "class SimpleWISDMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])\n",
    "        y = torch.tensor(self.y[idx])\n",
    "        return x, y\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle=True):\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      drop_last=False)\n",
    "\n",
    "# =========================================================\n",
    "# 3. 모델 (너 backbone 단일 헤드 버전)\n",
    "#    여기선 contrast 끄고, 그냥 CE만 쓰는 버전으로 단순화\n",
    "# =========================================================\n",
    "class CBAMChannel(nn.Module):\n",
    "    def __init__(self, c, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c, c//reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c//reduction, c, bias=False)\n",
    "        )\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,T)\n",
    "        avg_out = self.fc(self.avg(x).squeeze(-1))\n",
    "        max_out = self.fc(self.max(x).squeeze(-1))\n",
    "        attn = self.sig((avg_out+max_out).unsqueeze(-1))\n",
    "        return x * attn\n",
    "\n",
    "class CBAMTemporal(nn.Module):\n",
    "    def __init__(self, k=7):\n",
    "        super().__init__()\n",
    "        pad = (k-1)//2\n",
    "        self.conv = nn.Conv1d(2,1,kernel_size=k,padding=pad,bias=False)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,T)\n",
    "        avg_out = torch.mean(x,dim=1,keepdim=True)\n",
    "        max_out,_ = torch.max(x,dim=1,keepdim=True)\n",
    "        a = torch.cat([avg_out,max_out], dim=1)    # (B,2,T)\n",
    "        a = self.conv(a)                           # (B,1,T)\n",
    "        a = self.sig(a)\n",
    "        return x * a\n",
    "\n",
    "class CBAM1D(nn.Module):\n",
    "    def __init__(self, c, reduction=16, k=7):\n",
    "        super().__init__()\n",
    "        self.ca = CBAMChannel(c, reduction)\n",
    "        self.ta = CBAMTemporal(k)\n",
    "    def forward(self,x):\n",
    "        x = self.ca(x)\n",
    "        x = self.ta(x)\n",
    "        return x\n",
    "\n",
    "class CrossFormerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        h = self.norm1(x)\n",
    "        attn_out,_ = self.attn(h,h,h)  # self-attn\n",
    "        x = x + attn_out\n",
    "        h = self.norm2(x)\n",
    "        x = x + self.mlp(h)\n",
    "        return x\n",
    "\n",
    "class SimpleHARNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 seq_len=200,\n",
    "                 embed_dim=64,\n",
    "                 kernel_size=13,\n",
    "                 dropout=0.1,\n",
    "                 n_heads=8,\n",
    "                 n_classes=6):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, embed_dim,\n",
    "                      kernel_size=kernel_size,\n",
    "                      padding=(kernel_size-1)//2),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.cbam = CBAM1D(embed_dim, reduction=8, k=kernel_size)\n",
    "\n",
    "        self.xformer = CrossFormerBlock(\n",
    "            dim=embed_dim,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,T)\n",
    "        x = self.conv(x)         # (B,embed_dim,T)\n",
    "        x = self.cbam(x)         # (B,embed_dim,T)\n",
    "\n",
    "        x = x.transpose(1,2)     # (B,T,embed_dim)\n",
    "        x = self.xformer(x)      # (B,T,embed_dim)\n",
    "        x = x.transpose(1,2)     # (B,embed_dim,T)\n",
    "\n",
    "        x = self.pool(x).squeeze(-1)  # (B,embed_dim)\n",
    "        logits = self.cls(x)          # (B,n_classes)\n",
    "        return logits\n",
    "\n",
    "# =========================================================\n",
    "# 4. 학습 / 평가 루프\n",
    "# =========================================================\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(loader, desc=\"train\", leave=False):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(yb.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return avg_loss, acc, f1, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# =========================================================\n",
    "# 5. main\n",
    "# =========================================================\n",
    "def main():\n",
    "    config = {\n",
    "        'DATA_PATH': 'C://Users/park9/CBAM_HAR/WISDM/WISDM_ar_v1.1_raw.txt',\n",
    "        'SEED': 42,\n",
    "        'TEST_SIZE': 0.2,     # random split ratio\n",
    "        'VAL_SIZE': 0.2,      # from train portion\n",
    "        'WINDOW_SIZE': 200,\n",
    "        'STEP_SIZE': 100,\n",
    "        'BATCH': 128,\n",
    "        'EPOCHS': 80,\n",
    "        'LR': 5e-4,\n",
    "        'WEIGHT_DECAY': 1e-2,\n",
    "        'EMBED_DIM': 64,\n",
    "        'KERNEL_SIZE': 13,\n",
    "        'N_HEADS': 8,\n",
    "        'DROPOUT': 0.1,\n",
    "    }\n",
    "\n",
    "    seed_everything(config['SEED'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 1) 원본 로드\n",
    "    df_raw = load_wisdm_raw(config['DATA_PATH'])\n",
    "\n",
    "    # 2) 스케일링 (train 전에 fit 해야 하므로 잠깐 분할 필요)\n",
    "    #    여기서는 아직 subject-wise 안 하니까 그냥 랜덤 train/test 먼저 자르고,\n",
    "    #    나중에 train에서 fit한 scaler로 다시 전체 적용해도 돼.\n",
    "    df_train_full, df_test = train_test_split(\n",
    "        df_raw, test_size=config['TEST_SIZE'], shuffle=True, random_state=config['SEED']\n",
    "    )\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_train_full, test_size=config['VAL_SIZE'], shuffle=True, random_state=config['SEED']\n",
    "    )\n",
    "\n",
    "    # fit scaler on train only\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[['x','y','z']])\n",
    "\n",
    "    for d in [df_train, df_val, df_test]:\n",
    "        d[['x','y','z']] = scaler.transform(d[['x','y','z']])\n",
    "\n",
    "    # 3) 수동 오버샘플링으로 train 밸런스 맞추기\n",
    "    #    전략: 각 클래스마다 동일한 타깃 수로 맞춘다.\n",
    "    #    타깃 수는 train 안에서 가장 많은 클래스 count를 기준으로 골라도 되고,\n",
    "    #    혹은 적당히 20000 이런 식으로 고정해도 된다.\n",
    "    counts = df_train['activity'].value_counts().to_dict()\n",
    "    max_count = max(counts.values())\n",
    "    target_dict = {act: max_count for act in ACTIVITY_TO_ID.keys()}\n",
    "\n",
    "    df_train_bal = balance_by_oversampling(df_train, target_dict)\n",
    "\n",
    "    print(\"Class counts (before):\", counts)\n",
    "    print(\"Class counts (after):\", df_train_bal['activity'].value_counts().to_dict())\n",
    "\n",
    "    # 4) 윈도우 슬라이싱 (train_bal / val / test)\n",
    "    X_train, y_train = make_sliding_windows(\n",
    "        df_train_bal,\n",
    "        window_size=config['WINDOW_SIZE'],\n",
    "        step_size=config['STEP_SIZE']\n",
    "    )\n",
    "    X_val, y_val = make_sliding_windows(\n",
    "        df_val,\n",
    "        window_size=config['WINDOW_SIZE'],\n",
    "        step_size=config['STEP_SIZE']\n",
    "    )\n",
    "    X_test, y_test = make_sliding_windows(\n",
    "        df_test,\n",
    "        window_size=config['WINDOW_SIZE'],\n",
    "        step_size=config['STEP_SIZE']\n",
    "    )\n",
    "\n",
    "    print(\"Shapes:\",\n",
    "          X_train.shape, y_train.shape,\n",
    "          X_val.shape, y_val.shape,\n",
    "          X_test.shape, y_test.shape)\n",
    "\n",
    "    train_ds = SimpleWISDMDataset(X_train, y_train)\n",
    "    val_ds   = SimpleWISDMDataset(X_val,   y_val)\n",
    "    test_ds  = SimpleWISDMDataset(X_test,  y_test)\n",
    "\n",
    "    train_loader = make_loader(train_ds, config['BATCH'], shuffle=True)\n",
    "    val_loader   = make_loader(val_ds,   config['BATCH'], shuffle=False)\n",
    "    test_loader  = make_loader(test_ds,  config['BATCH'], shuffle=False)\n",
    "\n",
    "    # 5) 모델 초기화\n",
    "    model = SimpleHARNet(\n",
    "        in_channels=3,\n",
    "        seq_len=config['WINDOW_SIZE'],\n",
    "        embed_dim=config['EMBED_DIM'],\n",
    "        kernel_size=config['KERNEL_SIZE'],\n",
    "        dropout=config['DROPOUT'],\n",
    "        n_heads=config['N_HEADS'],\n",
    "        n_classes=6\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['LR'],\n",
    "        weight_decay=config['WEIGHT_DECAY']\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch+1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"[{epoch+1:03d}/{config['EPOCHS']:03d}] \"\n",
    "                  f\"TrainAcc={tr_acc:.4f} ValAcc={val_acc:.4f} ValF1={val_f1:.4f}\")\n",
    "\n",
    "    # 6) best로 테스트\n",
    "    model.load_state_dict(best_state)\n",
    "    _, test_acc, test_f1, test_preds, test_labels = eval_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"\\n================= RESULT =================\")\n",
    "    print(f\"Best Val Acc: {best_val_acc:.4f} @ epoch {best_epoch}\")\n",
    "    print(f\"Test Acc    : {test_acc:.4f}\")\n",
    "    print(f\"Test F1(w)  : {test_f1:.4f}\")\n",
    "    print(\"Classif Report:\")\n",
    "    print(classification_report(test_labels, test_preds,\n",
    "                                target_names=[ID_TO_ACTIVITY[i] for i in range(6)],\n",
    "                                digits=4))\n",
    "\n",
    "    cm = confusion_matrix(test_labels, test_preds, labels=[0,1,2,3,4,5])\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    ticks = [ID_TO_ACTIVITY[i] for i in range(6)]\n",
    "    plt.xticks(range(6), ticks, rotation=45, ha='right')\n",
    "    plt.yticks(range(6), ticks)\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            plt.text(j,i,cm[i,j],ha='center',va='center',fontsize=8,color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cm_simple_balanced.png\", dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved cm_simple_balanced.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
