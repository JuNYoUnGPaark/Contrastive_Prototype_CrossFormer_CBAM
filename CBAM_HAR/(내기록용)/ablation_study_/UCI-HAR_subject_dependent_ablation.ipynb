{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7340188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-1. Library Import \n",
    "# =================================================================================\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fvcore (optional)\n",
    "try:\n",
    "    from fvcore.nn import FlopCountAnalysis\n",
    "    FVCORE_AVAILABLE = True\n",
    "except Exception:\n",
    "    FlopCountAnalysis = None\n",
    "    FVCORE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9262ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-2. Define CONFIG \n",
    "# =================================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # ---------------------------\n",
    "    # Reproducibility / Experiment ID\n",
    "    # ---------------------------\n",
    "    \"seed\": 42,\n",
    "    \"dataset_name\": \"UCI-HAR\",\n",
    "\n",
    "    # ---------------------------\n",
    "    # Data / Split\n",
    "    # ---------------------------\n",
    "    \"data_dir\": \"C://Users/park9/CBAM_HAR/UCI-HAR/data\", \n",
    "    \"val_ratio\": 0.2,  # UCI-HAR: train split -> train/val portion\n",
    "    \"batch_size\": 128,  # dataloader\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training\n",
    "    # ---------------------------\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"contrast_weight\": 0.25,  # total_loss = CE + contrast_weight * contrast_loss\n",
    "    \"scheduler_type\": \"cosine\", # scheduler (CosineAnnealingLR)\n",
    "    \"scheduler_T_max\": 100,  # CosineAnnealingLR(T_max=EPOCHS)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Model architecture (UCI-HAR)\n",
    "    # ---------------------------\n",
    "    \"in_channels\": 9,        # 9 inertial signals (acc/gyro/total xyz)\n",
    "    \"seq_len\": 128,          # window length\n",
    "    \"n_classes\": 6,          # HAR classes\n",
    "    \"n_prototypes\": 6,       # prototype counts\n",
    "\n",
    "    \"embed_dim\": 64,         # Conv1d -> Transformer base dim\n",
    "    \"reduced_dim\": 32,       # if we use_dim_reduction == True\n",
    "    \"use_dim_reduction\": False,\n",
    "\n",
    "    \"n_heads\": 8,            # multi-head attention heads in CrossFormer block\n",
    "    \"dropout\": 0.1,\n",
    "\n",
    "    \"kernel_size\": 11,       # kernel sizes used in Conv1d embedding and CBAM temporal attention\n",
    "\n",
    "    # ---------------------------\n",
    "    # Feature toggles (ablations)\n",
    "    # ---------------------------\n",
    "    \"use_cbam\": True,        # CBAM\n",
    "    \"use_crossformer\": True, # CrossFormerBlock\n",
    "    \"use_contrast\": True,    # include contrastive prototype loss during training\n",
    "\n",
    "    # ---------------------------\n",
    "    # Contrast / Prototype behavior\n",
    "    # ---------------------------\n",
    "    \"temperature\": 0.05,     # temperature in contrastive loss\n",
    "\n",
    "    # ---------------------------\n",
    "    # Logging / Debug convenience\n",
    "    # ---------------------------\n",
    "    \"print_every\": 25,        # print every or bumped epoch\n",
    "    \"do_tsne\": True,        # t-SNE\n",
    "    \"profile_model\": True,  # FLOPs / Params\n",
    "}\n",
    "\n",
    "def pretty_print_config(cfg: dict):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT CONFIG\")\n",
    "    print(\"-\" * 80)\n",
    "    # key alignment\n",
    "    max_k = max(len(k) for k in cfg.keys())\n",
    "    for k in sorted(cfg.keys()):\n",
    "        print(f\"{k.ljust(max_k)} : {cfg[k]}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8cbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-3. Reproducibility helpers\n",
    "# =================================================================================\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Fix random seeds for reproducibility across random, numpy, torch (cpu & cuda).\n",
    "    Also configures CuDNN for deterministic behavior.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cudnn deterministic mode: reproducible but may be slower\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    \"\"\"\n",
    "    To make DataLoader workers deterministic.\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae708afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-4. Dataset: UCI-HAR\n",
    "# =================================================================================\n",
    "class UCIHARDataset(Dataset):\n",
    "    \"\"\"\n",
    "    UCI HAR Dataset loader.\n",
    "\n",
    "    Directory structure (expected):\n",
    "        data_dir/\n",
    "          train/\n",
    "            Inertial Signals/\n",
    "              body_acc_x_train.txt\n",
    "              body_acc_y_train.txt\n",
    "              ...\n",
    "              total_acc_z_train.txt\n",
    "            y_train.txt\n",
    "          test/\n",
    "            Inertial Signals/\n",
    "              body_acc_x_test.txt\n",
    "              ...\n",
    "            y_test.txt\n",
    "\n",
    "    Notes:\n",
    "    - UCI-HAR은 이미 subject-wise로 train/test가 고정되어 제공됨.\n",
    "      즉 train/ 폴더와 test/ 폴더에 다른 사람들(subjects)이 들어 있음.\n",
    "    - 우리는 train/ 안에서만 val을 떼어내서 train/val로 쓰고,\n",
    "      test/는 그대로 최종 generalization 평가용으로 사용.\n",
    "    - 라벨은 원본이 1~6이라서 여기서 0~5로 shift.\n",
    "    - 시퀀스 길이 T=128, 채널 수 C=9 (가속도/자이로/total_acc 각각 x/y/z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, train: bool = True):\n",
    "        subset = \"train\" if train else \"test\"\n",
    "\n",
    "        signal_types = [\n",
    "            \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
    "            \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
    "            \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
    "        ]  # (N, 9, 128)\n",
    "\n",
    "        signals = []\n",
    "        for signal in signal_types:\n",
    "            filename = os.path.join(\n",
    "                data_dir,\n",
    "                subset,\n",
    "                \"Inertial Signals\",\n",
    "                f\"{signal}_{subset}.txt\"\n",
    "            )\n",
    "\n",
    "            with open(filename, 'r') as f:\n",
    "                data = np.loadtxt(f)  # (N, 128) per channel\n",
    "            signals.append(data)\n",
    "\n",
    "        self.X = np.stack(signals, axis=1)  # stack -> (N, 9, 128)\n",
    "\n",
    "        # label: (N,), values in {1..6} -> shift to {0..5}\n",
    "        label_file = os.path.join(data_dir, subset, f'y_{subset}.txt')\n",
    "        with open(label_file, 'r') as f:\n",
    "            self.y = np.loadtxt(f, dtype=np.int32) - 1  # now {0..5}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            X_i: torch.FloatTensor, shape (C, T) = (9, 128)\n",
    "            y_i: torch.LongTensor scalar, shape ()\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664cfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCIHARAllDataset(Dataset):\n",
    "    \"\"\"\n",
    "    UCI-HAR 전체(train+test)를 한 번에 불러와서 합친 버전.\n",
    "    subject-wise 분리를 유지하지 않고 윈도우 단위로 다 섞어서 쓸 때 사용.\n",
    "\n",
    "    결과:\n",
    "        self.X : (N_all, 9, 128)\n",
    "        self.y : (N_all,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str):\n",
    "        # 1) train 부분 로드\n",
    "        train_ds = UCIHARDataset(data_dir, train=True)\n",
    "        # 2) test 부분 로드\n",
    "        test_ds  = UCIHARDataset(data_dir, train=False)\n",
    "\n",
    "        # 3) concat\n",
    "        self.X = np.concatenate([train_ds.X, test_ds.X], axis=0)  # (N_all, 9, 128)\n",
    "        self.y = np.concatenate([train_ds.y, test_ds.y], axis=0)  # (N_all,)\n",
    "\n",
    "        # float / long로 정리\n",
    "        self.X = self.X.astype(np.float32)\n",
    "        self.y = self.y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]).float(),          # (9,128)\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)     # ()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c9182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-1. CBAM1D\n",
    "# =================================================================================\n",
    "class ChannelAttention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Channel attention for 1D signals.\n",
    "    Input shape:  (B, C, T)\n",
    "    Output shape: (B, C, T) with per-channel reweighting.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x : (B, C, T)\n",
    "        avg_out = self.avg_pool(x).squeeze(-1)  # (B, C)\n",
    "        max_out = self.max_pool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        avg_out = self.fc(avg_out)  # (B, C)\n",
    "        max_out = self.fc(max_out)  # (B, C)\n",
    "\n",
    "        out = (avg_out + max_out).unsqueeze(-1)  # (B, C, 1)\n",
    "        scale = self.sigmoid(out)  # (B, C, 1)\n",
    "        return x * scale  # broadcast along T\n",
    "\n",
    "\n",
    "class TemporalAttention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention for 1D signals.\n",
    "    Input shape:  (B, C, T)\n",
    "    Output shape: (B, C, T) with per-timestep reweighting.\n",
    "\n",
    "    Internally uses a conv over [avg_pool; max_pool] across channels,\n",
    "    so conv input channel dim is fixed to 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=2,\n",
    "            out_channels=1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, C, T)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "\n",
    "        attn_in = torch.cat([avg_out, max_out], dim=1)  # (B, 2, T)\n",
    "        attn_map = self.conv(attn_in)                        # (B, 1, T)\n",
    "        attn_map = self.sigmoid(attn_map)\n",
    "        return x * attn_map  # broadcast along C\n",
    "\n",
    "\n",
    "class CBAM1D(nn.Module):\n",
    "    \"\"\"\n",
    "    CBAM-style attention for 1D sensor sequences.\n",
    "    Does channel attention -> temporal attention.\n",
    "\n",
    "    Input / Output shape: (B, C, T)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel_att = ChannelAttention1D(channels, reduction=reduction)\n",
    "        self.temporal_att = TemporalAttention1D(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x : (B, C, T)\n",
    "        x = self.channel_att(x)\n",
    "        x = self.temporal_att(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7715cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-2. CrossFormer Block (Cross-Attn between tokens and learnable prototypes)\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 n_prototypes: int = 6,\n",
    "                 n_heads: int = 4,\n",
    "                 mlp_ratio: float = 2.0,\n",
    "                 dropout: float = 0.1,\n",
    "                 initial_prototypes: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: token embedding dim\n",
    "            n_prototypes: number of learnable class prototypes\n",
    "            n_heads: attention heads (must divide dim)\n",
    "            mlp_ratio: FFN expansion ratio\n",
    "            dropout: dropout inside MHA/MLP\n",
    "            initial_prototypes: optional (n_prototypes, dim) tensor to init prototypes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Learnable prototypes\n",
    "        self.prototypes = nn.Parameter(torch.randn(n_prototypes, dim))\n",
    "\n",
    "        if initial_prototypes is not None:\n",
    "            assert initial_prototypes.shape == self.prototypes.shape, \\\n",
    "                f\"Shape mismatch: initial_prototypes {initial_prototypes.shape} vs self.prototypes {self.prototypes.shape}\"\n",
    "            self.prototypes.data.copy_(initial_prototypes)\n",
    "            print(\"Prototypes initialized with calculated mean features.\")\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.prototypes)\n",
    "            print(\"Prototypes initialized with Xavier Uniform.\")\n",
    "\n",
    "        # Cross-attention (tokens Q) x (prototypes K,V)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Self-attention on tokens\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # FFN\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Projection for contrastive proto features\n",
    "        self.proto_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                return_proto_features: bool = False,\n",
    "                skip_cross_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, C=dim)\n",
    "            return_proto_features: if True, also returns pooled/proj features for contrast\n",
    "            skip_cross_attention: if True, bypass cross-attn (used for proto init feature extraction)\n",
    "        Returns:\n",
    "            If return_proto_features:\n",
    "                (x_out, proto_features, cross_attn_weights)\n",
    "            else:\n",
    "                x_out\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        attn_weights = None\n",
    "\n",
    "        # 1) Cross-Attention (optional)\n",
    "        if not skip_cross_attention:\n",
    "            # normalize prototypes for stable attention keys/values\n",
    "            normalized_prototypes = F.normalize(self.prototypes, dim=1, eps=1e-6)  # (P, C)\n",
    "            prototypes = normalized_prototypes.unsqueeze(0).expand(B, -1, -1)  # (B, P, C)\n",
    "\n",
    "            x_norm = self.norm1(x)\n",
    "            cross_out, attn_weights = self.cross_attn(x_norm, prototypes, prototypes)\n",
    "            x = x + cross_out  # residual\n",
    "\n",
    "        # 2) Self-Attention\n",
    "        x_norm = self.norm2(x)\n",
    "        self_out, _ = self.self_attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self_out  # residual\n",
    "\n",
    "        # 3) FFN\n",
    "        x = x + self.mlp(self.norm3(x))  # residual\n",
    "\n",
    "        if return_proto_features:\n",
    "            proto_features = x.mean(dim=1)  # (B, C)\n",
    "            proto_features = self.proto_proj(proto_features)  # (B, C)\n",
    "            return x, proto_features, attn_weights\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f56fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-3. Contrastive Prototype Loss\n",
    "# =================================================================================\n",
    "class ContrastivePrototypeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised prototype contrast loss.\n",
    "\n",
    "    For each sample embedding f_i and class prototypes P (1 per class),\n",
    "    we compute a softmax over cosine similarities and apply cross-entropy\n",
    "    against the ground truth label.\n",
    "\n",
    "    Intuition:\n",
    "      - Pull sample features closer to their class prototype.\n",
    "      - Push them away from other class prototypes.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,    # (B, D)\n",
    "        prototypes: torch.Tensor,  # (num_classes, D)\n",
    "        labels: torch.Tensor       # (B,)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features:  batch embeddings (B, D)\n",
    "            prototypes: class prototype matrix (num_classes, D)\n",
    "                        usually num_classes == n_prototypes\n",
    "            labels:    ground-truth class indices, shape (B,), dtype long\n",
    "\n",
    "        Returns:\n",
    "            scalar loss (tensor)\n",
    "        \"\"\"\n",
    "        # L2 normalize\n",
    "        features = F.normalize(features, dim=1, eps=1e-6)\n",
    "        prototypes = F.normalize(prototypes, dim=1, eps=1e-6)\n",
    "\n",
    "        # cosine similarity\n",
    "        logits = torch.matmul(features, prototypes.t()) / self.temperature  # (B, num_classes)\n",
    "\n",
    "        # InfoNCE Loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d911c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-4. Final Model: embedding + (CBAM) + CrossFormer + classifier\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerCBAM_HAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Sensor sequence classifier with:\n",
    "      - Conv1d embedding (+ BatchNorm + GELU + Dropout)\n",
    "      - Optional CBAM (channel & temporal attention)\n",
    "      - Either:\n",
    "          (A) CrossFormer block with learnable prototypes\n",
    "        or\n",
    "          (B) TransformerEncoderLayer-only self-attention fallback\n",
    "      - Global average pooling over time\n",
    "      - MLP classifier head\n",
    "      - (Optional) contrastive prototype loss\n",
    "\n",
    "    Args:\n",
    "        in_channels:   # sensor channels (e.g. 9 for UCI-HAR)\n",
    "        seq_len:       # sequence length (e.g. 128 for UCI-HAR); mostly for reference / profiling\n",
    "        embed_dim:     # conv embedding dim (and final feature dim if no reduction)\n",
    "        reduced_dim:   # reduced dim if use_dim_reduction=True\n",
    "        n_classes:     # num activity classes\n",
    "        n_prototypes:  # number of learnable prototypes in CrossFormer\n",
    "        n_heads:       # attention heads for CrossFormer/self-attn\n",
    "        kernel_size:   # conv1d kernel size for embedding, and CBAM temporal kernel\n",
    "        dropout:       # dropout rate\n",
    "        temperature:   # temperature for contrastive loss\n",
    "        initial_prototypes:  # tensor to init CrossFormerBlock.prototypes, or None\n",
    "        use_cbam:      # if True, apply CBAM after embedding\n",
    "        use_crossformer:     # if True, use CrossFormerBlock; else use vanilla self-attn block\n",
    "        use_contrast:        # if True, model can return contrastive loss\n",
    "        use_dim_reduction:   # if True, reduce dim before attention and restore after\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 9,\n",
    "                 seq_len: int = 128,\n",
    "                 embed_dim: int = 64,\n",
    "                 reduced_dim: int = 32,\n",
    "                 n_classes: int = 6,\n",
    "                 n_prototypes: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 kernel_size: int = 7,\n",
    "                 dropout: float = 0.1,\n",
    "                 temperature: float = 0.07,\n",
    "                 initial_prototypes: torch.Tensor = None,\n",
    "                 use_cbam: bool = True,\n",
    "                 use_crossformer: bool = True,\n",
    "                 use_contrast: bool = True,\n",
    "                 use_dim_reduction: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save config\n",
    "        self.in_channels = in_channels\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.reduced_dim = reduced_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.n_heads = n_heads\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.use_cbam = use_cbam\n",
    "        self.use_crossformer = use_crossformer\n",
    "        self.use_contrast = use_contrast\n",
    "        self.use_dim_reduction = use_dim_reduction\n",
    "\n",
    "        # 1) Embedding: Conv1d -> BN -> GELU -> Dropout\n",
    "        #    Input:  (B, in_channels, T)\n",
    "        #    Output: (B, embed_dim, T)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                embed_dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=(kernel_size - 1) // 2,  # \"same\" padding for odd kernel\n",
    "            ),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # 2) Optional CBAM\n",
    "        #    Still (B, embed_dim, T)\n",
    "        if self.use_cbam:\n",
    "            self.cbam = CBAM1D(\n",
    "                channels=embed_dim,\n",
    "                reduction=8,\n",
    "                kernel_size=kernel_size,\n",
    "            )\n",
    "\n",
    "        # 3) (Optional) Dim Reduction before attention\n",
    "        #    We'll call this dimension 'working_dim'.\n",
    "        #    If use_dim_reduction=False, working_dim == embed_dim.\n",
    "        working_dim = reduced_dim if use_dim_reduction else embed_dim\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_reduce = nn.Linear(embed_dim, reduced_dim)\n",
    "\n",
    "        # 4) Attention backbone\n",
    "        #    A) CrossFormerBlock (our prototype-based block)\n",
    "        #    B) Fallback: vanilla TransformerEncoderLayer\n",
    "        #    Input to these blocks: (B, T, working_dim)\n",
    "        #    Output shape stays (B, T, working_dim)\n",
    "        if self.use_crossformer:\n",
    "            self.crossformer = ContrastCrossFormerBlock(\n",
    "                dim=working_dim,\n",
    "                n_prototypes=n_prototypes,\n",
    "                n_heads=n_heads,\n",
    "                mlp_ratio=2.0,\n",
    "                dropout=dropout,\n",
    "                initial_prototypes=initial_prototypes,\n",
    "            )\n",
    "        else:\n",
    "            # TransformerEncoderLayer returns same shape (B, T, working_dim)\n",
    "            self.self_attn = nn.TransformerEncoderLayer(\n",
    "                d_model=working_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=int(working_dim * 2),\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        # 5) (Optional) Dim restore after attention\n",
    "        #    Back to embed_dim if we reduced.\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_restore = nn.Linear(reduced_dim, embed_dim)\n",
    "\n",
    "        # 6) Temporal pooling + classifier head\n",
    "        #    After attention we get (B, T, embed_dim)\n",
    "        #    -> transpose to (B, embed_dim, T)\n",
    "        #    -> AdaptiveAvgPool1d(1) -> (B, embed_dim)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_classes),\n",
    "        )\n",
    "\n",
    "        # 7) Contrastive loss module (optional)\n",
    "        if self.use_contrast and self.use_crossformer:\n",
    "            self.contrast_loss = ContrastivePrototypeLoss(temperature=temperature)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                labels: torch.Tensor = None,\n",
    "                return_contrast_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C_in, T)  e.g. (B, 9, 128)\n",
    "            labels: (B,) long tensor with class indices [0..n_classes-1]\n",
    "            return_contrast_loss: if True, we also compute contrastive loss\n",
    "\n",
    "        Returns:\n",
    "            if return_contrast_loss and use_contrast:\n",
    "                (logits, contrast_loss)\n",
    "            else:\n",
    "                logits\n",
    "        \"\"\"\n",
    "        # 1) Conv embedding (+CBAM)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if self.use_cbam:\n",
    "            x = self.cbam(x)\n",
    "\n",
    "        # 2) Prepare for attention\n",
    "        #    (B, embed_dim, T) -> (B, T, embed_dim)\n",
    "        #    Optionally reduce dim\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_reduce(x)\n",
    "\n",
    "        # 3) Attention backbone\n",
    "        proto_features = None\n",
    "        if self.use_crossformer:\n",
    "            if return_contrast_loss and self.use_contrast:\n",
    "                x, proto_features, _ = self.crossformer(x, return_proto_features=True,\n",
    "                                                        skip_cross_attention=False)\n",
    "            else:\n",
    "                x = self.crossformer(x, return_proto_features=False,\n",
    "                                     skip_cross_attention=False)\n",
    "        else:\n",
    "            x = self.self_attn(x)\n",
    "\n",
    "        # 4) Restore dim if reduced\n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_restore(x)\n",
    "\n",
    "        # 5) Pool over time\n",
    "        #    (B, T, embed_dim) -> (B, embed_dim, T) -> pool -> (B, embed_dim)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        feat_vec = self.pool(x).squeeze(-1)\n",
    "\n",
    "        # 6) Classifier\n",
    "        logits = self.classifier(feat_vec)\n",
    "\n",
    "        # 7) Optional contrastive term\n",
    "        if (\n",
    "            return_contrast_loss\n",
    "            and self.use_contrast\n",
    "            and proto_features is not None\n",
    "            and labels is not None\n",
    "        ):\n",
    "            contrast_loss = self.contrast_loss(\n",
    "                proto_features,                # (B, dim)\n",
    "                self.crossformer.prototypes,   # (n_prototypes, dim)\n",
    "                labels                         # (B,)\n",
    "            )\n",
    "            return logits, contrast_loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2cada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-5. Prototype Initialization\n",
    "# =================================================================================\n",
    "def get_mean_prototypes(train_full_dataset, device, config):\n",
    "\n",
    "    temp_model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=config['in_channels'],\n",
    "        seq_len=config['seq_len'],\n",
    "        n_classes=config['n_classes'],\n",
    "        n_prototypes=config['n_prototypes'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        reduced_dim=config['reduced_dim'], \n",
    "        n_heads=config['n_heads'],\n",
    "        kernel_size=config['kernel_size'],\n",
    "        dropout=config['dropout'],\n",
    "        temperature=config['temperature'],\n",
    "        initial_prototypes=None,\n",
    "        use_cbam=config[\"use_cbam\"],\n",
    "        use_crossformer=config[\"use_crossformer\"],\n",
    "        use_contrast=False,  # 중요: 여기서는 contrast loss 안 씀\n",
    "        use_dim_reduction=config['use_dim_reduction']\n",
    "    ).to(device)\n",
    "\n",
    "    temp_model.eval()\n",
    "\n",
    "    temp_loader = DataLoader(\n",
    "        train_full_dataset,\n",
    "        batch_size=config[\"batch_size\"],  # CONFIG 키 맞춤 (BATCH_SIZE -> batch_size)\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(temp_loader, desc=\"Prototype Init\"):\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            x = temp_model.embedding(batch_x)\n",
    "            if temp_model.use_cbam:\n",
    "                x = temp_model.cbam(x)\n",
    "\n",
    "            x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "            if temp_model.use_dim_reduction:\n",
    "                x = temp_model.dim_reduce(x)\n",
    "\n",
    "            if temp_model.use_crossformer:\n",
    "                x = temp_model.crossformer(\n",
    "                    x,\n",
    "                    return_proto_features=False,\n",
    "                    skip_cross_attention=True\n",
    "                )                                   # (B, T, working_dim)\n",
    "            else:\n",
    "                x = temp_model.self_attn(x)         # (B, T, working_dim)\n",
    "\n",
    "            # Dim restore (if reduction was used)\n",
    "            if temp_model.use_dim_reduction:\n",
    "                x = temp_model.dim_restore(x)       # (B, T, embed_dim)\n",
    "\n",
    "            x = x.transpose(1, 2).contiguous()  # (B, embed_dim, T)\n",
    "\n",
    "            pooled_features = temp_model.pool(x).squeeze(-1)  # (B, embed_dim)\n",
    "\n",
    "            all_features.append(pooled_features.cpu())  # (N, embed_dim)\n",
    "            all_labels.append(batch_y.cpu())  # (N,)\n",
    "\n",
    "    num_classes = config[\"n_classes\"]\n",
    "    feature_dim = config[\"embed_dim\"]  # pooled_features의 dim과 맞춰줌\n",
    "\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    mean_prototypes = torch.zeros(num_classes, feature_dim, dtype=torch.float32)\n",
    "    for i in range(num_classes):\n",
    "        class_features = all_features[all_labels == i]\n",
    "        if len(class_features) > 0:\n",
    "            mean_prototypes[i] = class_features.mean(dim=0)\n",
    "        else:\n",
    "            mean_prototypes[i] = torch.randn(feature_dim)\n",
    "    \n",
    "    return mean_prototypes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada2ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 3-1. train & evaluation\n",
    "# =================================================================================\n",
    "def train_epoch(model,\n",
    "                dataloader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                device,\n",
    "                use_contrast=True,\n",
    "                contrast_weight=0.5):\n",
    "    model.train()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    ce_loss_sum = 0.0\n",
    "    contrast_loss_sum = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        batch_x = batch_x.to(device, non_blocking=True)\n",
    "        batch_y = batch_y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        if use_contrast and model.use_contrast and model.use_crossformer:\n",
    "            logits, contrast_loss = model(batch_x, batch_y, return_contrast_loss=True)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            total_loss = ce_loss + contrast_weight * contrast_loss\n",
    "            contrast_loss_sum  += contrast_loss.item()\n",
    "        else:\n",
    "            logits = model(batch_x)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            total_loss  = ce_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sum  += total_loss.item()\n",
    "        ce_loss_sum  += ce_loss.item()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "    \n",
    "    torch.cuda.synchronize() # 한 에폭 끝에서 동기화\n",
    "\n",
    "    avg_total_loss = total_loss / len(dataloader)\n",
    "    avg_ce_loss = ce_loss_sum  / len(dataloader)\n",
    "    avg_contrast_loss = contrast_loss_sum / len(dataloader) if contrast_loss_sum  > 0 else 0.0\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_total_loss, avg_ce_loss, avg_contrast_loss, acc, f1\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             criterion,\n",
    "             device,\n",
    "             use_contrast=True,\n",
    "             contrast_weight=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    ce_loss_sum = 0.0\n",
    "    contrast_loss_sum = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device, non_blocking=True)\n",
    "            batch_y = batch_y.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward (eval 모드에서는 no_grad)\n",
    "            if use_contrast and model.use_contrast and model.use_crossformer:\n",
    "                logits, contrast_loss = model(\n",
    "                    batch_x,\n",
    "                    batch_y,\n",
    "                    return_contrast_loss=True\n",
    "                )\n",
    "                ce_loss = criterion(logits, batch_y)\n",
    "                total_loss = ce_loss + contrast_weight * contrast_loss\n",
    "                contrast_loss_sum += contrast_loss.item()\n",
    "            else:\n",
    "                logits = model(batch_x)\n",
    "                ce_loss = criterion(logits, batch_y)\n",
    "                total_loss = ce_loss\n",
    "\n",
    "            total_loss_sum  += total_loss.item()\n",
    "            ce_loss_sum += ce_loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    avg_total_loss = total_loss_sum / len(dataloader)\n",
    "    avg_ce_loss = ce_loss_sum / len(dataloader)\n",
    "    avg_contrast_loss = contrast_loss_sum / len(dataloader) if contrast_loss_sum > 0 else 0.0\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_total_loss, acc, f1, all_preds, all_labels, avg_ce_loss, avg_contrast_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e48bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 4-1. Confusion Matrix & t-SNE\n",
    "# =================================================================================\n",
    "\n",
    "# UCI-HAR LABELS\n",
    "ACTIVITY_LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\",\n",
    "]\n",
    "\n",
    "def plot_classification_results(y_true, y_pred, save_path=None):\n",
    "    \"\"\"\n",
    "    Confusion Matrix 시각화 \n",
    "    \"\"\"\n",
    "    # 1. Classification Report 출력\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Classification Report\")\n",
    "    print(\"=\"*80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=ACTIVITY_LABELS,\n",
    "            digits=4,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2. 정규화된 Confusion Matrix 계산 및 시각화\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=ACTIVITY_LABELS, yticklabels=ACTIVITY_LABELS)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('')\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _extract_featvec_before_classifier(model, xb, device):\n",
    "    \"\"\"\n",
    "    model.forward()의 흐름을 그대로 따라가되\n",
    "    classifier 직전의 feature vector(feat_vec, shape (B, embed_dim))만 추출.\n",
    "    이건 visualize_tsne에서 feature space로 쓸 거임.\n",
    "\n",
    "    xb: (B, C_in, T) on CPU\n",
    "    returns: (B, embed_dim) on CPU (no grad)\n",
    "    \"\"\"\n",
    "    xb = xb.to(device)\n",
    "\n",
    "    # 1. Embedding (+CBAM)\n",
    "    x = model.embedding(xb)                  # (B, embed_dim, T)\n",
    "    if model.use_cbam:\n",
    "        x = model.cbam(x)                    # (B, embed_dim, T)\n",
    "\n",
    "    # 2. 준비: (B, C, T) -> (B, T, C)\n",
    "    x = x.transpose(1, 2).contiguous()       # (B, T, embed_dim)\n",
    "\n",
    "    # 3. Dim reduction (optional)\n",
    "    if model.use_dim_reduction:\n",
    "        x = model.dim_reduce(x)              # (B, T, reduced_dim)\n",
    "\n",
    "    # 4. Attention backbone\n",
    "    if model.use_crossformer:\n",
    "        # contrast 안 쓸 거라 proto_features 필요 없음\n",
    "        # eval 시에는 실제 inference 경로로 skip_cross_attention=False 가 더 정직\n",
    "        x = model.crossformer(\n",
    "            x,\n",
    "            return_proto_features=False,\n",
    "            skip_cross_attention=False\n",
    "        )                                    # (B, T, working_dim)\n",
    "    else:\n",
    "        # fallback self-attention\n",
    "        x = model.self_attn(x)               # (B, T, working_dim)\n",
    "\n",
    "    # 5. Dim restore (optional)\n",
    "    if model.use_dim_reduction:\n",
    "        x = model.dim_restore(x)             # (B, T, embed_dim)\n",
    "\n",
    "    # 6. Pooling과 동일하게 처리\n",
    "    # (B, T, embed_dim) -> (B, embed_dim, T)\n",
    "    x = x.transpose(1, 2).contiguous()       # (B, embed_dim, T)\n",
    "    feat_vec = model.pool(x).squeeze(-1)     # (B, embed_dim)\n",
    "\n",
    "    return feat_vec.detach().cpu()           # CPU로 반환, grad 없이\n",
    "\n",
    "def visualize_tsne(model, dataloader, device, save_path=None, max_points=2000):\n",
    "    \"\"\"\n",
    "    모델의 최종 분류기 직전 임베딩(feat_vec)을 모아서 t-SNE로 2D 시각화.\n",
    "\n",
    "    - dataloader: 보통 val_loader나 test_loader 넣는 걸 추천\n",
    "    - max_points: t-SNE에 넣을 최대 샘플 수 (메모리 / 시각화 과부하 방지)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dataloader):\n",
    "            # xb: (B, C, T) on CPU\n",
    "            # yb: (B,)\n",
    "            feat_vec = _extract_featvec_before_classifier(model, xb, device)  # (B, embed_dim) on CPU\n",
    "            all_features.append(feat_vec.numpy())\n",
    "            all_labels.append(yb.cpu().numpy())\n",
    "\n",
    "    all_features = np.concatenate(all_features, axis=0)  # (N, D)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)      # (N,)\n",
    "\n",
    "    # 샘플 수 줄이기 (max_points 초과 시 랜덤 샘플)\n",
    "    N = all_features.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if N > max_points:\n",
    "        idx = np.random.choice(N, size=max_points, replace=False)\n",
    "\n",
    "    X_sel = all_features[idx]  # (M, D)\n",
    "    y_sel = all_labels[idx]    # (M,)\n",
    "\n",
    "    # perplexity는 (샘플 수 - 1)보다 작아야 함\n",
    "    # 너무 작으면 이상해지니 최소 5 정도로 clamp\n",
    "    effective_perp = min(30, len(X_sel) - 1)\n",
    "    effective_perp = max(effective_perp, 5)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        perplexity=effective_perp,\n",
    "        max_iter=2000,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "    )\n",
    "    proj = tsne.fit_transform(X_sel)  # (M, 2)\n",
    "\n",
    "    df = pd.DataFrame(proj, columns=[\"Dim1\", \"Dim2\"])\n",
    "    df[\"label\"] = [ACTIVITY_LABELS[l] for l in y_sel]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"Dim1\",\n",
    "        y=\"Dim2\",\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hsv\", n_colors=len(ACTIVITY_LABELS)),\n",
    "        legend=\"full\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.title(\"t-SNE of embeddings extracted by the model\", fontsize=10)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    plt.legend(\n",
    "        title=\"Activity\",\n",
    "        loc=\"upper right\",\n",
    "        fontsize=6,\n",
    "        title_fontsize=7,\n",
    "        labelspacing=0.2,\n",
    "    )\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_tsne_raw(dataloader, device, save_path=None, max_points=2000):\n",
    "    \"\"\"\n",
    "    모델을 거치기 전의 raw window (shape: B x C x T)를 flatten해서 t-SNE로 투영.\n",
    "\n",
    "    - dataloader: 보통 val_loader나 test_loader 넣는 걸 추천 (train 넣으면 class 간경계 너무 깨끗하게 나오는 경우 많음)\n",
    "    - max_points: t-SNE에 사용할 최대 샘플 수\n",
    "    \"\"\"\n",
    "    all_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    for xb, yb in tqdm(dataloader):\n",
    "        all_raw.append(xb.cpu().numpy())     # (B, C, T)\n",
    "        all_labels.append(yb.cpu().numpy())  # (B,)\n",
    "\n",
    "    all_raw = np.concatenate(all_raw, axis=0)       # (N, C, T)\n",
    "    all_labels = np.concatenate(all_labels, axis=0) # (N,)\n",
    "\n",
    "    # 샘플 수 줄이기\n",
    "    N = all_raw.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if N > max_points:\n",
    "        idx = np.random.choice(N, size=max_points, replace=False)\n",
    "\n",
    "    X_sel = all_raw[idx]    # (M, C, T)\n",
    "    y_sel = all_labels[idx] # (M,)\n",
    "\n",
    "    # flatten: (M, C*T)\n",
    "    X_flat = X_sel.reshape(X_sel.shape[0], -1)\n",
    "\n",
    "    effective_perp = min(30, len(X_flat) - 1)\n",
    "    effective_perp = max(effective_perp, 5)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        perplexity=effective_perp,\n",
    "        max_iter=2000,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "    )\n",
    "    proj = tsne.fit_transform(X_flat)\n",
    "\n",
    "    df = pd.DataFrame(proj, columns=[\"Dim1\", \"Dim2\"])\n",
    "    df[\"label\"] = [ACTIVITY_LABELS[l] for l in y_sel]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"Dim1\",\n",
    "        y=\"Dim2\",\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hsv\", n_colors=len(ACTIVITY_LABELS)),\n",
    "        legend=\"full\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.title(\"t-SNE of raw data before model processing\", fontsize=10)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    plt.legend(\n",
    "        title=\"Activity\",\n",
    "        loc=\"upper right\",\n",
    "        fontsize=6,\n",
    "        title_fontsize=7,\n",
    "        labelspacing=0.2,\n",
    "    )\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f770ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 4-2. Model profiling: Param(M), FLOPs(M), Inference Time(ms)\n",
    "# =================================================================================\n",
    "def profile_model(model,\n",
    "                  sample_input: torch.Tensor,\n",
    "                  device: torch.device,\n",
    "                  warmup: int = 10,\n",
    "                  iters: int = 50):\n",
    "    \"\"\"\n",
    "    모델 구조/비용 측정:\n",
    "      - 파라미터 수 (M 단위)\n",
    "      - FLOPs per sample (M 단위, fvcore 있으면)\n",
    "      - 평균 추론 시간 (ms / sample)\n",
    "\n",
    "    fvcore가 stdout/stderr에 시끄럽게 프린트하는 걸 다 먹어버린다.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1) 파라미터 수\n",
    "    # -------------------------------------------------\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    params_m = total_params / 1e6  # million params\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
    "    #    - 모든 stdout/stderr을 임시 버퍼로 리다이렉트해서\n",
    "    #      \"Unsupported operator ...\" 같은 중얼거림을 완전히 감춘다.\n",
    "    # -------------------------------------------------\n",
    "    flops_m = None\n",
    "    if \"FVCORE_AVAILABLE\" in globals() and FVCORE_AVAILABLE and FlopCountAnalysis is not None:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                fake_out = io.StringIO()\n",
    "                fake_err = io.StringIO()\n",
    "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
    "                    flops = FlopCountAnalysis(model, (sample_input.to(device),))\n",
    "                    total_flops = flops.total()\n",
    "            flops_m = total_flops / 1e6  # to millions\n",
    "        except Exception:\n",
    "            flops_m = None\n",
    "    else:\n",
    "        flops_m = None\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) 추론 시간 측정\n",
    "    # -------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        # warmup\n",
    "        for _ in range(warmup):\n",
    "            _ = model(sample_input.to(device))\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(iters):\n",
    "            _ = model(sample_input.to(device))\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "    avg_sec = (end - start) / iters\n",
    "    inference_ms = avg_sec * 1000.0\n",
    "\n",
    "    return {\n",
    "        \"params_m\": params_m,\n",
    "        \"flops_m\": flops_m,\n",
    "        \"inference_ms\": inference_ms,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def print_model_profile(stats_dict):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Parameters      : {stats_dict['params_m']:.4f} M\")\n",
    "    print(f\"FLOPs / sample : {stats_dict['flops_m']:.3f} M\")\n",
    "    print(f\"Infer Time     : {stats_dict['inference_ms']:.2f} ms/sample\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67ab48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(base_config, variant_overrides, device):\n",
    "    \"\"\"\n",
    "    단일 variant (예: 'Embed+CBAM')에 대해\n",
    "    - 전체 UCI-HAR (train+test 합친 것) 불러오기\n",
    "    - 60/20/20 랜덤 split\n",
    "    - 모델 학습 (best val ckpt 유지)\n",
    "    - best ckpt로 test 측정\n",
    "    - confusion matrix / t-SNE / profile 산출\n",
    "    - 결과 row(dict)와 history(list[epoch log]) 반환\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) config 준비: base_config 복사 후 override로 키만 덮어쓰기\n",
    "    cfg = copy.deepcopy(base_config)\n",
    "    for k, v in variant_overrides.items():\n",
    "        if k == \"tag\":\n",
    "            continue\n",
    "        cfg[k] = v\n",
    "\n",
    "    seed_everything(cfg[\"seed\"])\n",
    "    DEVICE = device\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[{variant_overrides['tag']}] Dataset : {cfg['dataset_name']} (merged train+test, random split)\")\n",
    "    print(f\"Device         : {DEVICE}\")\n",
    "    print(\"=\" * 80)\n",
    "    pretty_print_config(cfg)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1) 전체 UCI-HAR 윈도우 (train+test 합침)\n",
    "    # -------------------------------------------------\n",
    "    full_dataset = UCIHARAllDataset(cfg[\"data_dir\"])\n",
    "    N = len(full_dataset)\n",
    "\n",
    "    # 60/20/20 윈도우 단위 랜덤 분할\n",
    "    idx_all = np.arange(N)\n",
    "    rng = np.random.default_rng(cfg[\"seed\"])\n",
    "    rng.shuffle(idx_all)\n",
    "\n",
    "    n_train = int(0.6 * N)\n",
    "    n_val   = int(0.2 * N)\n",
    "    train_idx = idx_all[:n_train]\n",
    "    val_idx   = idx_all[n_train:n_train+n_val]\n",
    "    test_idx  = idx_all[n_train+n_val:]\n",
    "\n",
    "    from torch.utils.data import Subset\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset   = Subset(full_dataset, val_idx)\n",
    "    test_dataset  = Subset(full_dataset, test_idx)\n",
    "\n",
    "    print(f\"Split sizes -> train:{len(train_dataset)} val:{len(val_dataset)} test:{len(test_dataset)}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) DataLoader\n",
    "    # -------------------------------------------------\n",
    "    g = torch.Generator().manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) 프로토타입 초기화\n",
    "    #    CrossFormer를 쓰는 variant만 평균 feature로 init\n",
    "    # -------------------------------------------------\n",
    "    if cfg[\"use_crossformer\"]:\n",
    "        initial_prototypes = get_mean_prototypes(\n",
    "            train_dataset,\n",
    "            DEVICE,\n",
    "            cfg\n",
    "        )\n",
    "    else:\n",
    "        initial_prototypes = None\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4) 모델 생성\n",
    "    # -------------------------------------------------\n",
    "    model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=cfg[\"in_channels\"],\n",
    "        seq_len=cfg[\"seq_len\"],\n",
    "        embed_dim=cfg[\"embed_dim\"],\n",
    "        reduced_dim=cfg[\"reduced_dim\"],\n",
    "        n_classes=cfg[\"n_classes\"],\n",
    "        n_prototypes=cfg[\"n_prototypes\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        kernel_size=cfg[\"kernel_size\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        initial_prototypes=initial_prototypes,\n",
    "        use_cbam=cfg[\"use_cbam\"],\n",
    "        use_crossformer=cfg[\"use_crossformer\"],\n",
    "        use_contrast=cfg[\"use_contrast\"],\n",
    "        use_dim_reduction=cfg[\"use_dim_reduction\"],\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5) Optimizer / Scheduler / Loss\n",
    "    # -------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg[\"learning_rate\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    if cfg[\"scheduler_type\"] == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg[\"scheduler_T_max\"],\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 6) Train Loop (val acc 최고의 ckpt 저장)\n",
    "    # -------------------------------------------------\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        # train\n",
    "        train_total, train_ce, train_ct, train_acc, train_f1 = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            DEVICE,\n",
    "            use_contrast=cfg[\"use_contrast\"],\n",
    "            contrast_weight=cfg[\"contrast_weight\"],\n",
    "        )\n",
    "\n",
    "        # val\n",
    "        val_total, val_acc, val_f1, _, _, val_ce, val_ct = evaluate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            DEVICE,\n",
    "            use_contrast=cfg[\"use_contrast\"],\n",
    "            contrast_weight=cfg[\"contrast_weight\"],\n",
    "        )\n",
    "\n",
    "        # scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # 기록\n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_total\": train_total,\n",
    "            \"train_ce\": train_ce,\n",
    "            \"train_ct\": train_ct,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_total\": val_total,\n",
    "            \"val_ce\": val_ce,\n",
    "            \"val_ct\": val_ct,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "        })\n",
    "\n",
    "        # best 갱신\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # 로그 출력\n",
    "        if (epoch + 1) % cfg[\"print_every\"] == 0:\n",
    "            ep_now   = epoch + 1\n",
    "            ep_total = cfg[\"epochs\"]\n",
    "            print(f\"[{variant_overrides['tag']}] Epoch {ep_now:03d}/{ep_total:03d}\")\n",
    "            print(\n",
    "                \"  train | \"\n",
    "                f\"total={train_total:.4f}  \"\n",
    "                f\"ce={train_ce:.4f}  \"\n",
    "                f\"ct={train_ct:.4f}  \"\n",
    "                f\"acc={train_acc:.4f}  \"\n",
    "                f\"f1={train_f1:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                \"  val   | \"\n",
    "                f\"total={val_total:.4f}  \"\n",
    "                f\"ce={val_ce:.4f}  \"\n",
    "                f\"ct={val_ct:.4f}  \"\n",
    "                f\"acc={val_acc:.4f}  \"\n",
    "                f\"f1={val_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 7) best ckpt 로드 후 test 평가\n",
    "    # -------------------------------------------------\n",
    "    assert best_state is not None, \"No best_state saved??\"\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    test_total, test_acc, test_f1, test_preds, test_labels, test_ce, test_ct = evaluate(\n",
    "        model,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        DEVICE,\n",
    "        use_contrast=cfg[\"use_contrast\"],\n",
    "        contrast_weight=cfg[\"contrast_weight\"],\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[{variant_overrides['tag']}] Training Complete!\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.4f} @ epoch {best_epoch}\")\n",
    "    print(\n",
    "        f\"  Test(best ckpt): \"\n",
    "        f\"acc={test_acc:.4f}, f1={test_f1:.4f}, \"\n",
    "        f\"total_loss={test_total:.4f}, ce={test_ce:.4f}, ct={test_ct:.4f}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 8) 모델 프로파일\n",
    "    # -------------------------------------------------\n",
    "    stats_profile = None\n",
    "    if cfg[\"profile_model\"]:\n",
    "        dummy_input = torch.randn(\n",
    "            1,\n",
    "            cfg[\"in_channels\"],\n",
    "            cfg[\"seq_len\"],\n",
    "        )\n",
    "        stats_profile = profile_model(model, dummy_input, DEVICE)\n",
    "        print_model_profile(stats_profile)\n",
    "    # -------------------------------------------------\n",
    "    # 10) 결과 dict (CSV / 표용)\n",
    "    # -------------------------------------------------\n",
    "    row = {\n",
    "        \"tag\":                    variant_overrides[\"tag\"],\n",
    "        \"use_cbam\":               cfg[\"use_cbam\"],\n",
    "        \"use_crossformer\":        cfg[\"use_crossformer\"],\n",
    "        \"use_contrast\":           cfg[\"use_contrast\"],\n",
    "        \"best_val_acc\":           best_val_acc,\n",
    "        \"test_acc\":               test_acc,\n",
    "        \"test_f1\":                test_f1,\n",
    "        \"test_total_loss\":        test_total,\n",
    "        \"test_ce\":                test_ce,\n",
    "        \"test_ct\":                test_ct,\n",
    "        \"params_m\":               stats_profile[\"params_m\"]     if stats_profile else None,\n",
    "        \"flops_m\":                stats_profile[\"flops_m\"]      if stats_profile else None,\n",
    "        \"inference_ms\":           stats_profile[\"inference_ms\"] if stats_profile else None,\n",
    "        \"best_epoch\":             best_epoch,\n",
    "    }\n",
    "\n",
    "    return row, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8861c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Embed+CBAM] Dataset : UCI-HAR (merged train+test, random split)\n",
      "Device         : cuda\n",
      "================================================================================\n",
      "================================================================================\n",
      "EXPERIMENT CONFIG\n",
      "--------------------------------------------------------------------------------\n",
      "batch_size        : 128\n",
      "contrast_weight   : 0.25\n",
      "data_dir          : C://Users/park9/CBAM_HAR/UCI-HAR/data\n",
      "dataset_name      : UCI-HAR\n",
      "do_tsne           : True\n",
      "dropout           : 0.1\n",
      "embed_dim         : 64\n",
      "epochs            : 100\n",
      "in_channels       : 9\n",
      "kernel_size       : 11\n",
      "learning_rate     : 0.0005\n",
      "n_classes         : 6\n",
      "n_heads           : 8\n",
      "n_prototypes      : 6\n",
      "print_every       : 25\n",
      "profile_model     : True\n",
      "reduced_dim       : 32\n",
      "scheduler_T_max   : 100\n",
      "scheduler_type    : cosine\n",
      "seed              : 42\n",
      "seq_len           : 128\n",
      "temperature       : 0.05\n",
      "use_cbam          : True\n",
      "use_contrast      : False\n",
      "use_crossformer   : False\n",
      "use_dim_reduction : False\n",
      "val_ratio         : 0.2\n",
      "weight_decay      : 0.0001\n",
      "================================================================================\n",
      "Split sizes -> train:6179 val:2059 test:2061\n",
      "Parameters: 45,596\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embed+CBAM] Epoch 025/100\n",
      "  train | total=0.0034  ce=0.0947  ct=0.0000  acc=0.9600  f1=0.9600\n",
      "  val   | total=0.1024  ce=0.1024  ct=0.0000  acc=0.9505  f1=0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embed+CBAM] Epoch 050/100\n",
      "  train | total=0.0015  ce=0.0694  ct=0.0000  acc=0.9728  f1=0.9728\n",
      "  val   | total=0.0772  ce=0.0772  ct=0.0000  acc=0.9704  f1=0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embed+CBAM] Epoch 075/100\n",
      "  train | total=0.0022  ce=0.0491  ct=0.0000  acc=0.9801  f1=0.9801\n",
      "  val   | total=0.0637  ce=0.0637  ct=0.0000  acc=0.9767  f1=0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embed+CBAM] Epoch 100/100\n",
      "  train | total=0.0013  ce=0.0452  ct=0.0000  acc=0.9827  f1=0.9827\n",
      "  val   | total=0.0610  ce=0.0610  ct=0.0000  acc=0.9767  f1=0.9767\n",
      "================================================================================\n",
      "[Embed+CBAM] Training Complete!\n",
      "  Best Val Acc: 0.9796 @ epoch 81\n",
      "  Test(best ckpt): acc=0.9796, f1=0.9796, total_loss=0.0565, ce=0.0565, ct=0.0000\n",
      "================================================================================\n",
      "Parameters      : 0.0456 M\n",
      "FLOPs / sample : 3.016 M\n",
      "Infer Time     : 1.31 ms/sample\n",
      "================================================================================\n",
      "================================================================================\n",
      "[CBAM+CrossFormer (noContrast)] Dataset : UCI-HAR (merged train+test, random split)\n",
      "Device         : cuda\n",
      "================================================================================\n",
      "================================================================================\n",
      "EXPERIMENT CONFIG\n",
      "--------------------------------------------------------------------------------\n",
      "batch_size        : 128\n",
      "contrast_weight   : 0.25\n",
      "data_dir          : C://Users/park9/CBAM_HAR/UCI-HAR/data\n",
      "dataset_name      : UCI-HAR\n",
      "do_tsne           : True\n",
      "dropout           : 0.1\n",
      "embed_dim         : 64\n",
      "epochs            : 100\n",
      "in_channels       : 9\n",
      "kernel_size       : 11\n",
      "learning_rate     : 0.0005\n",
      "n_classes         : 6\n",
      "n_heads           : 8\n",
      "n_prototypes      : 6\n",
      "print_every       : 25\n",
      "profile_model     : True\n",
      "reduced_dim       : 32\n",
      "scheduler_T_max   : 100\n",
      "scheduler_type    : cosine\n",
      "seed              : 42\n",
      "seq_len           : 128\n",
      "temperature       : 0.05\n",
      "use_cbam          : True\n",
      "use_contrast      : False\n",
      "use_crossformer   : True\n",
      "use_dim_reduction : False\n",
      "val_ratio         : 0.2\n",
      "weight_decay      : 0.0001\n",
      "================================================================================\n",
      "Split sizes -> train:6179 val:2059 test:2061\n",
      "Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prototype Init: 100%|██████████| 49/49 [00:00<00:00, 223.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes initialized with calculated mean features.\n",
      "Parameters: 71,068\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer (noContrast)] Epoch 025/100\n",
      "  train | total=0.0003  ce=0.0554  ct=0.0000  acc=0.9767  f1=0.9767\n",
      "  val   | total=0.0752  ce=0.0752  ct=0.0000  acc=0.9689  f1=0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer (noContrast)] Epoch 050/100\n",
      "  train | total=0.0009  ce=0.0245  ct=0.0000  acc=0.9903  f1=0.9903\n",
      "  val   | total=0.0407  ce=0.0407  ct=0.0000  acc=0.9869  f1=0.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer (noContrast)] Epoch 075/100\n",
      "  train | total=0.0004  ce=0.0139  ct=0.0000  acc=0.9943  f1=0.9943\n",
      "  val   | total=0.0410  ce=0.0410  ct=0.0000  acc=0.9864  f1=0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer (noContrast)] Epoch 100/100\n",
      "  train | total=0.0000  ce=0.0093  ct=0.0000  acc=0.9974  f1=0.9974\n",
      "  val   | total=0.0416  ce=0.0416  ct=0.0000  acc=0.9874  f1=0.9874\n",
      "================================================================================\n",
      "[CBAM+CrossFormer (noContrast)] Training Complete!\n",
      "  Best Val Acc: 0.9888 @ epoch 61\n",
      "  Test(best ckpt): acc=0.9864, f1=0.9864, total_loss=0.0390, ce=0.0390, ct=0.0000\n",
      "================================================================================\n",
      "Parameters      : 0.0711 M\n",
      "FLOPs / sample : 4.253 M\n",
      "Infer Time     : 2.24 ms/sample\n",
      "================================================================================\n",
      "================================================================================\n",
      "[CBAM+CrossFormer+Contrast (Full Model)] Dataset : UCI-HAR (merged train+test, random split)\n",
      "Device         : cuda\n",
      "================================================================================\n",
      "================================================================================\n",
      "EXPERIMENT CONFIG\n",
      "--------------------------------------------------------------------------------\n",
      "batch_size        : 128\n",
      "contrast_weight   : 0.25\n",
      "data_dir          : C://Users/park9/CBAM_HAR/UCI-HAR/data\n",
      "dataset_name      : UCI-HAR\n",
      "do_tsne           : True\n",
      "dropout           : 0.1\n",
      "embed_dim         : 64\n",
      "epochs            : 100\n",
      "in_channels       : 9\n",
      "kernel_size       : 11\n",
      "learning_rate     : 0.0005\n",
      "n_classes         : 6\n",
      "n_heads           : 8\n",
      "n_prototypes      : 6\n",
      "print_every       : 25\n",
      "profile_model     : True\n",
      "reduced_dim       : 32\n",
      "scheduler_T_max   : 100\n",
      "scheduler_type    : cosine\n",
      "seed              : 42\n",
      "seq_len           : 128\n",
      "temperature       : 0.05\n",
      "use_cbam          : True\n",
      "use_contrast      : True\n",
      "use_crossformer   : True\n",
      "use_dim_reduction : False\n",
      "val_ratio         : 0.2\n",
      "weight_decay      : 0.0001\n",
      "================================================================================\n",
      "Split sizes -> train:6179 val:2059 test:2061\n",
      "Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prototype Init: 100%|██████████| 49/49 [00:00<00:00, 215.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes initialized with calculated mean features.\n",
      "Parameters: 71,068\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer+Contrast (Full Model)] Epoch 025/100\n",
      "  train | total=0.0016  ce=0.0605  ct=0.0633  acc=0.9746  f1=0.9746\n",
      "  val   | total=0.0701  ce=0.0555  ct=0.0583  acc=0.9752  f1=0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer+Contrast (Full Model)] Epoch 050/100\n",
      "  train | total=0.0027  ce=0.0383  ct=0.0394  acc=0.9849  f1=0.9850\n",
      "  val   | total=0.0569  ce=0.0456  ct=0.0450  acc=0.9811  f1=0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer+Contrast (Full Model)] Epoch 075/100\n",
      "  train | total=0.0004  ce=0.0183  ct=0.0191  acc=0.9935  f1=0.9935\n",
      "  val   | total=0.0523  ce=0.0420  ct=0.0410  acc=0.9849  f1=0.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CBAM+CrossFormer+Contrast (Full Model)] Epoch 100/100\n",
      "  train | total=0.0000  ce=0.0162  ct=0.0161  acc=0.9943  f1=0.9943\n",
      "  val   | total=0.0567  ce=0.0458  ct=0.0436  acc=0.9849  f1=0.9849\n",
      "================================================================================\n",
      "[CBAM+CrossFormer+Contrast (Full Model)] Training Complete!\n",
      "  Best Val Acc: 0.9874 @ epoch 78\n",
      "  Test(best ckpt): acc=0.9874, f1=0.9874, total_loss=0.0452, ce=0.0360, ct=0.0366\n",
      "================================================================================\n",
      "Parameters      : 0.0711 M\n",
      "FLOPs / sample : 4.253 M\n",
      "Infer Time     : 2.07 ms/sample\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "🏁 UCI-HAR (subject-dependent random split) Ablation Summary\n",
      "================================================================================\n",
      "                                   tag  use_cbam  use_crossformer  use_contrast  best_val_acc  test_acc  test_f1  test_total_loss  test_ce  test_ct  params_m  flops_m  inference_ms  best_epoch\n",
      "                            Embed+CBAM      True            False         False      0.979602  0.979622 0.979584         0.056533 0.056533 0.000000  0.045596 3.015808      1.312099          81\n",
      "         CBAM+CrossFormer (noContrast)      True             True         False      0.988830  0.986414 0.986399         0.038980 0.038980 0.000000  0.071068 4.252800      2.240758          61\n",
      "CBAM+CrossFormer+Contrast (Full Model)      True             True          True      0.987373  0.987385 0.987371         0.045186 0.036047 0.036553  0.071068 4.252800      2.065420          78\n",
      "Saved ablation_results_UCI-HAR_subject-dep.csv\n"
     ]
    }
   ],
   "source": [
    "def main_ablation():\n",
    "    \"\"\"\n",
    "    UCI-HAR (train+test 합침, subject-dependent random split)에서\n",
    "    아래 세 가지 variant를 전부 돌려서 결과 표랑 CSV를 만든다.\n",
    "\n",
    "    1) Embed+CBAM\n",
    "    2) CBAM+CrossFormer (noContrast)\n",
    "    3) CBAM+CrossFormer+Contrast (Full Model)\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    variants = [\n",
    "        {\n",
    "            # 가장 라이트: Conv Embedding + CBAM + Global Pool + Classifier\n",
    "            \"tag\": \"Embed+CBAM\",\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": False,\n",
    "            \"use_contrast\": False,\n",
    "        },\n",
    "        {\n",
    "            # CrossFormer까지 쓰지만 contrast는 끔\n",
    "            \"tag\": \"CBAM+CrossFormer (noContrast)\",\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": True,\n",
    "            \"use_contrast\": False,\n",
    "        },\n",
    "        {\n",
    "            # 풀 모델: CrossFormer + Contrastive Prototype Loss\n",
    "            \"tag\": \"CBAM+CrossFormer+Contrast (Full Model)\",\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": True,\n",
    "            \"use_contrast\": True,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_rows = []\n",
    "    all_histories = {}\n",
    "\n",
    "    for v in variants:\n",
    "        row, hist = run_experiment(CONFIG, v, device)\n",
    "        all_rows.append(row)\n",
    "        all_histories[v[\"tag\"]] = hist  # 에폭별 curve 필요하면 나중에 그림 그릴 수 있음\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"tag\",\n",
    "        \"use_cbam\",\n",
    "        \"use_crossformer\",\n",
    "        \"use_contrast\",\n",
    "        \"best_val_acc\",\n",
    "        \"test_acc\",\n",
    "        \"test_f1\",\n",
    "        \"test_total_loss\",\n",
    "        \"test_ce\",\n",
    "        \"test_ct\",\n",
    "        \"params_m\",\n",
    "        \"flops_m\",\n",
    "        \"inference_ms\",\n",
    "        \"best_epoch\",\n",
    "    ])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🏁 UCI-HAR (subject-dependent random split) Ablation Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    df.to_csv(\"ablation_results_UCI-HAR_subject-dep.csv\", index=False)\n",
    "    print(\"Saved ablation_results_UCI-HAR_subject-dep.csv\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main_ablation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
