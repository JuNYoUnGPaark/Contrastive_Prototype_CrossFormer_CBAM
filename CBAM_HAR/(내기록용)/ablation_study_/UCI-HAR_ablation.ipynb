{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7340188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-1. Library Import \n",
    "# =================================================================================\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fvcore (optional)\n",
    "try:\n",
    "    from fvcore.nn import FlopCountAnalysis\n",
    "    FVCORE_AVAILABLE = True\n",
    "except Exception:\n",
    "    FlopCountAnalysis = None\n",
    "    FVCORE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9262ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-2. Define CONFIG \n",
    "# =================================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # ---------------------------\n",
    "    # Reproducibility / Experiment ID\n",
    "    # ---------------------------\n",
    "    \"seed\": 42,\n",
    "    \"dataset_name\": \"UCI-HAR\",\n",
    "\n",
    "    # ---------------------------\n",
    "    # Data / Split\n",
    "    # ---------------------------\n",
    "    \"data_dir\": \"C://Users/park9/CBAM_HAR/UCI-HAR/data\", \n",
    "    \"val_ratio\": 0.2,  # UCI-HAR: train split -> train/val portion\n",
    "    \"batch_size\": 128,  # dataloader\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training\n",
    "    # ---------------------------\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"contrast_weight\": 0.25,  # total_loss = CE + contrast_weight * contrast_loss\n",
    "    \"scheduler_type\": \"cosine\", # scheduler (CosineAnnealingLR)\n",
    "    \"scheduler_T_max\": 100,  # CosineAnnealingLR(T_max=EPOCHS)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Model architecture (UCI-HAR)\n",
    "    # ---------------------------\n",
    "    \"in_channels\": 9,        # 9 inertial signals (acc/gyro/total xyz)\n",
    "    \"seq_len\": 128,          # window length\n",
    "    \"n_classes\": 6,          # HAR classes\n",
    "    \"n_prototypes\": 6,       # prototype counts\n",
    "\n",
    "    \"embed_dim\": 64,         # Conv1d -> Transformer base dim\n",
    "    \"reduced_dim\": 32,       # if we use_dim_reduction == True\n",
    "    \"use_dim_reduction\": False,\n",
    "\n",
    "    \"n_heads\": 8,            # multi-head attention heads in CrossFormer block\n",
    "    \"dropout\": 0.1,\n",
    "\n",
    "    \"kernel_size\": 11,       # kernel sizes used in Conv1d embedding and CBAM temporal attention\n",
    "\n",
    "    # ---------------------------\n",
    "    # Feature toggles (ablations)\n",
    "    # ---------------------------\n",
    "    \"use_cbam\": True,        # CBAM\n",
    "    \"use_crossformer\": True, # CrossFormerBlock\n",
    "    \"use_contrast\": True,    # include contrastive prototype loss during training\n",
    "\n",
    "    # ---------------------------\n",
    "    # Contrast / Prototype behavior\n",
    "    # ---------------------------\n",
    "    \"temperature\": 0.05,     # temperature in contrastive loss\n",
    "\n",
    "    # ---------------------------\n",
    "    # Logging / Debug convenience\n",
    "    # ---------------------------\n",
    "    \"print_every\": 100,        # print every or bumped epoch\n",
    "    \"do_tsne\": True,        # t-SNE\n",
    "    \"profile_model\": True,  # FLOPs / Params\n",
    "}\n",
    "\n",
    "def pretty_print_config(cfg: dict):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT CONFIG\")\n",
    "    print(\"-\" * 80)\n",
    "    # key alignment\n",
    "    max_k = max(len(k) for k in cfg.keys())\n",
    "    for k in sorted(cfg.keys()):\n",
    "        print(f\"{k.ljust(max_k)} : {cfg[k]}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8cbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-3. Reproducibility helpers\n",
    "# =================================================================================\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Fix random seeds for reproducibility across random, numpy, torch (cpu & cuda).\n",
    "    Also configures CuDNN for deterministic behavior.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cudnn deterministic mode: reproducible but may be slower\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    \"\"\"\n",
    "    To make DataLoader workers deterministic.\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae708afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 1-4. Dataset: UCI-HAR\n",
    "# =================================================================================\n",
    "class UCIHARDataset(Dataset):\n",
    "    \"\"\"\n",
    "    UCI HAR Dataset loader.\n",
    "\n",
    "    Directory structure (expected):\n",
    "        data_dir/\n",
    "          train/\n",
    "            Inertial Signals/\n",
    "              body_acc_x_train.txt\n",
    "              body_acc_y_train.txt\n",
    "              ...\n",
    "              total_acc_z_train.txt\n",
    "            y_train.txt\n",
    "          test/\n",
    "            Inertial Signals/\n",
    "              body_acc_x_test.txt\n",
    "              ...\n",
    "            y_test.txt\n",
    "\n",
    "    Notes:\n",
    "    - UCI-HAR은 이미 subject-wise로 train/test가 고정되어 제공됨.\n",
    "      즉 train/ 폴더와 test/ 폴더에 다른 사람들(subjects)이 들어 있음.\n",
    "    - 우리는 train/ 안에서만 val을 떼어내서 train/val로 쓰고,\n",
    "      test/는 그대로 최종 generalization 평가용으로 사용.\n",
    "    - 라벨은 원본이 1~6이라서 여기서 0~5로 shift.\n",
    "    - 시퀀스 길이 T=128, 채널 수 C=9 (가속도/자이로/total_acc 각각 x/y/z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, train: bool = True):\n",
    "        subset = \"train\" if train else \"test\"\n",
    "\n",
    "        signal_types = [\n",
    "            \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
    "            \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
    "            \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
    "        ]  # (N, 9, 128)\n",
    "\n",
    "        signals = []\n",
    "        for signal in signal_types:\n",
    "            filename = os.path.join(\n",
    "                data_dir,\n",
    "                subset,\n",
    "                \"Inertial Signals\",\n",
    "                f\"{signal}_{subset}.txt\"\n",
    "            )\n",
    "\n",
    "            with open(filename, 'r') as f:\n",
    "                data = np.loadtxt(f)  # (N, 128) per channel\n",
    "            signals.append(data)\n",
    "\n",
    "        self.X = np.stack(signals, axis=1)  # stack -> (N, 9, 128)\n",
    "\n",
    "        # label: (N,), values in {1..6} -> shift to {0..5}\n",
    "        label_file = os.path.join(data_dir, subset, f'y_{subset}.txt')\n",
    "        with open(label_file, 'r') as f:\n",
    "            self.y = np.loadtxt(f, dtype=np.int32) - 1  # now {0..5}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            X_i: torch.FloatTensor, shape (C, T) = (9, 128)\n",
    "            y_i: torch.LongTensor scalar, shape ()\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c9182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-1. CBAM1D\n",
    "# =================================================================================\n",
    "class ChannelAttention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Channel attention for 1D signals.\n",
    "    Input shape:  (B, C, T)\n",
    "    Output shape: (B, C, T) with per-channel reweighting.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x : (B, C, T)\n",
    "        avg_out = self.avg_pool(x).squeeze(-1)  # (B, C)\n",
    "        max_out = self.max_pool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        avg_out = self.fc(avg_out)  # (B, C)\n",
    "        max_out = self.fc(max_out)  # (B, C)\n",
    "\n",
    "        out = (avg_out + max_out).unsqueeze(-1)  # (B, C, 1)\n",
    "        scale = self.sigmoid(out)  # (B, C, 1)\n",
    "        return x * scale  # broadcast along T\n",
    "\n",
    "\n",
    "class TemporalAttention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention for 1D signals.\n",
    "    Input shape:  (B, C, T)\n",
    "    Output shape: (B, C, T) with per-timestep reweighting.\n",
    "\n",
    "    Internally uses a conv over [avg_pool; max_pool] across channels,\n",
    "    so conv input channel dim is fixed to 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=2,\n",
    "            out_channels=1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, C, T)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # (B, 1, T)\n",
    "\n",
    "        attn_in = torch.cat([avg_out, max_out], dim=1)  # (B, 2, T)\n",
    "        attn_map = self.conv(attn_in)                        # (B, 1, T)\n",
    "        attn_map = self.sigmoid(attn_map)\n",
    "        return x * attn_map  # broadcast along C\n",
    "\n",
    "\n",
    "class CBAM1D(nn.Module):\n",
    "    \"\"\"\n",
    "    CBAM-style attention for 1D sensor sequences.\n",
    "    Does channel attention -> temporal attention.\n",
    "\n",
    "    Input / Output shape: (B, C, T)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel_att = ChannelAttention1D(channels, reduction=reduction)\n",
    "        self.temporal_att = TemporalAttention1D(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x : (B, C, T)\n",
    "        x = self.channel_att(x)\n",
    "        x = self.temporal_att(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7715cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-2. CrossFormer Block (Cross-Attn between tokens and learnable prototypes)\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 n_prototypes: int = 6,\n",
    "                 n_heads: int = 4,\n",
    "                 mlp_ratio: float = 2.0,\n",
    "                 dropout: float = 0.1,\n",
    "                 initial_prototypes: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: token embedding dim\n",
    "            n_prototypes: number of learnable class prototypes\n",
    "            n_heads: attention heads (must divide dim)\n",
    "            mlp_ratio: FFN expansion ratio\n",
    "            dropout: dropout inside MHA/MLP\n",
    "            initial_prototypes: optional (n_prototypes, dim) tensor to init prototypes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Learnable prototypes\n",
    "        self.prototypes = nn.Parameter(torch.randn(n_prototypes, dim))\n",
    "\n",
    "        if initial_prototypes is not None:\n",
    "            assert initial_prototypes.shape == self.prototypes.shape, \\\n",
    "                f\"Shape mismatch: initial_prototypes {initial_prototypes.shape} vs self.prototypes {self.prototypes.shape}\"\n",
    "            self.prototypes.data.copy_(initial_prototypes)\n",
    "            print(\"Prototypes initialized with calculated mean features.\")\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.prototypes)\n",
    "            print(\"Prototypes initialized with Xavier Uniform.\")\n",
    "\n",
    "        # Cross-attention (tokens Q) x (prototypes K,V)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Self-attention on tokens\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # FFN\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Projection for contrastive proto features\n",
    "        self.proto_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                return_proto_features: bool = False,\n",
    "                skip_cross_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, C=dim)\n",
    "            return_proto_features: if True, also returns pooled/proj features for contrast\n",
    "            skip_cross_attention: if True, bypass cross-attn (used for proto init feature extraction)\n",
    "        Returns:\n",
    "            If return_proto_features:\n",
    "                (x_out, proto_features, cross_attn_weights)\n",
    "            else:\n",
    "                x_out\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        attn_weights = None\n",
    "\n",
    "        # 1) Cross-Attention (optional)\n",
    "        if not skip_cross_attention:\n",
    "            # normalize prototypes for stable attention keys/values\n",
    "            normalized_prototypes = F.normalize(self.prototypes, dim=1, eps=1e-6)  # (P, C)\n",
    "            prototypes = normalized_prototypes.unsqueeze(0).expand(B, -1, -1)  # (B, P, C)\n",
    "\n",
    "            x_norm = self.norm1(x)\n",
    "            cross_out, attn_weights = self.cross_attn(x_norm, prototypes, prototypes)\n",
    "            x = x + cross_out  # residual\n",
    "\n",
    "        # 2) Self-Attention\n",
    "        x_norm = self.norm2(x)\n",
    "        self_out, _ = self.self_attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self_out  # residual\n",
    "\n",
    "        # 3) FFN\n",
    "        x = x + self.mlp(self.norm3(x))  # residual\n",
    "\n",
    "        if return_proto_features:\n",
    "            proto_features = x.mean(dim=1)  # (B, C)\n",
    "            proto_features = self.proto_proj(proto_features)  # (B, C)\n",
    "            return x, proto_features, attn_weights\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f56fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-3. Contrastive Prototype Loss\n",
    "# =================================================================================\n",
    "class ContrastivePrototypeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised prototype contrast loss.\n",
    "\n",
    "    For each sample embedding f_i and class prototypes P (1 per class),\n",
    "    we compute a softmax over cosine similarities and apply cross-entropy\n",
    "    against the ground truth label.\n",
    "\n",
    "    Intuition:\n",
    "      - Pull sample features closer to their class prototype.\n",
    "      - Push them away from other class prototypes.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,    # (B, D)\n",
    "        prototypes: torch.Tensor,  # (num_classes, D)\n",
    "        labels: torch.Tensor       # (B,)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features:  batch embeddings (B, D)\n",
    "            prototypes: class prototype matrix (num_classes, D)\n",
    "                        usually num_classes == n_prototypes\n",
    "            labels:    ground-truth class indices, shape (B,), dtype long\n",
    "\n",
    "        Returns:\n",
    "            scalar loss (tensor)\n",
    "        \"\"\"\n",
    "        # L2 normalize\n",
    "        features = F.normalize(features, dim=1, eps=1e-6)\n",
    "        prototypes = F.normalize(prototypes, dim=1, eps=1e-6)\n",
    "\n",
    "        # cosine similarity\n",
    "        logits = torch.matmul(features, prototypes.t()) / self.temperature  # (B, num_classes)\n",
    "\n",
    "        # InfoNCE Loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d911c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-4. Final Model: embedding + (CBAM) + CrossFormer + classifier\n",
    "# =================================================================================\n",
    "class ContrastCrossFormerCBAM_HAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Sensor sequence classifier with:\n",
    "      - Conv1d embedding (+ BatchNorm + GELU + Dropout)\n",
    "      - Optional CBAM (channel & temporal attention)\n",
    "      - Either:\n",
    "          (A) CrossFormer block with learnable prototypes\n",
    "        or\n",
    "          (B) TransformerEncoderLayer-only self-attention fallback\n",
    "      - Global average pooling over time\n",
    "      - MLP classifier head\n",
    "      - (Optional) contrastive prototype loss\n",
    "\n",
    "    Args:\n",
    "        in_channels:   # sensor channels (e.g. 9 for UCI-HAR)\n",
    "        seq_len:       # sequence length (e.g. 128 for UCI-HAR); mostly for reference / profiling\n",
    "        embed_dim:     # conv embedding dim (and final feature dim if no reduction)\n",
    "        reduced_dim:   # reduced dim if use_dim_reduction=True\n",
    "        n_classes:     # num activity classes\n",
    "        n_prototypes:  # number of learnable prototypes in CrossFormer\n",
    "        n_heads:       # attention heads for CrossFormer/self-attn\n",
    "        kernel_size:   # conv1d kernel size for embedding, and CBAM temporal kernel\n",
    "        dropout:       # dropout rate\n",
    "        temperature:   # temperature for contrastive loss\n",
    "        initial_prototypes:  # tensor to init CrossFormerBlock.prototypes, or None\n",
    "        use_cbam:      # if True, apply CBAM after embedding\n",
    "        use_crossformer:     # if True, use CrossFormerBlock; else use vanilla self-attn block\n",
    "        use_contrast:        # if True, model can return contrastive loss\n",
    "        use_dim_reduction:   # if True, reduce dim before attention and restore after\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 9,\n",
    "                 seq_len: int = 128,\n",
    "                 embed_dim: int = 64,\n",
    "                 reduced_dim: int = 32,\n",
    "                 n_classes: int = 6,\n",
    "                 n_prototypes: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 kernel_size: int = 7,\n",
    "                 dropout: float = 0.1,\n",
    "                 temperature: float = 0.07,\n",
    "                 initial_prototypes: torch.Tensor = None,\n",
    "                 use_cbam: bool = True,\n",
    "                 use_crossformer: bool = True,\n",
    "                 use_contrast: bool = True,\n",
    "                 use_dim_reduction: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save config\n",
    "        self.in_channels = in_channels\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.reduced_dim = reduced_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.n_heads = n_heads\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.use_cbam = use_cbam\n",
    "        self.use_crossformer = use_crossformer\n",
    "        self.use_contrast = use_contrast\n",
    "        self.use_dim_reduction = use_dim_reduction\n",
    "\n",
    "        # 1) Embedding: Conv1d -> BN -> GELU -> Dropout\n",
    "        #    Input:  (B, in_channels, T)\n",
    "        #    Output: (B, embed_dim, T)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                embed_dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=(kernel_size - 1) // 2,  # \"same\" padding for odd kernel\n",
    "            ),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # 2) Optional CBAM\n",
    "        #    Still (B, embed_dim, T)\n",
    "        if self.use_cbam:\n",
    "            self.cbam = CBAM1D(\n",
    "                channels=embed_dim,\n",
    "                reduction=8,\n",
    "                kernel_size=kernel_size,\n",
    "            )\n",
    "\n",
    "        # 3) (Optional) Dim Reduction before attention\n",
    "        #    We'll call this dimension 'working_dim'.\n",
    "        #    If use_dim_reduction=False, working_dim == embed_dim.\n",
    "        working_dim = reduced_dim if use_dim_reduction else embed_dim\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_reduce = nn.Linear(embed_dim, reduced_dim)\n",
    "\n",
    "        # 4) Attention backbone\n",
    "        #    A) CrossFormerBlock (our prototype-based block)\n",
    "        #    B) Fallback: vanilla TransformerEncoderLayer\n",
    "        #    Input to these blocks: (B, T, working_dim)\n",
    "        #    Output shape stays (B, T, working_dim)\n",
    "        if self.use_crossformer:\n",
    "            self.crossformer = ContrastCrossFormerBlock(\n",
    "                dim=working_dim,\n",
    "                n_prototypes=n_prototypes,\n",
    "                n_heads=n_heads,\n",
    "                mlp_ratio=2.0,\n",
    "                dropout=dropout,\n",
    "                initial_prototypes=initial_prototypes,\n",
    "            )\n",
    "        else:\n",
    "            # TransformerEncoderLayer returns same shape (B, T, working_dim)\n",
    "            self.self_attn = nn.TransformerEncoderLayer(\n",
    "                d_model=working_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=int(working_dim * 2),\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        # 5) (Optional) Dim restore after attention\n",
    "        #    Back to embed_dim if we reduced.\n",
    "        if self.use_dim_reduction:\n",
    "            self.dim_restore = nn.Linear(reduced_dim, embed_dim)\n",
    "\n",
    "        # 6) Temporal pooling + classifier head\n",
    "        #    After attention we get (B, T, embed_dim)\n",
    "        #    -> transpose to (B, embed_dim, T)\n",
    "        #    -> AdaptiveAvgPool1d(1) -> (B, embed_dim)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_classes),\n",
    "        )\n",
    "\n",
    "        # 7) Contrastive loss module (optional)\n",
    "        if self.use_contrast and self.use_crossformer:\n",
    "            self.contrast_loss = ContrastivePrototypeLoss(temperature=temperature)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                labels: torch.Tensor = None,\n",
    "                return_contrast_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C_in, T)  e.g. (B, 9, 128)\n",
    "            labels: (B,) long tensor with class indices [0..n_classes-1]\n",
    "            return_contrast_loss: if True, we also compute contrastive loss\n",
    "\n",
    "        Returns:\n",
    "            if return_contrast_loss and use_contrast:\n",
    "                (logits, contrast_loss)\n",
    "            else:\n",
    "                logits\n",
    "        \"\"\"\n",
    "        # 1) Conv embedding (+CBAM)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if self.use_cbam:\n",
    "            x = self.cbam(x)\n",
    "\n",
    "        # 2) Prepare for attention\n",
    "        #    (B, embed_dim, T) -> (B, T, embed_dim)\n",
    "        #    Optionally reduce dim\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_reduce(x)\n",
    "\n",
    "        # 3) Attention backbone\n",
    "        proto_features = None\n",
    "        if self.use_crossformer:\n",
    "            if return_contrast_loss and self.use_contrast:\n",
    "                x, proto_features, _ = self.crossformer(x, return_proto_features=True,\n",
    "                                                        skip_cross_attention=False)\n",
    "            else:\n",
    "                x = self.crossformer(x, return_proto_features=False,\n",
    "                                     skip_cross_attention=False)\n",
    "        else:\n",
    "            x = self.self_attn(x)\n",
    "\n",
    "        # 4) Restore dim if reduced\n",
    "        if self.use_dim_reduction:\n",
    "            x = self.dim_restore(x)\n",
    "\n",
    "        # 5) Pool over time\n",
    "        #    (B, T, embed_dim) -> (B, embed_dim, T) -> pool -> (B, embed_dim)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        feat_vec = self.pool(x).squeeze(-1)\n",
    "\n",
    "        # 6) Classifier\n",
    "        logits = self.classifier(feat_vec)\n",
    "\n",
    "        # 7) Optional contrastive term\n",
    "        if (\n",
    "            return_contrast_loss\n",
    "            and self.use_contrast\n",
    "            and proto_features is not None\n",
    "            and labels is not None\n",
    "        ):\n",
    "            contrast_loss = self.contrast_loss(\n",
    "                proto_features,                # (B, dim)\n",
    "                self.crossformer.prototypes,   # (n_prototypes, dim)\n",
    "                labels                         # (B,)\n",
    "            )\n",
    "            return logits, contrast_loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2cada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 2-5. Prototype Initialization\n",
    "# =================================================================================\n",
    "def get_mean_prototypes(train_full_dataset, device, config):\n",
    "\n",
    "    temp_model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=config['in_channels'],\n",
    "        seq_len=config['seq_len'],\n",
    "        n_classes=config['n_classes'],\n",
    "        n_prototypes=config['n_prototypes'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        reduced_dim=config['reduced_dim'], \n",
    "        n_heads=config['n_heads'],\n",
    "        kernel_size=config['kernel_size'],\n",
    "        dropout=config['dropout'],\n",
    "        temperature=config['temperature'],\n",
    "        initial_prototypes=None,\n",
    "        use_cbam=config[\"use_cbam\"],\n",
    "        use_crossformer=config[\"use_crossformer\"],\n",
    "        use_contrast=False,  # 중요: 여기서는 contrast loss 안 씀\n",
    "        use_dim_reduction=config['use_dim_reduction']\n",
    "    ).to(device)\n",
    "\n",
    "    temp_model.eval()\n",
    "\n",
    "    temp_loader = DataLoader(\n",
    "        train_full_dataset,\n",
    "        batch_size=config[\"batch_size\"],  # CONFIG 키 맞춤 (BATCH_SIZE -> batch_size)\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(temp_loader, desc=\"Prototype Init\"):\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            x = temp_model.embedding(batch_x)\n",
    "            if temp_model.use_cbam:\n",
    "                x = temp_model.cbam(x)\n",
    "\n",
    "            x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "            if temp_model.use_dim_reduction:\n",
    "                x = temp_model.dim_reduce(x)\n",
    "\n",
    "            if temp_model.use_crossformer:\n",
    "                x = temp_model.crossformer(\n",
    "                    x,\n",
    "                    return_proto_features=False,\n",
    "                    skip_cross_attention=True\n",
    "                )                                   # (B, T, working_dim)\n",
    "            else:\n",
    "                x = temp_model.self_attn(x)         # (B, T, working_dim)\n",
    "\n",
    "            # Dim restore (if reduction was used)\n",
    "            if temp_model.use_dim_reduction:\n",
    "                x = temp_model.dim_restore(x)       # (B, T, embed_dim)\n",
    "\n",
    "            x = x.transpose(1, 2).contiguous()  # (B, embed_dim, T)\n",
    "\n",
    "            pooled_features = temp_model.pool(x).squeeze(-1)  # (B, embed_dim)\n",
    "\n",
    "            all_features.append(pooled_features.cpu())  # (N, embed_dim)\n",
    "            all_labels.append(batch_y.cpu())  # (N,)\n",
    "\n",
    "    num_classes = config[\"n_classes\"]\n",
    "    feature_dim = config[\"embed_dim\"]  # pooled_features의 dim과 맞춰줌\n",
    "\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    mean_prototypes = torch.zeros(num_classes, feature_dim, dtype=torch.float32)\n",
    "    for i in range(num_classes):\n",
    "        class_features = all_features[all_labels == i]\n",
    "        if len(class_features) > 0:\n",
    "            mean_prototypes[i] = class_features.mean(dim=0)\n",
    "        else:\n",
    "            mean_prototypes[i] = torch.randn(feature_dim)\n",
    "    \n",
    "    return mean_prototypes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada2ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 3-1. train & evaluation\n",
    "# =================================================================================\n",
    "def train_epoch(model,\n",
    "                dataloader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                device,\n",
    "                use_contrast=True,\n",
    "                contrast_weight=0.5):\n",
    "    model.train()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    ce_loss_sum = 0.0\n",
    "    contrast_loss_sum = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        batch_x = batch_x.to(device, non_blocking=True)\n",
    "        batch_y = batch_y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        if use_contrast and model.use_contrast and model.use_crossformer:\n",
    "            logits, contrast_loss = model(batch_x, batch_y, return_contrast_loss=True)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            total_loss = ce_loss + contrast_weight * contrast_loss\n",
    "            contrast_loss_sum  += contrast_loss.item()\n",
    "        else:\n",
    "            logits = model(batch_x)\n",
    "            ce_loss = criterion(logits, batch_y)\n",
    "            total_loss  = ce_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sum  += total_loss.item()\n",
    "        ce_loss_sum  += ce_loss.item()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "    \n",
    "    torch.cuda.synchronize() # 한 에폭 끝에서 동기화\n",
    "\n",
    "    avg_total_loss = total_loss / len(dataloader)\n",
    "    avg_ce_loss = ce_loss_sum  / len(dataloader)\n",
    "    avg_contrast_loss = contrast_loss_sum / len(dataloader) if contrast_loss_sum  > 0 else 0.0\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_total_loss, avg_ce_loss, avg_contrast_loss, acc, f1\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             criterion,\n",
    "             device,\n",
    "             use_contrast=True,\n",
    "             contrast_weight=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    ce_loss_sum = 0.0\n",
    "    contrast_loss_sum = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device, non_blocking=True)\n",
    "            batch_y = batch_y.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward (eval 모드에서는 no_grad)\n",
    "            if use_contrast and model.use_contrast and model.use_crossformer:\n",
    "                logits, contrast_loss = model(\n",
    "                    batch_x,\n",
    "                    batch_y,\n",
    "                    return_contrast_loss=True\n",
    "                )\n",
    "                ce_loss = criterion(logits, batch_y)\n",
    "                total_loss = ce_loss + contrast_weight * contrast_loss\n",
    "                contrast_loss_sum += contrast_loss.item()\n",
    "            else:\n",
    "                logits = model(batch_x)\n",
    "                ce_loss = criterion(logits, batch_y)\n",
    "                total_loss = ce_loss\n",
    "\n",
    "            total_loss_sum  += total_loss.item()\n",
    "            ce_loss_sum += ce_loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    avg_total_loss = total_loss_sum / len(dataloader)\n",
    "    avg_ce_loss = ce_loss_sum / len(dataloader)\n",
    "    avg_contrast_loss = contrast_loss_sum / len(dataloader) if contrast_loss_sum > 0 else 0.0\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_total_loss, acc, f1, all_preds, all_labels, avg_ce_loss, avg_contrast_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f770ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# 4-2. Model profiling: Param(M), FLOPs(M), Inference Time(ms)\n",
    "# =================================================================================\n",
    "def profile_model(model,\n",
    "                  sample_input: torch.Tensor,\n",
    "                  device: torch.device,\n",
    "                  warmup: int = 10,\n",
    "                  iters: int = 50):\n",
    "    \"\"\"\n",
    "    모델 구조/비용 측정:\n",
    "      - 파라미터 수 (M 단위)\n",
    "      - FLOPs per sample (M 단위, fvcore 있으면)\n",
    "      - 평균 추론 시간 (ms / sample)\n",
    "\n",
    "    fvcore가 stdout/stderr에 시끄럽게 프린트하는 걸 다 먹어버린다.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1) 파라미터 수\n",
    "    # -------------------------------------------------\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    params_m = total_params / 1e6  # million params\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
    "    #    - 모든 stdout/stderr을 임시 버퍼로 리다이렉트해서\n",
    "    #      \"Unsupported operator ...\" 같은 중얼거림을 완전히 감춘다.\n",
    "    # -------------------------------------------------\n",
    "    flops_m = None\n",
    "    if \"FVCORE_AVAILABLE\" in globals() and FVCORE_AVAILABLE and FlopCountAnalysis is not None:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                fake_out = io.StringIO()\n",
    "                fake_err = io.StringIO()\n",
    "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
    "                    flops = FlopCountAnalysis(model, (sample_input.to(device),))\n",
    "                    total_flops = flops.total()\n",
    "            flops_m = total_flops / 1e6  # to millions\n",
    "        except Exception:\n",
    "            flops_m = None\n",
    "    else:\n",
    "        flops_m = None\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) 추론 시간 측정\n",
    "    # -------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        # warmup\n",
    "        for _ in range(warmup):\n",
    "            _ = model(sample_input.to(device))\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(iters):\n",
    "            _ = model(sample_input.to(device))\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "    avg_sec = (end - start) / iters\n",
    "    inference_ms = avg_sec * 1000.0\n",
    "\n",
    "    return {\n",
    "        \"params_m\": params_m,\n",
    "        \"flops_m\": flops_m,\n",
    "        \"inference_ms\": inference_ms,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def print_model_profile(stats_dict):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Parameters      : {stats_dict['params_m']:.4f} M\")\n",
    "    print(f\"FLOPs / sample : {stats_dict['flops_m']:.3f} M\")\n",
    "    print(f\"Infer Time     : {stats_dict['inference_ms']:.2f} ms/sample\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db173191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ablation Study (independent full runs)\n",
      "Device: cuda\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "### Running Embed+CBAM_only\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/100]\n",
      "  train | total=0.0015  ce=0.0464  ct=0.0000  acc=0.9787  f1=0.9787\n",
      "  val   | total=0.0451  ce=0.0451  ct=0.0000  acc=0.9830  f1=0.9830\n",
      "================================================================================\n",
      "[Embed+CBAM_only] Done.\n",
      "  Best Val Acc: 0.9864 @ epoch 87\n",
      "  Test(best ckpt): acc=0.9352, f1=0.9350, total_loss=0.2500, ce=0.2500, ct=0.0000\n",
      "================================================================================\n",
      "Parameters      : 0.0456 M\n",
      "FLOPs / sample : 3.016 M\n",
      "Infer Time     : 1.34 ms/sample\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "### Running CBAM+CrossFormer_noContrast\n",
      "################################################################################\n",
      "Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prototype Init: 100%|██████████| 58/58 [00:00<00:00, 176.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes initialized with calculated mean features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/100]\n",
      "  train | total=0.0001  ce=0.0120  ct=0.0000  acc=0.9951  f1=0.9951\n",
      "  val   | total=0.0352  ce=0.0352  ct=0.0000  acc=0.9918  f1=0.9918\n",
      "================================================================================\n",
      "[CBAM+CrossFormer_noContrast] Done.\n",
      "  Best Val Acc: 0.9918 @ epoch 85\n",
      "  Test(best ckpt): acc=0.9539, f1=0.9535, total_loss=0.2500, ce=0.2500, ct=0.0000\n",
      "================================================================================\n",
      "Parameters      : 0.0711 M\n",
      "FLOPs / sample : 4.253 M\n",
      "Infer Time     : 2.12 ms/sample\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "### Running CBAM+CrossFormer+Contrast (Full Model)\n",
      "################################################################################\n",
      "Prototypes initialized with Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prototype Init: 100%|██████████| 58/58 [00:00<00:00, 182.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes initialized with calculated mean features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/100]\n",
      "  train | total=0.0003  ce=0.0124  ct=0.0125  acc=0.9956  f1=0.9956\n",
      "  val   | total=0.0422  ce=0.0334  ct=0.0349  acc=0.9898  f1=0.9898\n",
      "================================================================================\n",
      "[CBAM+CrossFormer+Contrast (Full Model)] Done.\n",
      "  Best Val Acc: 0.9918 @ epoch 80\n",
      "  Test(best ckpt): acc=0.9667, f1=0.9666, total_loss=0.1865, ce=0.1520, ct=0.1380\n",
      "================================================================================\n",
      "Parameters      : 0.0711 M\n",
      "FLOPs / sample : 4.253 M\n",
      "Infer Time     : 2.18 ms/sample\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Ablation Summary\n",
      "================================================================================\n",
      "                                   tag  use_cbam  use_crossformer  use_contrast  test_acc  test_f1  params_m  flops_m  inference_ms  best_epoch\n",
      "                       Embed+CBAM_only      True            False         False  0.935188 0.935029  0.045596 3.015808      1.338530          87\n",
      "           CBAM+CrossFormer_noContrast      True             True         False  0.953851 0.953463  0.071068 4.252800      2.121449          85\n",
      "CBAM+CrossFormer+Contrast (Full Model)      True             True          True  0.966746 0.966552  0.071068 4.252800      2.175279          80\n",
      "Saved ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(base_config, overrides, device):\n",
    "    \"\"\"\n",
    "    base_config: 전역 CONFIG (원본)\n",
    "    overrides:   {\"tag\": \"...\", \"use_cbam\":..., \"use_crossformer\":..., \"use_contrast\":...}\n",
    "                 -> 이 값만 바꿔서 single run 수행\n",
    "    device: torch.device\n",
    "\n",
    "    리턴: dict (acc, f1 등)\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # 0. config 준비\n",
    "    # -----------------------------\n",
    "    cfg = copy.deepcopy(base_config)\n",
    "    for k, v in overrides.items():\n",
    "        if k == \"tag\":\n",
    "            continue\n",
    "        cfg[k] = v\n",
    "\n",
    "    # 시드 고정 (main과 동일)\n",
    "    seed_everything(cfg[\"seed\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Dataset & Split (main 그대로)\n",
    "    # -----------------------------\n",
    "    train_full_dataset = UCIHARDataset(cfg[\"data_dir\"], train=True)\n",
    "    test_dataset       = UCIHARDataset(cfg[\"data_dir\"], train=False)\n",
    "\n",
    "    train_size = int((1.0 - cfg[\"val_ratio\"]) * len(train_full_dataset))\n",
    "    val_size   = len(train_full_dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_full_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(cfg[\"seed\"])\n",
    "    )\n",
    "\n",
    "    g = torch.Generator().manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Prototype Init (main 그대로)\n",
    "    # -----------------------------\n",
    "    if cfg[\"use_crossformer\"]:\n",
    "        initial_prototypes = get_mean_prototypes(\n",
    "            train_full_dataset,\n",
    "            device,\n",
    "            cfg\n",
    "        )\n",
    "    else:\n",
    "        initial_prototypes = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Model 생성 (main 그대로)\n",
    "    # -----------------------------\n",
    "    model = ContrastCrossFormerCBAM_HAR(\n",
    "        in_channels=cfg[\"in_channels\"],\n",
    "        seq_len=cfg[\"seq_len\"],\n",
    "        embed_dim=cfg[\"embed_dim\"],\n",
    "        reduced_dim=cfg[\"reduced_dim\"],\n",
    "        n_classes=cfg[\"n_classes\"],\n",
    "        n_prototypes=cfg[\"n_prototypes\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        kernel_size=cfg[\"kernel_size\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        initial_prototypes=initial_prototypes,\n",
    "        use_cbam=cfg[\"use_cbam\"],\n",
    "        use_crossformer=cfg[\"use_crossformer\"],\n",
    "        use_contrast=cfg[\"use_contrast\"],\n",
    "        use_dim_reduction=cfg[\"use_dim_reduction\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # optimizer / scheduler / criterion (main 그대로)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg[\"learning_rate\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    if cfg[\"scheduler_type\"] == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg[\"scheduler_T_max\"],\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "\n",
    "    history = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Train Loop (main 그대로)\n",
    "    # -----------------------------\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        train_total, train_ce, train_ct, train_acc, train_f1 = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            use_contrast=cfg[\"use_contrast\"],\n",
    "            contrast_weight=cfg[\"contrast_weight\"],\n",
    "        )\n",
    "\n",
    "        val_total, val_acc, val_f1, _, _, val_ce, val_ct = evaluate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device,\n",
    "            use_contrast=cfg[\"use_contrast\"],\n",
    "            contrast_weight=cfg[\"contrast_weight\"],\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_total\": train_total,\n",
    "            \"train_ce\": train_ce,\n",
    "            \"train_ct\": train_ct,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_total\": val_total,\n",
    "            \"val_ce\": val_ce,\n",
    "            \"val_ct\": val_ct,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "        })\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if (epoch + 1) % cfg[\"print_every\"] == 0:\n",
    "            print(f\"[{epoch+1:03d}/{cfg['epochs']:03d}]\")\n",
    "            print(\n",
    "                \"  train | \"\n",
    "                f\"total={train_total:.4f}  \"\n",
    "                f\"ce={train_ce:.4f}  \"\n",
    "                f\"ct={train_ct:.4f}  \"\n",
    "                f\"acc={train_acc:.4f}  \"\n",
    "                f\"f1={train_f1:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                \"  val   | \"\n",
    "                f\"total={val_total:.4f}  \"\n",
    "                f\"ce={val_ce:.4f}  \"\n",
    "                f\"ct={val_ct:.4f}  \"\n",
    "                f\"acc={val_acc:.4f}  \"\n",
    "                f\"f1={val_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Best ckpt로 교체 후 TEST 평가 (main 그대로)\n",
    "    # -----------------------------\n",
    "    assert best_state is not None\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    test_total, test_acc, test_f1, test_preds, test_labels, test_ce, test_ct = evaluate(\n",
    "        model,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "        use_contrast=cfg[\"use_contrast\"],\n",
    "        contrast_weight=cfg[\"contrast_weight\"],\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[{overrides['tag']}] Done.\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.4f} @ epoch {best_epoch}\")\n",
    "    print(\n",
    "        f\"  Test(best ckpt): \"\n",
    "        f\"acc={test_acc:.4f}, f1={test_f1:.4f}, \"\n",
    "        f\"total_loss={test_total:.4f}, ce={test_ce:.4f}, ct={test_ct:.4f}\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. 프로파일링 (optional, main 그대로)\n",
    "    # -----------------------------\n",
    "    stats_profile = None\n",
    "    if cfg[\"profile_model\"]:\n",
    "        dummy_input = torch.randn(\n",
    "            1, cfg[\"in_channels\"], cfg[\"seq_len\"],\n",
    "        )\n",
    "        stats_profile = profile_model(model, dummy_input, device)\n",
    "        print_model_profile(stats_profile)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8. 결과 dict 반환 (표용)\n",
    "    # -----------------------------\n",
    "    return {\n",
    "        \"tag\": overrides[\"tag\"],\n",
    "        \"use_cbam\":        cfg[\"use_cbam\"],\n",
    "        \"use_crossformer\": cfg[\"use_crossformer\"],\n",
    "        \"use_contrast\":    cfg[\"use_contrast\"],\n",
    "        \"best_val_acc\":    best_val_acc,\n",
    "        \"test_acc\":        test_acc,\n",
    "        \"test_f1\":         test_f1,\n",
    "        \"test_total_loss\": test_total,\n",
    "        \"test_ce\":         test_ce,\n",
    "        \"test_ct\":         test_ct,\n",
    "        \"params_m\":        stats_profile[\"params_m\"]     if stats_profile else None,\n",
    "        \"flops_m\":         stats_profile[\"flops_m\"]      if stats_profile else None,\n",
    "        \"inference_ms\":    stats_profile[\"inference_ms\"] if stats_profile else None,\n",
    "        \"best_epoch\":      best_epoch,\n",
    "    }, history\n",
    "\n",
    "\n",
    "def main_ablation():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Ablation Study (independent full runs)\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    variants = [\n",
    "        {\n",
    "            \"tag\": \"Embed+CBAM_only\",\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": False,\n",
    "            \"use_contrast\": False,\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"CBAM+CrossFormer_noContrast\",\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": True,\n",
    "            \"use_contrast\": False,\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"CBAM+CrossFormer+Contrast (Full Model)\",  # full model\n",
    "            \"use_cbam\": True,\n",
    "            \"use_crossformer\": True,\n",
    "            \"use_contrast\": True,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_rows = []\n",
    "    all_histories = {}\n",
    "\n",
    "    for v in variants:\n",
    "        print(\"\\n\" + \"#\" * 80)\n",
    "        print(f\"### Running {v['tag']}\")\n",
    "        print(\"#\" * 80)\n",
    "\n",
    "        row, hist = run_experiment(CONFIG, v, device)\n",
    "        all_rows.append(row)\n",
    "        all_histories[v[\"tag\"]] = hist\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"tag\",\n",
    "        \"use_cbam\",\n",
    "        \"use_crossformer\",\n",
    "        \"use_contrast\",\n",
    "        \"test_acc\",\n",
    "        \"test_f1\",\n",
    "        \"params_m\",\n",
    "        \"flops_m\",\n",
    "        \"inference_ms\",\n",
    "        \"best_epoch\",\n",
    "    ])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Ablation Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    df.to_csv(\"ablation_results.csv\", index=False)\n",
    "    print(\"Saved ablation_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_ablation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
