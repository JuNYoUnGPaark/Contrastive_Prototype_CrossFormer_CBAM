{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad7451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Random Seed ========================\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f78ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Ïã§Ìóò ÏÑ§Ï†ï\"\"\"\n",
    "    data_dir: str = \"C://Users/park9/HAR/SSL_HAR/data\"\n",
    "    save_dir: str = \"C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\"\n",
    "\n",
    "    # SSL Pretrain ÌååÎùºÎØ∏ÌÑ∞\n",
    "    pretrain_epochs: int = 100\n",
    "    pretrain_batch_size: int = 512  # ‚úÖ InfoNCE ÏÑ±Îä• Ìñ•ÏÉÅ\n",
    "    pretrain_lr: float = 1e-3\n",
    "    pretrain_warmup_epochs: int = 10  # ‚úÖ Warmup\n",
    "\n",
    "    # Supervised / Linear Eval / Fine-tune ÌååÎùºÎØ∏ÌÑ∞\n",
    "    finetune_epochs: int = 50\n",
    "    finetune_batch_size: int = 128\n",
    "    finetune_lr: float = 3e-4\n",
    "    finetune_warmup_epochs: int = 5  # ‚úÖ Warmup\n",
    "    finetune_backbone_lr_ratio: float = 0.1  # ‚úÖ Î∞±Î≥∏ LR ÎπÑÏú®\n",
    "\n",
    "    # Í≥µÌÜµ ÌååÎùºÎØ∏ÌÑ∞\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.05\n",
    "    use_ema: bool = True  # ‚úÖ EMA ÏÇ¨Ïö©\n",
    "    ema_decay: float = 0.9995  # ‚úÖ EMA decay\n",
    "    consistency_weight: float = 0.2  # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ Í∞ÄÏ§ëÏπò\n",
    "\n",
    "    # Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    hyperbolic_c_init: float = 1.0  # ‚úÖ Ï¥àÍ∏∞Í∞í (ÌïôÏäµ Í∞ÄÎä•)\n",
    "\n",
    "    # SSL ÌååÎùºÎØ∏ÌÑ∞\n",
    "    temperature: float = 0.07\n",
    "    projection_dim: int = 128\n",
    "\n",
    "    # Augmentation ÌååÎùºÎØ∏ÌÑ∞ (‚úÖ Ï†ÑÏù¥ Í∞ïÍ±¥ÏÑ± ÏµúÏ†ÅÌôî)\n",
    "    aug_jitter_scale: float = 0.05\n",
    "    aug_scale_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    aug_channel_drop_prob: float = 0.2\n",
    "    aug_time_warp_prob: float = 0.10  # ‚úÖ 0.3 ‚Üí 0.10\n",
    "    aug_cutout_prob: float = 0.20     # ‚úÖ 0.3 ‚Üí 0.20\n",
    "    aug_cutout_ratio: float = 0.10    # ‚úÖ 0.2 ‚Üí 0.10\n",
    "\n",
    "    # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c29fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Dataset Configuration ========================\n",
    "\n",
    "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
    "RAW_CHANNELS = [\n",
    "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
    "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
    "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
    "]\n",
    "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
    "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
    "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
    "\n",
    "def load_split_raw(root: str, split: str):\n",
    "    assert split in (\"train\", \"test\")\n",
    "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
    "\n",
    "    # Ï°¥Ïû¨ ÌôïÏù∏(Î¨∏Ï†ú ÏûàÏúºÎ©¥ Î∞îÎ°ú Ïñ¥ÎîîÍ∞Ä ÏóÜÎäîÏßÄ ÏïåÎ†§Ï§å)\n",
    "    if not os.path.isdir(inertial_path):\n",
    "        raise FileNotFoundError(f\"[Missing dir] {inertial_path}\")\n",
    "\n",
    "    X_list = []\n",
    "    for p, e in RAW_CHANNELS:\n",
    "        fpath = os.path.join(inertial_path, f\"{p}{split}.{e}\")  # ex) body_acc_x_train.txt\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise FileNotFoundError(f\"[Missing file] {fpath}\")\n",
    "        # URL Ïò§Ïù∏ Î∞©ÏßÄ: ÌååÏùº Ìï∏Îì§Î°ú Ï†ÑÎã¨\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            arr = np.loadtxt(f)       # (N, 128)\n",
    "        X_list.append(arr[..., None]) # (N, 128, 1)\n",
    "\n",
    "    # Ï±ÑÎÑê Î™®Îëê Í∞ôÏùÄ ÏÉòÌîå ÏàòÏù∏ÏßÄ Ï≤¥ÌÅ¨(ÏïàÏ†ÑÏû•Ïπò)\n",
    "    n_samples = {x.shape[0] for x in X_list}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(f\"Ï±ÑÎÑêÎ≥Ñ ÏÉòÌîå Ïàò Î∂àÏùºÏπò: {n_samples}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)  # (N, 9, 128)\n",
    "\n",
    "    y_path = os.path.join(root, split, f\"y_{split}.txt\")\n",
    "    if not os.path.isfile(y_path):\n",
    "        raise FileNotFoundError(f\"[Missing file] {y_path}\")\n",
    "    with open(y_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        y = np.loadtxt(f).astype(int) - 1  # 0-based\n",
    "\n",
    "    print(f\"[OK] {split}: X{X.shape}, y{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "class UCIHARInertial(Dataset):\n",
    "    \"\"\"UCI-HAR Dataset (‚úÖ Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï)\"\"\"\n",
    "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
    "                 preloaded_data: Tuple[np.ndarray, np.ndarray] = None):\n",
    "        super().__init__()\n",
    "        if preloaded_data is not None:\n",
    "            X, y = preloaded_data\n",
    "        else:\n",
    "            X, y = load_split_raw(root, split)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
    "\n",
    "        # mean/std ÏÑ∏ÌåÖ\n",
    "        if mean is not None and std is not None:\n",
    "            self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
    "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
    "\n",
    "        # ‚úÖ preloaded_data Ïó¨Î∂ÄÏôÄ Î¨¥Í¥ÄÌïòÍ≤å Ìï≠ÏÉÅ train ÌÜµÍ≥ÑÎ°ú Ï†ïÍ∑úÌôî\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c846a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Label-Independent Augmentations ========================\n",
    "\n",
    "def random_jitter(x: torch.Tensor, scale: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = torch.randn_like(x) * scale\n",
    "    return x + noise\n",
    "\n",
    "def random_scaling(x: torch.Tensor, scale_range: Tuple[float, float] = (0.8, 1.2)) -> torch.Tensor:\n",
    "    \"\"\"Random scaling of amplitudes\"\"\"\n",
    "    scale = torch.empty(x.size(0), x.size(1), 1, device=x.device).uniform_(*scale_range)\n",
    "    return x * scale\n",
    "\n",
    "def random_channel_drop(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly drop channels (set to zero)\"\"\"\n",
    "    B, C, T = x.shape\n",
    "    mask = torch.rand(B, C, 1, device=x.device) > drop_prob\n",
    "    return x * mask.float()\n",
    "\n",
    "def random_time_warp(x: torch.Tensor, warp_prob: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Simple time warping by random interpolation\"\"\"\n",
    "    if random.random() > warp_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    warp_factor = random.uniform(0.8, 1.2)\n",
    "    new_T = int(T * warp_factor)\n",
    "\n",
    "    x_warped = F.interpolate(x, size=new_T, mode='linear', align_corners=False)\n",
    "\n",
    "    if new_T > T:\n",
    "        start = random.randint(0, new_T - T)\n",
    "        x_warped = x_warped[:, :, start:start+T]\n",
    "    elif new_T < T:\n",
    "        pad_total = T - new_T\n",
    "        pad_left = random.randint(0, pad_total)\n",
    "        pad_right = pad_total - pad_left\n",
    "        x_warped = F.pad(x_warped, (pad_left, pad_right), mode='replicate')\n",
    "\n",
    "    return x_warped\n",
    "\n",
    "def random_cutout(x: torch.Tensor, cutout_prob: float = 0.20, cutout_ratio: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Randomly mask out a temporal segment\"\"\"\n",
    "    if random.random() > cutout_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    cutout_len = int(T * cutout_ratio)\n",
    "    start = random.randint(0, T - cutout_len)\n",
    "    x_cut = x.clone()\n",
    "    x_cut[:, :, start:start+cutout_len] = 0\n",
    "    return x_cut\n",
    "\n",
    "def augment_time_series(x: torch.Tensor, cfg: Config) -> torch.Tensor:\n",
    "    \"\"\"Label-independent augmentation pipeline\"\"\"\n",
    "    x_aug = x.clone()\n",
    "    x_aug = random_jitter(x_aug, scale=cfg.aug_jitter_scale)\n",
    "    x_aug = random_scaling(x_aug, scale_range=cfg.aug_scale_range)\n",
    "    x_aug = random_channel_drop(x_aug, drop_prob=cfg.aug_channel_drop_prob)\n",
    "    x_aug = random_time_warp(x_aug, warp_prob=cfg.aug_time_warp_prob)\n",
    "    x_aug = random_cutout(x_aug, cutout_prob=cfg.aug_cutout_prob, cutout_ratio=cfg.aug_cutout_ratio)\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Tail-Head Stitch (Ï†ÑÏù¥ Ïú†ÏÇ¨ ÌòºÌï©) ========================\n",
    "\n",
    "def tail_head_stitch(x_a: torch.Tensor, x_b: torch.Tensor, mix: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tail-Head Stitch: x_aÏùò ÏïûÎ∂ÄÎ∂Ñ + x_bÏùò Îí∑Î∂ÄÎ∂Ñ\n",
    "    Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖãÍ≥º Ïú†ÏÇ¨Ìïú Í≤ΩÍ≥Ñ ÌòºÌï© ÏÉùÏÑ±\n",
    "    \"\"\"\n",
    "    B, C, T = x_a.shape\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    x_mix = x_a.clone()\n",
    "    x_mix[:, :, -mix_pts:] = x_b[:, :, :mix_pts]\n",
    "\n",
    "    return x_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2a6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== ResNet Building Blocks ========================\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"1D Residual Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_blocks=[2, 2, 2]):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(d_model, num_blocks[2], stride=2)\n",
    "\n",
    "        self.stride = 16\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResBlock1D(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock1D(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6ca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transformer Encoder ========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Module\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a59e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Backbone ========================\n",
    "\n",
    "class ResNetTransformerBackbone(nn.Module):\n",
    "    \"\"\"ResNet + Transformer Encoder Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet1D(in_channels=in_channels, d_model=d_model)\n",
    "        self.transformer = TransformerEncoder(d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.stride = self.resnet.stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = self.resnet(x)\n",
    "        fmap = self.transformer(fmap)\n",
    "        return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b872092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Projection Head ========================\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"MLP projection head for contrastive learning\"\"\"\n",
    "    def __init__(self, d_model, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06574fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Classification Heads ========================\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Linear Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "class HyperbolicProjection(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Projection (‚úÖ ÌïôÏäµ Í∞ÄÎä•Ìïú c)\"\"\"\n",
    "    def __init__(self, c_init=1.0):\n",
    "        super().__init__()\n",
    "        self.c = nn.Parameter(torch.tensor(c_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ‚úÖ Feature norm clipping\n",
    "        x = torch.clamp(x, -5.0, 5.0)\n",
    "\n",
    "        c = torch.clamp(self.c, min=0.1, max=10.0)\n",
    "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
    "        max_norm = (1.0 / math.sqrt(c)) - 1e-4\n",
    "        scale = torch.clamp(norm, max=max_norm) / norm\n",
    "        return x * scale\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int, c_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pre_proj = nn.Linear(d_model, d_model)\n",
    "        self.hyperbolic_proj = HyperbolicProjection(c_init=c_init)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        h = self.pre_proj(pooled)\n",
    "        h_hyp = self.hyperbolic_proj(h)\n",
    "        logits = self.fc(h_hyp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249fa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SSL Model ========================\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    \"\"\"Self-Supervised Learning Model\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetTransformerBackbone(\n",
    "            in_channels=9, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(d_model, projection_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns normalized projection\"\"\"\n",
    "        fmap = self.backbone(x)\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        z = self.projection_head(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42036e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== EMA Utility ========================\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model parameters\"\"\"\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: param.clone().detach()\n",
    "                       for name, param in model.named_parameters() if param.requires_grad}\n",
    "        self.backup = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        \"\"\"Update EMA parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                self.shadow[name].mul_(self.decay).add_(param.data, alpha=1 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        \"\"\"Apply EMA parameters to model\"\"\"\n",
    "        self.backup = {name: param.data.clone() for name, param in model.named_parameters() if param.requires_grad}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        \"\"\"Restore original parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f86599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Learning Rate Scheduler ========================\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Cosine annealing with linear warmup\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48ebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Contrastive Loss ========================\n",
    "\n",
    "def contrastive_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"NT-Xent Loss (InfoNCE)\"\"\"\n",
    "    B = z1.shape[0]\n",
    "    device = z1.device\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
    "\n",
    "    labels = torch.arange(B, device=device)\n",
    "    labels = torch.cat([labels + B, labels], dim=0)\n",
    "\n",
    "    mask = torch.eye(2 * B, device=device, dtype=torch.bool)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc243dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Consistency Loss ========================\n",
    "\n",
    "def consistency_loss(model: nn.Module, head: nn.Module, x_a: torch.Tensor, x_b: torch.Tensor,\n",
    "                     mix: float = 0.5, device: str = \"cuda\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§\n",
    "    Tail-Head Stitch ÌõÑ ÏòàÏ∏° Î∂ÑÌè¨Ïùò KL divergence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        fmap_a = model.backbone(x_a) if hasattr(model, 'backbone') else model(x_a)\n",
    "        fmap_b = model.backbone(x_b) if hasattr(model, 'backbone') else model(x_b)\n",
    "\n",
    "        logits_a = head(fmap_a)\n",
    "        logits_b = head(fmap_b)\n",
    "\n",
    "        p_a = F.softmax(logits_a, dim=-1)\n",
    "        p_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    x_mix = tail_head_stitch(x_a, x_b, mix=mix)\n",
    "\n",
    "    fmap_mix = model.backbone(x_mix) if hasattr(model, 'backbone') else model(x_mix)\n",
    "    logits_mix = head(fmap_mix)\n",
    "    p_mix = F.log_softmax(logits_mix, dim=-1)\n",
    "\n",
    "    # Soft target: 0.5*p_a + 0.5*p_b\n",
    "    p_target = 0.5 * p_a + 0.5 * p_b\n",
    "\n",
    "    loss = F.kl_div(p_mix, p_target, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Training Functions ========================\n",
    "\n",
    "def pretrain_one_epoch(model: SSLModel, loader: DataLoader, opt: torch.optim.Optimizer,\n",
    "                       scheduler, ema: Optional[EMA], cfg: Config):\n",
    "    \"\"\"SSL Pretrain: No labels, only contrastive loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        x1 = augment_time_series(x, cfg)\n",
    "        x2 = augment_time_series(x, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = contrastive_loss(z1, z2, temperature=cfg.temperature)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "    return {\"ssl_loss\": total_loss / total_samples}\n",
    "\n",
    "def linear_eval_epoch(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                      opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Linear evaluation: Freeze backbone, train head only\"\"\"\n",
    "    if train:\n",
    "        backbone.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fmap = backbone(x)\n",
    "\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "def finetune_epoch(model: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   opt: torch.optim.Optimizer, scheduler, ema: Optional[EMA],\n",
    "                   cfg: Config, train: bool = True):\n",
    "    \"\"\"Fine-tuning: Train both backbone and head (‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ Ï∂îÍ∞Ä)\"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "        head.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "    total_correct, total_samples = 0, 0\n",
    "\n",
    "    data_iter = iter(loader)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        # Forward pass\n",
    "        fmap = model.backbone(x) if hasattr(model, 'backbone') else model(x)\n",
    "        logits = head(fmap)\n",
    "        loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (ÌïôÏäµ ÏãúÏóêÎßå)\n",
    "        loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "        if train and cfg.consistency_weight > 0:\n",
    "            try:\n",
    "                x_b, _ = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(loader)\n",
    "                x_b, _ = next(data_iter)\n",
    "\n",
    "            x_b = x_b.to(cfg.device)\n",
    "            if x_b.size(0) == x.size(0):\n",
    "                loss_cons = consistency_loss(model, head, x, x_b, mix=0.5, device=cfg.device)\n",
    "\n",
    "        loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(head.parameters()), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_ce_loss += loss_ce.item() * y.size(0)\n",
    "        total_cons_loss += loss_cons.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"ce_loss\": total_ce_loss / total_samples,\n",
    "        \"cons_loss\": total_cons_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   ema: Optional[EMA], cfg: Config, use_ema: bool = False):\n",
    "    \"\"\"Evaluate model (‚úÖ EMA ÏßÄÏõê)\"\"\"\n",
    "    if use_ema and ema is not None:\n",
    "        ema.apply_shadow(backbone)\n",
    "\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        fmap = backbone(x)\n",
    "        logits = head(fmap)\n",
    "        y_pred.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    if use_ema and ema is not None:\n",
    "        ema.restore(backbone)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8275be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transitional Test Set ========================\n",
    "\n",
    "def create_transitional_test_set(\n",
    "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
    ") -> Tuple[UCIHARInertial, dict]:\n",
    "    \"\"\"Create transitional test set (‚úÖ Ï†ïÍ∑úÌôî Î≥¥Ïû•)\"\"\"\n",
    "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
    "    N, C, T = X.shape\n",
    "\n",
    "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
    "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
    "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
    "    for t, s in zip(targets_A, sources_B):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
    "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
    "    for t, s in zip(targets_B, sources_A):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    # (ÏàòÏ†ï) Ïù¥Ï§ë Ï†ïÍ∑úÌôî Î∞©ÏßÄ: ÏõêÎ≥∏ Ïä§ÏºÄÏùºÎ°ú Î≥µÏõê\n",
    "    X_restored = (X * orig_dataset.std) + orig_dataset.mean\n",
    "\n",
    "    # ‚úÖ train ÌÜµÍ≥ÑÎ°ú Ï†ïÍ∑úÌôîÌïòÎèÑÎ°ù mean/std Ï†ÑÎã¨\n",
    "    mod_dataset = UCIHARInertial(\n",
    "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
    "        preloaded_data=(X_restored, y)\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        'class_A': class_A,\n",
    "        'class_B': class_B,\n",
    "        'p': p,\n",
    "        'mix': mix,\n",
    "        'modified_samples': len(targets_A) + len(targets_B),\n",
    "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
    "    }\n",
    "    return mod_dataset, info\n",
    "\n",
    "# ======================== JSON Encoder ========================\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON Encoder for NumPy types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# ======================== Main Experiment Function ========================\n",
    "\n",
    "def run_full_comparison(cfg: Config):\n",
    "    \"\"\"Run complete supervised vs SSL comparison\"\"\"\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\nüì¶ Loading UCI-HAR Dataset...\")\n",
    "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
    "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
    "    print(f\"   - Train samples: {len(train_set)}\")\n",
    "    print(f\"   - Test samples: {len(test_set_orig)}\")\n",
    "\n",
    "    # Create transitional test sets (‚úÖ 2Î†àÎ≤® Í∞ïÎèÑ)\n",
    "    scenarios = [\n",
    "        # Level 1: Moderate (Ï§ëÍ∞Ñ Í∞ïÎèÑ)\n",
    "        (\"STANDING\", \"SITTING\", 0.50, 0.40),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.55, 0.42),\n",
    "        # Level 2: Strong (Í∞ïÌïú Í∞ïÎèÑ)\n",
    "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
    "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"    üî¨ TRANSITIONAL TEST SETS ÏÉùÏÑ± (2Î†àÎ≤® Í∞ïÎèÑ)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    transition_test_data = []\n",
    "    for i, (clsA, clsB, p, mix) in enumerate(scenarios):\n",
    "        test_set_mod, info = create_transitional_test_set(test_set_orig, clsA, clsB, p=p, mix=mix)\n",
    "        transition_test_data.append((test_set_mod, info))\n",
    "        level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "        print(f\"   - [{level}] {clsA}‚Üî{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}Í∞ú ÏÉòÌîå Î≥ÄÌòï\")\n",
    "\n",
    "    # Experiment configurations\n",
    "    experiment_configs = [\n",
    "        {\"name\": \"Supervised_Linear\", \"method\": \"supervised\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"Supervised_Hyperbolic\", \"method\": \"supervised\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_LinearEval_Linear\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_LinearEval_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_FineTune_Linear\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_FineTune_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": True},\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    for exp_cfg in experiment_configs:\n",
    "        print(f\"\\n{'='*80}\\n   Ïã§Ìóò: {exp_cfg['name']}\\n{'='*80}\")\n",
    "\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "        if exp_cfg['method'] == 'supervised':\n",
    "            # Supervised Learning\n",
    "            print(\"\\nüìö Supervised Learning (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            backbone = ResNetTransformerBackbone(\n",
    "                in_channels=9, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "                n_layers=cfg.n_layers, dropout=cfg.dropout\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            params = list(backbone.parameters()) + list(head.parameters())\n",
    "            opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ‚úÖ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "            warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "            scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "            # ‚úÖ EMA\n",
    "            ema = EMA(backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            def train_supervised_epoch(backbone, head, loader, opt, scheduler, ema, cfg, train=True):\n",
    "                if train:\n",
    "                    backbone.train()\n",
    "                    head.train()\n",
    "                else:\n",
    "                    backbone.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "                total_correct, total_samples = 0, 0\n",
    "\n",
    "                data_iter = iter(loader)\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "                    fmap = backbone(x)\n",
    "                    logits = head(fmap)\n",
    "                    loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "                    # ‚úÖ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§\n",
    "                    loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "                    if train and cfg.consistency_weight > 0:\n",
    "                        try:\n",
    "                            x_b, _ = next(data_iter)\n",
    "                        except StopIteration:\n",
    "                            data_iter = iter(loader)\n",
    "                            x_b, _ = next(data_iter)\n",
    "\n",
    "                        x_b = x_b.to(cfg.device)\n",
    "                        if x_b.size(0) == x.size(0):\n",
    "                            loss_cons = consistency_loss(\n",
    "                                type('Model', (), {'backbone': backbone})(),\n",
    "                                head, x, x_b, mix=0.5, device=cfg.device\n",
    "                            )\n",
    "\n",
    "                    loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "                    if train:\n",
    "                        opt.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "                        opt.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                        if ema is not None:\n",
    "                            ema.update(backbone)\n",
    "\n",
    "                    pred = logits.argmax(dim=-1)\n",
    "                    total_correct += (pred == y).sum().item()\n",
    "                    total_loss += loss.item() * y.size(0)\n",
    "                    total_ce_loss += loss_ce.item() * y.size(0)\n",
    "                    total_cons_loss += loss_cons.item() * y.size(0)\n",
    "                    total_samples += y.size(0)\n",
    "\n",
    "                return {\n",
    "                    \"loss\": total_loss / total_samples,\n",
    "                    \"ce_loss\": total_ce_loss / total_samples,\n",
    "                    \"cons_loss\": total_cons_loss / total_samples,\n",
    "                    \"acc\": total_correct / total_samples\n",
    "                }\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"Training for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_supervised_epoch(backbone, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "                te_acc, te_f1 = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'backbone': copy.deepcopy(backbone.state_dict()),\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if ema is not None:\n",
    "                        best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Supervised {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                backbone.load_state_dict(best_wts['backbone'])\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if ema is not None and 'ema_shadow' in best_wts:\n",
    "                    ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"‚úÖ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(backbone, head, test_loader_mod, ema, cfg, use_ema=cfg.use_ema)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}‚Üî{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ‚úÖ Î†àÎ≤®Î≥Ñ Î∂ÑÏÑù\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"supervised\",\n",
    "                \"mode\": \"supervised\",\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "        else:  # SSL\n",
    "            # Stage 1: SSL Pretrain\n",
    "            print(\"\\nüìö Stage 1: Self-Supervised Pretraining (No Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            ssl_model = SSLModel(\n",
    "                d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers,\n",
    "                dropout=cfg.dropout, projection_dim=cfg.projection_dim\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            pretrain_loader = DataLoader(train_set, cfg.pretrain_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            ssl_opt = torch.optim.AdamW(ssl_model.parameters(), lr=cfg.pretrain_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ‚úÖ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.pretrain_epochs * len(pretrain_loader)\n",
    "            warmup_steps = cfg.pretrain_warmup_epochs * len(pretrain_loader)\n",
    "            ssl_scheduler = get_cosine_schedule_with_warmup(ssl_opt, warmup_steps, total_steps)\n",
    "\n",
    "            # ‚úÖ EMA\n",
    "            ssl_ema = EMA(ssl_model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            print(f\"Pretraining for {cfg.pretrain_epochs} epochs...\")\n",
    "            for epoch in range(1, cfg.pretrain_epochs + 1):\n",
    "                stats = pretrain_one_epoch(ssl_model, pretrain_loader, ssl_opt, ssl_scheduler, ssl_ema, cfg)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Pretrain {epoch:03d}/{cfg.pretrain_epochs}] SSL Loss: {stats['ssl_loss']:.4f}\")\n",
    "\n",
    "            # ‚úÖ EMA Ï†ÅÏö©\n",
    "            if ssl_ema is not None:\n",
    "                ssl_ema.apply_shadow(ssl_model)\n",
    "\n",
    "            print(\"‚úÖ Pretraining Complete!\")\n",
    "\n",
    "            # Stage 2: Linear Eval or Fine-tune\n",
    "            print(f\"\\nüìö Stage 2: {exp_cfg['mode'].upper()} (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            if exp_cfg['mode'] == 'linear_eval':\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                opt = torch.optim.AdamW(head.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                ema = None  # Linear evalÏóêÏÑúÎäî Î∞±Î≥∏Ïù¥ frozenÏù¥ÎØÄÎ°ú EMA Î∂àÌïÑÏöî\n",
    "\n",
    "                train_fn = lambda: linear_eval_epoch(ssl_model.backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            else:  # finetune\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                # ‚úÖ Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•†\n",
    "                backbone_params = list(ssl_model.backbone.parameters())\n",
    "                head_params = list(head.parameters())\n",
    "\n",
    "                opt = torch.optim.AdamW([\n",
    "                    {'params': backbone_params, 'lr': cfg.finetune_lr * cfg.finetune_backbone_lr_ratio},\n",
    "                    {'params': head_params, 'lr': cfg.finetune_lr},\n",
    "                ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                # ‚úÖ EMA (Î∞±Î≥∏Îßå)\n",
    "                ema = EMA(ssl_model.backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "                train_fn = lambda: finetune_epoch(ssl_model, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"{exp_cfg['mode']} for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_fn()\n",
    "                te_acc, te_f1 = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if exp_cfg['mode'] == 'finetune':\n",
    "                        best_wts['backbone'] = copy.deepcopy(ssl_model.backbone.state_dict())\n",
    "                        if ema is not None:\n",
    "                            best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    if 'cons_loss' in stats:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if exp_cfg['mode'] == 'finetune':\n",
    "                    ssl_model.backbone.load_state_dict(best_wts['backbone'])\n",
    "                    if ema is not None and 'ema_shadow' in best_wts:\n",
    "                        ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"‚úÖ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(ssl_model.backbone, head, test_loader_mod, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}‚Üî{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ‚úÖ Î†àÎ≤®Î≥Ñ Î∂ÑÏÑù\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"ssl\",\n",
    "                \"mode\": exp_cfg['mode'],\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"   üìä SUPERVISED vs TRUE SSL Ïã§Ìóò Í≤∞Í≥º (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Method':<12} {'Mode':<12} {'Classifier':<12} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<10}\")\n",
    "    print(\"-\" * 115)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['method']:<12} {r['mode']:<12} {r['classifier']:<12} \"\n",
    "              f\"{r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
    "              f\"{r['avg_drop']:.4f}  {r['retention']:.2f}%\")\n",
    "\n",
    "    # ‚úÖ Î†àÎ≤®Î≥Ñ Í≤∞Í≥º Ï∂îÍ∞Ä\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Î†àÎ≤®Î≥Ñ Retention Î∂ÑÏÑù\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Overall':<12} {'Moderate':<12} {'Strong':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['retention']:>6.2f}%      {r['retention_moderate']:>6.2f}%       {r['retention_strong']:>6.2f}%\")\n",
    "\n",
    "    # Detailed analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä ÏÉÅÏÑ∏ ÎπÑÍµê Î∂ÑÏÑù\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Final ranking\n",
    "    sorted_results = sorted(results_table, key=lambda x: x['retention'], reverse=True)\n",
    "    print(\"\\nüèÜ ÏµúÏ¢Ö ÏÑ±Îä• Îû≠ÌÇπ (Overall Retention Í∏∞Ï§Ä)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for rank, r in enumerate(sorted_results, 1):\n",
    "        method_mode = f\"{r['method']}-{r['mode']}\" if r['method'] == 'ssl' else r['method']\n",
    "        print(f\"   {rank}. {r['config']:<35} ({method_mode:<20}) \"\n",
    "              f\"Retention: {r['retention']:.2f}% (Mod: {r['retention_moderate']:.2f}% | Str: {r['retention_strong']:.2f}%)\")\n",
    "\n",
    "    best_config = sorted_results[0]\n",
    "    best_ssl = max([r for r in results_table if r['method'] == 'ssl'], key=lambda x: x['retention'])\n",
    "    best_sup = max([r for r in results_table if r['method'] == 'supervised'], key=lambda x: x['retention'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Í≤∞Î°†\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - ÏµúÍ≥† ÏÑ±Îä•: {best_config['config']} (Retention: {best_config['retention']:.2f}%)\")\n",
    "    print(f\"   - Supervised baseline: {best_sup['retention']:.2f}% (Mod: {best_sup['retention_moderate']:.2f}% | Str: {best_sup['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - SSL best: {best_ssl['retention']:.2f}% (Mod: {best_ssl['retention_moderate']:.2f}% | Str: {best_ssl['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - Performance gap: {abs(best_ssl['retention'] - best_sup['retention']):.2f}pp\")\n",
    "\n",
    "    # ‚úÖ Í∞úÏÑ† Ìö®Í≥º Î∂ÑÏÑù\n",
    "    print(\"\\n   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\")\n",
    "    print(\"   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï ‚úÖ\")\n",
    "    print(\"   - Cosine + Warmup Ïä§ÏºÄÏ§ÑÎü¨ ‚úÖ\")\n",
    "    print(\"   - EMA (Exponential Moving Average) ‚úÖ\")\n",
    "    print(\"   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune) ‚úÖ\")\n",
    "    print(\"   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch) ‚úÖ\")\n",
    "    print(f\"   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (warp: 0.10, cutout: 0.10) ‚úÖ\")\n",
    "    print(\"   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c ‚úÖ\")\n",
    "    print(\"   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong) ‚úÖ\")\n",
    "\n",
    "    # Save results\n",
    "    save_path = os.path.join(cfg.save_dir, \"supervised_vs_ssl_results_optimized.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    visualization_data = {\n",
    "        'configs': [r['config'] for r in results_table],\n",
    "        'methods': [r['method'] for r in results_table],\n",
    "        'modes': [r['mode'] for r in results_table],\n",
    "        'classifiers': [r['classifier'] for r in results_table],\n",
    "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
    "        'orig_f1': [r['orig_f1'] for r in results_table],\n",
    "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
    "        'trans_f1': [r['avg_trans_f1'] for r in results_table],\n",
    "        'retention': [r['retention'] for r in results_table],\n",
    "        'retention_moderate': [r['retention_moderate'] for r in results_table],\n",
    "        'retention_strong': [r['retention_strong'] for r in results_table],\n",
    "        'avg_drop': [r['avg_drop'] for r in results_table]\n",
    "    }\n",
    "\n",
    "    viz_path = os.path.join(cfg.save_dir, \"visualization_data_optimized.json\")\n",
    "    with open(viz_path, \"w\") as f:\n",
    "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n‚úÖ Results saved to:\")\n",
    "    print(f\"   - {save_path}\")\n",
    "    print(f\"   - {viz_path}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c68009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   üß™ UCI-HAR Comprehensive Comparison (‚ú® OPTIMIZED)\n",
      "   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\n",
      "   Architecture: ResNet + Transformer Encoder\n",
      "================================================================================\n",
      "\n",
      "   üìã Ïã§Ìóò ÏÑ§Í≥Ñ:\n",
      "   1. ÎèôÏùºÌïú Î∞±Î≥∏ (ResNet + Transformer)\n",
      "   2. 2Î†àÎ≤® Ï†ÑÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖã (Moderate/Strong)\n",
      "   3. 6Í∞ÄÏßÄ ÏÑ§Ï†ï ÎπÑÍµê:\n",
      "      ‚îú‚îÄ Supervised √ó (Linear, Hyperbolic)\n",
      "      ‚îú‚îÄ SSL Linear Eval √ó (Linear, Hyperbolic)\n",
      "      ‚îî‚îÄ SSL Fine-tune √ó (Linear, Hyperbolic)\n",
      "================================================================================\n",
      "\n",
      "   ‚öôÔ∏è  Supervised ÏÑ§Ï†ï:\n",
      "   - Epochs: 50\n",
      "   - Batch size: 128\n",
      "   - Learning rate: 0.0003\n",
      "   - Warmup: 5 epochs\n",
      "   - EMA decay: 0.9995\n",
      "   - Consistency weight: 0.2\n",
      "   - Training: End-to-end with labels + consistency loss\n",
      "\n",
      "   ‚öôÔ∏è  SSL ÏÑ§Ï†ï:\n",
      "   - Stage 1 (Pretrain): 100 epochs, batch=512, lr=0.001\n",
      "     ‚Üí Contrastive learning only (NO LABELS)\n",
      "     ‚Üí Label-independent augmentation\n",
      "     ‚Üí Warmup: 10 epochs\n",
      "     ‚Üí EMA decay: 0.9995\n",
      "   - Stage 2 (Eval/FT): 50 epochs, batch=128, lr=0.0003\n",
      "     ‚Üí Linear Eval: Freeze backbone\n",
      "     ‚Üí Fine-tune: Train all (backbone LR √ó 0.1)\n",
      "     ‚Üí Consistency weight: 0.2\n",
      "\n",
      "   üîß Augmentations (SSL - Optimized):\n",
      "   - Jitter (scale=0.05)\n",
      "   - Scaling (range=(0.8, 1.2))\n",
      "   - Channel Drop (prob=0.2)\n",
      "   - Time Warp (prob=0.1) ‚úÖ 0.3‚Üí0.10\n",
      "   - Cutout (prob=0.2, ratio=0.1) ‚úÖ 0.2‚Üí0.10\n",
      "   - ALL label-independent!\n",
      "\n",
      "   üèóÔ∏è  Architecture:\n",
      "   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads=4, layers=2)\n",
      "   - d_model: 128, dropout: 0.1\n",
      "   - Classifier: Linear vs Hyperbolic (c_init=1.0, learnable ‚úÖ)\n",
      "   - Projection dim (SSL): 128\n",
      "\n",
      "   üî¨ SSL Contrastive Learning:\n",
      "   - Loss: NT-Xent (InfoNCE)\n",
      "   - Temperature: 0.07\n",
      "   - Negative samples: 2*batch_size - 2\n",
      "\n",
      "   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\n",
      "   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï (Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã)\n",
      "   - Cosine Annealing + Warmup\n",
      "   - EMA (Exponential Moving Average)\n",
      "   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune)\n",
      "   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch)\n",
      "   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (Í≤ΩÍ≥Ñ Ï†ïÎ≥¥ Î≥¥Ï°¥)\n",
      "   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c\n",
      "   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üì¶ Loading UCI-HAR Dataset...\n",
      "[OK] train: X(7352, 9, 128), y(7352,)\n",
      "[OK] test: X(2947, 9, 128), y(2947,)\n",
      "   - Train samples: 7352\n",
      "   - Test samples: 2947\n",
      "\n",
      "================================================================================\n",
      "    üî¨ TRANSITIONAL TEST SETS ÏÉùÏÑ± (2Î†àÎ≤® Í∞ïÎèÑ)\n",
      "================================================================================\n",
      "   - [Moderate] STANDING‚ÜîSITTING (p=0.50, mix=0.40): 511Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Moderate] WALKING‚ÜîWALKING_UPSTAIRS (p=0.55, mix=0.42): 531Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] STANDING‚ÜîSITTING (p=0.70, mix=0.55): 715Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] WALKING‚ÜîWALKING_UPSTAIRS (p=0.65, mix=0.52): 628Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "   - [Strong] SITTING‚ÜîLAYING (p=0.75, mix=0.58): 770Í∞ú ÏÉòÌîå Î≥ÄÌòï\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: Supervised_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.2416 CE:1.2327 Cons:0.0440 A:0.5866 | Test A:0.4591 F1:0.3215\n",
      "[Supervised 10/50] Train L:0.3228 CE:0.3192 Cons:0.0181 A:0.9607 | Test A:0.5110 F1:0.4325\n",
      "[Supervised 20/50] Train L:0.3022 CE:0.2994 Cons:0.0140 A:0.9701 | Test A:0.7044 F1:0.6445\n",
      "[Supervised 30/50] Train L:0.2891 CE:0.2864 Cons:0.0134 A:0.9769 | Test A:0.8544 F1:0.8459\n",
      "[Supervised 40/50] Train L:0.2723 CE:0.2689 Cons:0.0168 A:0.9857 | Test A:0.9216 F1:0.9206\n",
      "[Supervised 50/50] Train L:0.2712 CE:0.2678 Cons:0.0172 A:0.9857 | Test A:0.9253 F1:0.9251\n",
      "‚úÖ Best Test Acc: 0.9294\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.8867 F1=0.8848 (Drop=0.0428)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8775 F1=0.8725 (Drop=0.0519)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8409 F1=0.8360 (Drop=0.0886)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8405 F1=0.8267 (Drop=0.0889)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.8120 F1=0.7906 (Drop=0.1174)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: Supervised_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.7034 CE:1.7033 Cons:0.0006 A:0.4396 | Test A:0.4079 F1:0.2542\n",
      "[Supervised 10/50] Train L:0.6709 CE:0.6693 Cons:0.0078 A:0.9610 | Test A:0.1853 F1:0.0845\n",
      "[Supervised 20/50] Train L:0.4042 CE:0.4021 Cons:0.0105 A:0.9648 | Test A:0.7133 F1:0.6760\n",
      "[Supervised 30/50] Train L:0.3339 CE:0.3312 Cons:0.0130 A:0.9744 | Test A:0.7034 F1:0.6484\n",
      "[Supervised 40/50] Train L:0.3047 CE:0.3019 Cons:0.0141 A:0.9849 | Test A:0.8446 F1:0.8319\n",
      "[Supervised 50/50] Train L:0.2996 CE:0.2966 Cons:0.0152 A:0.9878 | Test A:0.8731 F1:0.8683\n",
      "‚úÖ Best Test Acc: 0.8731\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.8493 F1=0.8374 (Drop=0.0238)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.8290 F1=0.8227 (Drop=0.0441)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.8249 F1=0.8062 (Drop=0.0482)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.7615 F1=0.7482 (Drop=0.1116)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.7743 F1=0.7417 (Drop=0.0987)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_LinearEval_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 10/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 20/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 30/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 40/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "[linear_eval 50/50] Train L:2.0955 A:0.1751 | Test A:0.1666 F1:0.0476\n",
      "‚úÖ Best Test Acc: 0.1666\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.1666 F1=0.0476 (Drop=0.0000)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_LinearEval_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 10/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 20/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 30/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 40/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "[linear_eval 50/50] Train L:1.7812 A:0.1869 | Test A:0.1785 F1:0.0661\n",
      "‚úÖ Best Test Acc: 0.1785\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.1775 F1=0.0652 (Drop=0.0010)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.1785 F1=0.0658 (Drop=0.0000)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.1778 F1=0.0655 (Drop=0.0007)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.1795 F1=0.0674 (Drop=-0.0010)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.1819 F1=0.0670 (Drop=-0.0034)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_FineTune_Linear\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.8212 CE:1.8143 Cons:0.0343 A:0.1974 | Test A:0.1452 F1:0.1147\n",
      "[finetune 10/50] Train L:0.3364 CE:0.3205 Cons:0.0795 A:0.9618 | Test A:0.3536 F1:0.3221\n",
      "[finetune 20/50] Train L:0.3127 CE:0.3002 Cons:0.0625 A:0.9706 | Test A:0.3519 F1:0.3056\n",
      "[finetune 30/50] Train L:0.2958 CE:0.2845 Cons:0.0561 A:0.9796 | Test A:0.3641 F1:0.3323\n",
      "[finetune 40/50] Train L:0.2929 CE:0.2816 Cons:0.0566 A:0.9810 | Test A:0.3526 F1:0.3161\n",
      "[finetune 50/50] Train L:0.2857 CE:0.2748 Cons:0.0548 A:0.9848 | Test A:0.3380 F1:0.3133\n",
      "‚úÖ Best Test Acc: 0.3946\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.3780 F1=0.3540 (Drop=0.0166)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.3543 F1=0.3043 (Drop=0.0404)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.3410 F1=0.3288 (Drop=0.0536)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.3488 F1=0.2934 (Drop=0.0458)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.3604 F1=0.3248 (Drop=0.0343)\n",
      "\n",
      "================================================================================\n",
      "   Ïã§Ìóò: SSL_FineTune_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "üìö Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 5.6494\n",
      "[Pretrain 010/100] SSL Loss: 2.4570\n",
      "[Pretrain 020/100] SSL Loss: 1.2836\n",
      "[Pretrain 030/100] SSL Loss: 1.1030\n",
      "[Pretrain 040/100] SSL Loss: 1.0725\n",
      "[Pretrain 050/100] SSL Loss: 0.8030\n",
      "[Pretrain 060/100] SSL Loss: 0.7131\n",
      "[Pretrain 070/100] SSL Loss: 0.6814\n",
      "[Pretrain 080/100] SSL Loss: 0.7931\n",
      "[Pretrain 090/100] SSL Loss: 0.5313\n",
      "[Pretrain 100/100] SSL Loss: 0.6155\n",
      "‚úÖ Pretraining Complete!\n",
      "\n",
      "üìö Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.7690 CE:1.7689 Cons:0.0004 A:0.2216 | Test A:0.2257 F1:0.1328\n",
      "[finetune 10/50] Train L:0.6626 CE:0.6589 Cons:0.0185 A:0.9593 | Test A:0.6485 F1:0.6061\n",
      "[finetune 20/50] Train L:0.4208 CE:0.4155 Cons:0.0262 A:0.9705 | Test A:0.6634 F1:0.6368\n",
      "[finetune 30/50] Train L:0.3565 CE:0.3505 Cons:0.0299 A:0.9786 | Test A:0.6264 F1:0.5814\n",
      "[finetune 40/50] Train L:0.3340 CE:0.3277 Cons:0.0314 A:0.9827 | Test A:0.6318 F1:0.5905\n",
      "[finetune 50/50] Train L:0.3313 CE:0.3253 Cons:0.0301 A:0.9838 | Test A:0.6284 F1:0.5843\n",
      "‚úÖ Best Test Acc: 0.6634\n",
      "\n",
      "   üîç Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä...\n",
      "     - [Moderate] Scenario 1 (STANDING‚ÜîSITTING): Acc=0.6566 F1=0.6293 (Drop=0.0068)\n",
      "     - [Moderate] Scenario 2 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.6407 F1=0.6060 (Drop=0.0227)\n",
      "     - [Strong] Scenario 3 (STANDING‚ÜîSITTING): Acc=0.6390 F1=0.6082 (Drop=0.0244)\n",
      "     - [Strong] Scenario 4 (WALKING‚ÜîWALKING_UPSTAIRS): Acc=0.6240 F1=0.5850 (Drop=0.0394)\n",
      "     - [Strong] Scenario 5 (SITTING‚ÜîLAYING): Acc=0.5005 F1=0.4954 (Drop=0.1629)\n",
      "\n",
      "================================================================================\n",
      "   üìä SUPERVISED vs TRUE SSL Ïã§Ìóò Í≤∞Í≥º (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)\n",
      "================================================================================\n",
      "Config                              Method       Mode         Classifier   Orig Acc   Trans Acc   Drop       Retention \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Supervised_Linear                   supervised   supervised   Linear       0.9294     0.8515      0.0779  91.62%\n",
      "Supervised_Hyperbolic               supervised   supervised   Hyperbolic   0.8731     0.8078      0.0653  92.52%\n",
      "SSL_LinearEval_Linear               ssl          linear_eval  Linear       0.1666     0.1666      0.0000  100.00%\n",
      "SSL_LinearEval_Hyperbolic           ssl          linear_eval  Hyperbolic   0.1785     0.1790      -0.0005  100.30%\n",
      "SSL_FineTune_Linear                 ssl          finetune     Linear       0.3946     0.3565      0.0381  90.34%\n",
      "SSL_FineTune_Hyperbolic             ssl          finetune     Hyperbolic   0.6634     0.6121      0.0512  92.28%\n",
      "\n",
      "================================================================================\n",
      "üìä Î†àÎ≤®Î≥Ñ Retention Î∂ÑÏÑù\n",
      "================================================================================\n",
      "Config                              Overall      Moderate     Strong      \n",
      "--------------------------------------------------------------------------------\n",
      "Supervised_Linear                    91.62%       94.91%        89.42%\n",
      "Supervised_Hyperbolic                92.52%       96.11%        90.13%\n",
      "SSL_LinearEval_Linear               100.00%      100.00%       100.00%\n",
      "SSL_LinearEval_Hyperbolic           100.30%       99.71%       100.70%\n",
      "SSL_FineTune_Linear                  90.34%       92.78%        88.71%\n",
      "SSL_FineTune_Hyperbolic              92.28%       97.77%        88.61%\n",
      "\n",
      "================================================================================\n",
      "üìä ÏÉÅÏÑ∏ ÎπÑÍµê Î∂ÑÏÑù\n",
      "================================================================================\n",
      "\n",
      "üèÜ ÏµúÏ¢Ö ÏÑ±Îä• Îû≠ÌÇπ (Overall Retention Í∏∞Ï§Ä)\n",
      "--------------------------------------------------------------------------------\n",
      "   1. SSL_LinearEval_Hyperbolic           (ssl-linear_eval     ) Retention: 100.30% (Mod: 99.71% | Str: 100.70%)\n",
      "   2. SSL_LinearEval_Linear               (ssl-linear_eval     ) Retention: 100.00% (Mod: 100.00% | Str: 100.00%)\n",
      "   3. Supervised_Hyperbolic               (supervised          ) Retention: 92.52% (Mod: 96.11% | Str: 90.13%)\n",
      "   4. SSL_FineTune_Hyperbolic             (ssl-finetune        ) Retention: 92.28% (Mod: 97.77% | Str: 88.61%)\n",
      "   5. Supervised_Linear                   (supervised          ) Retention: 91.62% (Mod: 94.91% | Str: 89.42%)\n",
      "   6. SSL_FineTune_Linear                 (ssl-finetune        ) Retention: 90.34% (Mod: 92.78% | Str: 88.71%)\n",
      "\n",
      "================================================================================\n",
      "üéØ Í≤∞Î°†\n",
      "================================================================================\n",
      "   - ÏµúÍ≥† ÏÑ±Îä•: SSL_LinearEval_Hyperbolic (Retention: 100.30%)\n",
      "   - Supervised baseline: 92.52% (Mod: 96.11% | Str: 90.13%)\n",
      "   - SSL best: 100.30% (Mod: 99.71% | Str: 100.70%)\n",
      "   - Performance gap: 7.78pp\n",
      "\n",
      "   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\n",
      "   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï ‚úÖ\n",
      "   - Cosine + Warmup Ïä§ÏºÄÏ§ÑÎü¨ ‚úÖ\n",
      "   - EMA (Exponential Moving Average) ‚úÖ\n",
      "   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune) ‚úÖ\n",
      "   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch) ‚úÖ\n",
      "   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (warp: 0.10, cutout: 0.10) ‚úÖ\n",
      "   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c ‚úÖ\n",
      "   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong) ‚úÖ\n",
      "\n",
      "‚úÖ Results saved to:\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\\supervised_vs_ssl_results_optimized.json\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/ALL\\visualization_data_optimized.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================== Main Entry Point ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"   üß™ UCI-HAR Comprehensive Comparison (‚ú® OPTIMIZED)\")\n",
    "    print(\"   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\")\n",
    "    print(\"   Architecture: ResNet + Transformer Encoder\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   üìã Ïã§Ìóò ÏÑ§Í≥Ñ:\")\n",
    "    print(\"   1. ÎèôÏùºÌïú Î∞±Î≥∏ (ResNet + Transformer)\")\n",
    "    print(\"   2. 2Î†àÎ≤® Ï†ÑÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖã (Moderate/Strong)\")\n",
    "    print(\"   3. 6Í∞ÄÏßÄ ÏÑ§Ï†ï ÎπÑÍµê:\")\n",
    "    print(\"      ‚îú‚îÄ Supervised √ó (Linear, Hyperbolic)\")\n",
    "    print(\"      ‚îú‚îÄ SSL Linear Eval √ó (Linear, Hyperbolic)\")\n",
    "    print(\"      ‚îî‚îÄ SSL Fine-tune √ó (Linear, Hyperbolic)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   ‚öôÔ∏è  Supervised ÏÑ§Ï†ï:\")\n",
    "    print(f\"   - Epochs: {config.finetune_epochs}\")\n",
    "    print(f\"   - Batch size: {config.finetune_batch_size}\")\n",
    "    print(f\"   - Learning rate: {config.finetune_lr}\")\n",
    "    print(f\"   - Warmup: {config.finetune_warmup_epochs} epochs\")\n",
    "    print(f\"   - EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"   - Training: End-to-end with labels + consistency loss\")\n",
    "    print(f\"\\n   ‚öôÔ∏è  SSL ÏÑ§Ï†ï:\")\n",
    "    print(f\"   - Stage 1 (Pretrain): {config.pretrain_epochs} epochs, batch={config.pretrain_batch_size}, lr={config.pretrain_lr}\")\n",
    "    print(f\"     ‚Üí Contrastive learning only (NO LABELS)\")\n",
    "    print(f\"     ‚Üí Label-independent augmentation\")\n",
    "    print(f\"     ‚Üí Warmup: {config.pretrain_warmup_epochs} epochs\")\n",
    "    print(f\"     ‚Üí EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Stage 2 (Eval/FT): {config.finetune_epochs} epochs, batch={config.finetune_batch_size}, lr={config.finetune_lr}\")\n",
    "    print(f\"     ‚Üí Linear Eval: Freeze backbone\")\n",
    "    print(f\"     ‚Üí Fine-tune: Train all (backbone LR √ó {config.finetune_backbone_lr_ratio})\")\n",
    "    print(f\"     ‚Üí Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"\\n   üîß Augmentations (SSL - Optimized):\")\n",
    "    print(f\"   - Jitter (scale={config.aug_jitter_scale})\")\n",
    "    print(f\"   - Scaling (range={config.aug_scale_range})\")\n",
    "    print(f\"   - Channel Drop (prob={config.aug_channel_drop_prob})\")\n",
    "    print(f\"   - Time Warp (prob={config.aug_time_warp_prob}) ‚úÖ 0.3‚Üí0.10\")\n",
    "    print(f\"   - Cutout (prob={config.aug_cutout_prob}, ratio={config.aug_cutout_ratio}) ‚úÖ 0.2‚Üí0.10\")\n",
    "    print(\"   - ALL label-independent!\")\n",
    "    print(f\"\\n   üèóÔ∏è  Architecture:\")\n",
    "    print(f\"   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads={config.n_heads}, layers={config.n_layers})\")\n",
    "    print(f\"   - d_model: {config.d_model}, dropout: {config.dropout}\")\n",
    "    print(f\"   - Classifier: Linear vs Hyperbolic (c_init={config.hyperbolic_c_init}, learnable ‚úÖ)\")\n",
    "    print(f\"   - Projection dim (SSL): {config.projection_dim}\")\n",
    "    print(f\"\\n   üî¨ SSL Contrastive Learning:\")\n",
    "    print(f\"   - Loss: NT-Xent (InfoNCE)\")\n",
    "    print(f\"   - Temperature: {config.temperature}\")\n",
    "    print(f\"   - Negative samples: 2*batch_size - 2\")\n",
    "    print(f\"\\n   ‚ú® ÏµúÏ†ÅÌôî Í∞úÏÑ†ÏÇ¨Ìï≠:\")\n",
    "    print(\"   - Ï†ïÍ∑úÌôî Î≤ÑÍ∑∏ ÏàòÏ†ï (Ï†ÑÏù¥ ÌÖåÏä§Ìä∏ÏÖã)\")\n",
    "    print(\"   - Cosine Annealing + Warmup\")\n",
    "    print(\"   - EMA (Exponential Moving Average)\")\n",
    "    print(\"   - Î∞±Î≥∏/Ìó§Îìú Î∂ÑÎ¶¨ ÌïôÏäµÎ•† (Fine-tune)\")\n",
    "    print(\"   - Ï†ÑÏù¥-Ïú†ÏÇ¨ ÏùºÍ¥ÄÏÑ± ÏÜêÏã§ (Tail-Head Stitch)\")\n",
    "    print(\"   - Ï¶ùÍ∞ï Í∞ïÎèÑ ÏµúÏ†ÅÌôî (Í≤ΩÍ≥Ñ Ï†ïÎ≥¥ Î≥¥Ï°¥)\")\n",
    "    print(\"   - ÌïôÏäµ Í∞ÄÎä•Ìïú Hyperbolic c\")\n",
    "    print(\"   - 2Î†àÎ≤® Ï†ÑÏù¥ ÏãúÎÇòÎ¶¨Ïò§ (Moderate/Strong)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    run_full_comparison(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee20d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
