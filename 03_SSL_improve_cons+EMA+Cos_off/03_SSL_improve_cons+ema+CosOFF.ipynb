{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad7451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Random Seed ========================\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f78ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"실험 설정\"\"\"\n",
    "    data_dir: str = \"C://Users/park9/HAR/SSL_HAR/data\"\n",
    "    save_dir: str = \"C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\"\n",
    "\n",
    "    # SSL Pretrain 파라미터\n",
    "    pretrain_epochs: int = 100\n",
    "    pretrain_batch_size: int = 512  # ✅ InfoNCE 성능 향상\n",
    "    pretrain_lr: float = 1e-3\n",
    "    pretrain_warmup_epochs: int = 0  # ✅ Warmup\n",
    "\n",
    "    # Supervised / Linear Eval / Fine-tune 파라미터\n",
    "    finetune_epochs: int = 50\n",
    "    finetune_batch_size: int = 128\n",
    "    finetune_lr: float = 3e-4\n",
    "    finetune_warmup_epochs: int = 0  # ✅ Warmup\n",
    "    finetune_backbone_lr_ratio: float = 0.1  # ✅ 백본 LR 비율\n",
    "\n",
    "    # 공통 파라미터\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.05\n",
    "    use_ema: bool = False  # ✅ EMA 사용\n",
    "    ema_decay: float = 0.9995  # ✅ EMA decay\n",
    "    consistency_weight: float = 0.0  # ✅ 일관성 손실 가중치\n",
    "\n",
    "    # 모델 파라미터\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    hyperbolic_c_init: float = 1.0  # ✅ 초기값 (학습 가능)\n",
    "\n",
    "    # SSL 파라미터\n",
    "    temperature: float = 0.07\n",
    "    projection_dim: int = 128\n",
    "\n",
    "    # Augmentation 파라미터 (✅ 전이 강건성 최적화)\n",
    "    aug_jitter_scale: float = 0.05\n",
    "    aug_scale_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    aug_channel_drop_prob: float = 0.2\n",
    "    aug_time_warp_prob: float = 0.10  # ✅ 0.3 → 0.10\n",
    "    aug_cutout_prob: float = 0.20     # ✅ 0.3 → 0.20\n",
    "    aug_cutout_ratio: float = 0.10    # ✅ 0.2 → 0.10\n",
    "\n",
    "    # 시스템 파라미터\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c29fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Dataset Configuration ========================\n",
    "\n",
    "INERTIAL_SIGNALS_FOLDER = \"Inertial Signals\"\n",
    "RAW_CHANNELS = [\n",
    "    (\"total_acc_x_\", \"txt\"), (\"total_acc_y_\", \"txt\"), (\"total_acc_z_\", \"txt\"),\n",
    "    (\"body_acc_x_\", \"txt\"), (\"body_acc_y_\", \"txt\"), (\"body_acc_z_\", \"txt\"),\n",
    "    (\"body_gyro_x_\", \"txt\"), (\"body_gyro_y_\", \"txt\"), (\"body_gyro_z_\", \"txt\"),\n",
    "]\n",
    "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
    "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
    "LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
    "\n",
    "def load_split_raw(root: str, split: str):\n",
    "    assert split in (\"train\", \"test\")\n",
    "    inertial_path = os.path.join(root, split, INERTIAL_SIGNALS_FOLDER)\n",
    "\n",
    "    # 존재 확인(문제 있으면 바로 어디가 없는지 알려줌)\n",
    "    if not os.path.isdir(inertial_path):\n",
    "        raise FileNotFoundError(f\"[Missing dir] {inertial_path}\")\n",
    "\n",
    "    X_list = []\n",
    "    for p, e in RAW_CHANNELS:\n",
    "        fpath = os.path.join(inertial_path, f\"{p}{split}.{e}\")  # ex) body_acc_x_train.txt\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise FileNotFoundError(f\"[Missing file] {fpath}\")\n",
    "        # URL 오인 방지: 파일 핸들로 전달\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            arr = np.loadtxt(f)       # (N, 128)\n",
    "        X_list.append(arr[..., None]) # (N, 128, 1)\n",
    "\n",
    "    # 채널 모두 같은 샘플 수인지 체크(안전장치)\n",
    "    n_samples = {x.shape[0] for x in X_list}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(f\"채널별 샘플 수 불일치: {n_samples}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)  # (N, 9, 128)\n",
    "\n",
    "    y_path = os.path.join(root, split, f\"y_{split}.txt\")\n",
    "    if not os.path.isfile(y_path):\n",
    "        raise FileNotFoundError(f\"[Missing file] {y_path}\")\n",
    "    with open(y_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        y = np.loadtxt(f).astype(int) - 1  # 0-based\n",
    "\n",
    "    print(f\"[OK] {split}: X{X.shape}, y{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "class UCIHARInertial(Dataset):\n",
    "    \"\"\"UCI-HAR Dataset (✅ 정규화 버그 수정)\"\"\"\n",
    "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
    "                 preloaded_data: Tuple[np.ndarray, np.ndarray] = None):\n",
    "        super().__init__()\n",
    "        if preloaded_data is not None:\n",
    "            X, y = preloaded_data\n",
    "        else:\n",
    "            X, y = load_split_raw(root, split)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
    "\n",
    "        # mean/std 세팅\n",
    "        if mean is not None and std is not None:\n",
    "            self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.X.mean(axis=(0,2), keepdims=True)\n",
    "            self.std = self.X.std(axis=(0,2), keepdims=True) + 1e-6\n",
    "\n",
    "        # ✅ preloaded_data 여부와 무관하게 항상 train 통계로 정규화\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c846a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Label-Independent Augmentations ========================\n",
    "\n",
    "def random_jitter(x: torch.Tensor, scale: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = torch.randn_like(x) * scale\n",
    "    return x + noise\n",
    "\n",
    "def random_scaling(x: torch.Tensor, scale_range: Tuple[float, float] = (0.8, 1.2)) -> torch.Tensor:\n",
    "    \"\"\"Random scaling of amplitudes\"\"\"\n",
    "    scale = torch.empty(x.size(0), x.size(1), 1, device=x.device).uniform_(*scale_range)\n",
    "    return x * scale\n",
    "\n",
    "def random_channel_drop(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Randomly drop channels (set to zero)\"\"\"\n",
    "    B, C, T = x.shape\n",
    "    mask = torch.rand(B, C, 1, device=x.device) > drop_prob\n",
    "    return x * mask.float()\n",
    "\n",
    "def random_time_warp(x: torch.Tensor, warp_prob: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Simple time warping by random interpolation\"\"\"\n",
    "    if random.random() > warp_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    warp_factor = random.uniform(0.8, 1.2)\n",
    "    new_T = int(T * warp_factor)\n",
    "\n",
    "    x_warped = F.interpolate(x, size=new_T, mode='linear', align_corners=False)\n",
    "\n",
    "    if new_T > T:\n",
    "        start = random.randint(0, new_T - T)\n",
    "        x_warped = x_warped[:, :, start:start+T]\n",
    "    elif new_T < T:\n",
    "        pad_total = T - new_T\n",
    "        pad_left = random.randint(0, pad_total)\n",
    "        pad_right = pad_total - pad_left\n",
    "        x_warped = F.pad(x_warped, (pad_left, pad_right), mode='replicate')\n",
    "\n",
    "    return x_warped\n",
    "\n",
    "def random_cutout(x: torch.Tensor, cutout_prob: float = 0.20, cutout_ratio: float = 0.10) -> torch.Tensor:\n",
    "    \"\"\"Randomly mask out a temporal segment\"\"\"\n",
    "    if random.random() > cutout_prob:\n",
    "        return x\n",
    "\n",
    "    B, C, T = x.shape\n",
    "    cutout_len = int(T * cutout_ratio)\n",
    "    start = random.randint(0, T - cutout_len)\n",
    "    x_cut = x.clone()\n",
    "    x_cut[:, :, start:start+cutout_len] = 0\n",
    "    return x_cut\n",
    "\n",
    "def augment_time_series(x: torch.Tensor, cfg: Config) -> torch.Tensor:\n",
    "    \"\"\"Label-independent augmentation pipeline\"\"\"\n",
    "    x_aug = x.clone()\n",
    "    x_aug = random_jitter(x_aug, scale=cfg.aug_jitter_scale)\n",
    "    x_aug = random_scaling(x_aug, scale_range=cfg.aug_scale_range)\n",
    "    x_aug = random_channel_drop(x_aug, drop_prob=cfg.aug_channel_drop_prob)\n",
    "    x_aug = random_time_warp(x_aug, warp_prob=cfg.aug_time_warp_prob)\n",
    "    x_aug = random_cutout(x_aug, cutout_prob=cfg.aug_cutout_prob, cutout_ratio=cfg.aug_cutout_ratio)\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Tail-Head Stitch (전이 유사 혼합) ========================\n",
    "\n",
    "def tail_head_stitch(x_a: torch.Tensor, x_b: torch.Tensor, mix: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tail-Head Stitch: x_a의 앞부분 + x_b의 뒷부분\n",
    "    전이 테스트셋과 유사한 경계 혼합 생성\n",
    "    \"\"\"\n",
    "    B, C, T = x_a.shape\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    x_mix = x_a.clone()\n",
    "    x_mix[:, :, -mix_pts:] = x_b[:, :, :mix_pts]\n",
    "\n",
    "    return x_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2a6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== ResNet Building Blocks ========================\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"1D Residual Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_blocks=[2, 2, 2]):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(d_model, num_blocks[2], stride=2)\n",
    "\n",
    "        self.stride = 16\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResBlock1D(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock1D(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6ca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transformer Encoder ========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Module\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a59e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Backbone ========================\n",
    "\n",
    "class ResNetTransformerBackbone(nn.Module):\n",
    "    \"\"\"ResNet + Transformer Encoder Backbone\"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet1D(in_channels=in_channels, d_model=d_model)\n",
    "        self.transformer = TransformerEncoder(d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.stride = self.resnet.stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = self.resnet(x)\n",
    "        fmap = self.transformer(fmap)\n",
    "        return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b872092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Projection Head ========================\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"MLP projection head for contrastive learning\"\"\"\n",
    "    def __init__(self, d_model, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06574fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Classification Heads ========================\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Linear Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "class HyperbolicProjection(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Projection (✅ 학습 가능한 c)\"\"\"\n",
    "    def __init__(self, c_init=1.0):\n",
    "        super().__init__()\n",
    "        self.c = nn.Parameter(torch.tensor(c_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ✅ Feature norm clipping\n",
    "        x = torch.clamp(x, -5.0, 5.0)\n",
    "\n",
    "        c = torch.clamp(self.c, min=0.1, max=10.0)\n",
    "        norm = torch.clamp(torch.norm(x, dim=-1, keepdim=True), min=1e-8)\n",
    "        max_norm = (1.0 / math.sqrt(c)) - 1e-4\n",
    "        scale = torch.clamp(norm, max=max_norm) / norm\n",
    "        return x * scale\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    \"\"\"Hyperbolic Space Classification Head\"\"\"\n",
    "    def __init__(self, d_model: int, num_classes: int, c_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pre_proj = nn.Linear(d_model, d_model)\n",
    "        self.hyperbolic_proj = HyperbolicProjection(c_init=c_init)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        h = self.pre_proj(pooled)\n",
    "        h_hyp = self.hyperbolic_proj(h)\n",
    "        logits = self.fc(h_hyp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249fa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SSL Model ========================\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    \"\"\"Self-Supervised Learning Model\"\"\"\n",
    "    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetTransformerBackbone(\n",
    "            in_channels=9, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(d_model, projection_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns normalized projection\"\"\"\n",
    "        fmap = self.backbone(x)\n",
    "        pooled = self.gap(fmap).squeeze(-1)\n",
    "        z = self.projection_head(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42036e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== EMA Utility ========================\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model parameters\"\"\"\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: param.clone().detach()\n",
    "                       for name, param in model.named_parameters() if param.requires_grad}\n",
    "        self.backup = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        \"\"\"Update EMA parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                self.shadow[name].mul_(self.decay).add_(param.data, alpha=1 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        \"\"\"Apply EMA parameters to model\"\"\"\n",
    "        self.backup = {name: param.data.clone() for name, param in model.named_parameters() if param.requires_grad}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        \"\"\"Restore original parameters\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f86599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Learning Rate Scheduler ========================\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Cosine annealing with linear warmup\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48ebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Contrastive Loss ========================\n",
    "\n",
    "def contrastive_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"NT-Xent Loss (InfoNCE)\"\"\"\n",
    "    B = z1.shape[0]\n",
    "    device = z1.device\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
    "\n",
    "    labels = torch.arange(B, device=device)\n",
    "    labels = torch.cat([labels + B, labels], dim=0)\n",
    "\n",
    "    mask = torch.eye(2 * B, device=device, dtype=torch.bool)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc243dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Consistency Loss ========================\n",
    "\n",
    "def consistency_loss(model: nn.Module, head: nn.Module, x_a: torch.Tensor, x_b: torch.Tensor,\n",
    "                     mix: float = 0.5, device: str = \"cuda\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    전이-유사 일관성 손실\n",
    "    Tail-Head Stitch 후 예측 분포의 KL divergence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        fmap_a = model.backbone(x_a) if hasattr(model, 'backbone') else model(x_a)\n",
    "        fmap_b = model.backbone(x_b) if hasattr(model, 'backbone') else model(x_b)\n",
    "\n",
    "        logits_a = head(fmap_a)\n",
    "        logits_b = head(fmap_b)\n",
    "\n",
    "        p_a = F.softmax(logits_a, dim=-1)\n",
    "        p_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    x_mix = tail_head_stitch(x_a, x_b, mix=mix)\n",
    "\n",
    "    fmap_mix = model.backbone(x_mix) if hasattr(model, 'backbone') else model(x_mix)\n",
    "    logits_mix = head(fmap_mix)\n",
    "    p_mix = F.log_softmax(logits_mix, dim=-1)\n",
    "\n",
    "    # Soft target: 0.5*p_a + 0.5*p_b\n",
    "    p_target = 0.5 * p_a + 0.5 * p_b\n",
    "\n",
    "    loss = F.kl_div(p_mix, p_target, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Training Functions ========================\n",
    "\n",
    "def pretrain_one_epoch(model: SSLModel, loader: DataLoader, opt: torch.optim.Optimizer,\n",
    "                       scheduler, ema: Optional[EMA], cfg: Config):\n",
    "    \"\"\"SSL Pretrain: No labels, only contrastive loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        x1 = augment_time_series(x, cfg)\n",
    "        x2 = augment_time_series(x, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = contrastive_loss(z1, z2, temperature=cfg.temperature)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "    return {\"ssl_loss\": total_loss / total_samples}\n",
    "\n",
    "def linear_eval_epoch(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                      opt: torch.optim.Optimizer, cfg: Config, train: bool = True):\n",
    "    \"\"\"Linear evaluation: Freeze backbone, train head only\"\"\"\n",
    "    if train:\n",
    "        backbone.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fmap = backbone(x)\n",
    "\n",
    "        logits = head(fmap)\n",
    "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "def finetune_epoch(model: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   opt: torch.optim.Optimizer, scheduler, ema: Optional[EMA],\n",
    "                   cfg: Config, train: bool = True):\n",
    "    \"\"\"Fine-tuning: Train both backbone and head (✅ 일관성 손실 추가)\"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "        head.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        head.eval()\n",
    "\n",
    "    total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "    total_correct, total_samples = 0, 0\n",
    "\n",
    "    data_iter = iter(loader)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "        # Forward pass\n",
    "        fmap = model.backbone(x) if hasattr(model, 'backbone') else model(x)\n",
    "        logits = head(fmap)\n",
    "        loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "        # ✅ 일관성 손실 (학습 시에만)\n",
    "        loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "        if train and cfg.consistency_weight > 0:\n",
    "            try:\n",
    "                x_b, _ = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(loader)\n",
    "                x_b, _ = next(data_iter)\n",
    "\n",
    "            x_b = x_b.to(cfg.device)\n",
    "            if x_b.size(0) == x.size(0):\n",
    "                loss_cons = consistency_loss(model, head, x, x_b, mix=0.5, device=cfg.device)\n",
    "\n",
    "        loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(head.parameters()), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_ce_loss += loss_ce.item() * y.size(0)\n",
    "        total_cons_loss += loss_cons.item() * y.size(0)\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"ce_loss\": total_ce_loss / total_samples,\n",
    "        \"cons_loss\": total_cons_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(backbone: nn.Module, head: nn.Module, loader: DataLoader,\n",
    "                   ema: Optional[EMA], cfg: Config, use_ema: bool = False):\n",
    "    \"\"\"Evaluate model (✅ EMA 지원)\"\"\"\n",
    "    if use_ema and ema is not None:\n",
    "        ema.apply_shadow(backbone)\n",
    "\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.device)\n",
    "        fmap = backbone(x)\n",
    "        logits = head(fmap)\n",
    "        y_pred.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    if use_ema and ema is not None:\n",
    "        ema.restore(backbone)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8275be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Transitional Test Set ========================\n",
    "\n",
    "def create_transitional_test_set(\n",
    "    orig_dataset: UCIHARInertial, class_A: str, class_B: str, p: float, mix: float\n",
    ") -> Tuple[UCIHARInertial, dict]:\n",
    "    \"\"\"Create transitional test set (✅ 정규화 보장)\"\"\"\n",
    "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
    "    N, C, T = X.shape\n",
    "\n",
    "    code_A, code_B = LABEL_NAME_TO_CODE[class_A], LABEL_NAME_TO_CODE[class_B]\n",
    "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
    "    mix_pts = int(T * mix)\n",
    "\n",
    "    targets_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p)), replace=False)\n",
    "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
    "    for t, s in zip(targets_A, sources_B):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    targets_B = np.random.choice(idx_B, max(1, int(len(idx_B) * p)), replace=False)\n",
    "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
    "    for t, s in zip(targets_B, sources_A):\n",
    "        X[t, :, -mix_pts:] = orig_dataset.X[s, :, :mix_pts]\n",
    "\n",
    "    # (수정) 이중 정규화 방지: 원본 스케일로 복원\n",
    "    X_restored = (X * orig_dataset.std) + orig_dataset.mean\n",
    "\n",
    "    # ✅ train 통계로 정규화하도록 mean/std 전달\n",
    "    mod_dataset = UCIHARInertial(\n",
    "        root=\"\", split=\"test\", mean=orig_dataset.mean, std=orig_dataset.std,\n",
    "        preloaded_data=(X_restored, y)\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        'class_A': class_A,\n",
    "        'class_B': class_B,\n",
    "        'p': p,\n",
    "        'mix': mix,\n",
    "        'modified_samples': len(targets_A) + len(targets_B),\n",
    "        'modified_ratio': (len(targets_A) + len(targets_B)) / N,\n",
    "    }\n",
    "    return mod_dataset, info\n",
    "\n",
    "# ======================== JSON Encoder ========================\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON Encoder for NumPy types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# ======================== Main Experiment Function ========================\n",
    "\n",
    "def run_full_comparison(cfg: Config):\n",
    "    \"\"\"Run complete supervised vs SSL comparison\"\"\"\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\n📦 Loading UCI-HAR Dataset...\")\n",
    "    train_set = UCIHARInertial(cfg.data_dir, \"train\")\n",
    "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=train_set.mean, std=train_set.std)\n",
    "    print(f\"   - Train samples: {len(train_set)}\")\n",
    "    print(f\"   - Test samples: {len(test_set_orig)}\")\n",
    "\n",
    "    # Create transitional test sets (✅ 2레벨 강도)\n",
    "    scenarios = [\n",
    "        # Level 1: Moderate (중간 강도)\n",
    "        (\"STANDING\", \"SITTING\", 0.50, 0.40),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.55, 0.42),\n",
    "        # Level 2: Strong (강한 강도)\n",
    "        (\"STANDING\", \"SITTING\", 0.70, 0.55),\n",
    "        (\"WALKING\", \"WALKING_UPSTAIRS\", 0.65, 0.52),\n",
    "        (\"SITTING\", \"LAYING\", 0.75, 0.58),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"    🔬 TRANSITIONAL TEST SETS 생성 (2레벨 강도)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    transition_test_data = []\n",
    "    for i, (clsA, clsB, p, mix) in enumerate(scenarios):\n",
    "        test_set_mod, info = create_transitional_test_set(test_set_orig, clsA, clsB, p=p, mix=mix)\n",
    "        transition_test_data.append((test_set_mod, info))\n",
    "        level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "        print(f\"   - [{level}] {clsA}↔{clsB} (p={p:.2f}, mix={mix:.2f}): {info['modified_samples']}개 샘플 변형\")\n",
    "\n",
    "    # Experiment configurations\n",
    "    experiment_configs = [\n",
    "        {\"name\": \"Supervised_Linear\", \"method\": \"supervised\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"Supervised_Hyperbolic\", \"method\": \"supervised\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_LinearEval_Linear\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_LinearEval_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"linear_eval\", \"use_hyperbolic\": True},\n",
    "        {\"name\": \"SSL_FineTune_Linear\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": False},\n",
    "        {\"name\": \"SSL_FineTune_Hyperbolic\", \"method\": \"ssl\", \"mode\": \"finetune\", \"use_hyperbolic\": True},\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    for exp_cfg in experiment_configs:\n",
    "        print(f\"\\n{'='*80}\\n   실험: {exp_cfg['name']}\\n{'='*80}\")\n",
    "\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "        if exp_cfg['method'] == 'supervised':\n",
    "            # Supervised Learning\n",
    "            print(\"\\n📚 Supervised Learning (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            backbone = ResNetTransformerBackbone(\n",
    "                in_channels=9, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "                n_layers=cfg.n_layers, dropout=cfg.dropout\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            params = list(backbone.parameters()) + list(head.parameters())\n",
    "            opt = torch.optim.AdamW(params, lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ✅ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "            warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "            # scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "            scheduler = None\n",
    "\n",
    "            # ✅ EMA\n",
    "            ema = EMA(backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            def train_supervised_epoch(backbone, head, loader, opt, scheduler, ema, cfg, train=True):\n",
    "                if train:\n",
    "                    backbone.train()\n",
    "                    head.train()\n",
    "                else:\n",
    "                    backbone.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                total_loss, total_ce_loss, total_cons_loss = 0.0, 0.0, 0.0\n",
    "                total_correct, total_samples = 0, 0\n",
    "\n",
    "                data_iter = iter(loader)\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "                    fmap = backbone(x)\n",
    "                    logits = head(fmap)\n",
    "                    loss_ce = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing if train else 0.0)\n",
    "\n",
    "                    # ✅ 일관성 손실\n",
    "                    loss_cons = torch.tensor(0.0, device=cfg.device)\n",
    "                    if train and cfg.consistency_weight > 0:\n",
    "                        try:\n",
    "                            x_b, _ = next(data_iter)\n",
    "                        except StopIteration:\n",
    "                            data_iter = iter(loader)\n",
    "                            x_b, _ = next(data_iter)\n",
    "\n",
    "                        x_b = x_b.to(cfg.device)\n",
    "                        if x_b.size(0) == x.size(0):\n",
    "                            loss_cons = consistency_loss(\n",
    "                                type('Model', (), {'backbone': backbone})(),\n",
    "                                head, x, x_b, mix=0.5, device=cfg.device\n",
    "                            )\n",
    "\n",
    "                    loss = loss_ce + cfg.consistency_weight * loss_cons\n",
    "\n",
    "                    if train:\n",
    "                        opt.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "                        opt.step()\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                        if ema is not None:\n",
    "                            ema.update(backbone)\n",
    "\n",
    "                    pred = logits.argmax(dim=-1)\n",
    "                    total_correct += (pred == y).sum().item()\n",
    "                    total_loss += loss.item() * y.size(0)\n",
    "                    total_ce_loss += loss_ce.item() * y.size(0)\n",
    "                    total_cons_loss += loss_cons.item() * y.size(0)\n",
    "                    total_samples += y.size(0)\n",
    "\n",
    "                return {\n",
    "                    \"loss\": total_loss / total_samples,\n",
    "                    \"ce_loss\": total_ce_loss / total_samples,\n",
    "                    \"cons_loss\": total_cons_loss / total_samples,\n",
    "                    \"acc\": total_correct / total_samples\n",
    "                }\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"Training for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_supervised_epoch(backbone, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "                te_acc, te_f1 = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'backbone': copy.deepcopy(backbone.state_dict()),\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if ema is not None:\n",
    "                        best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Supervised {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                          f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                          f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                backbone.load_state_dict(best_wts['backbone'])\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if ema is not None and 'ema_shadow' in best_wts:\n",
    "                    ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(backbone, head, test_loader_orig, ema, cfg, use_ema=cfg.use_ema)\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(backbone, head, test_loader_mod, ema, cfg, use_ema=cfg.use_ema)\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}↔{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ✅ 레벨별 분석\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"supervised\",\n",
    "                \"mode\": \"supervised\",\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "        else:  # SSL\n",
    "            # Stage 1: SSL Pretrain\n",
    "            print(\"\\n📚 Stage 1: Self-Supervised Pretraining (No Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            ssl_model = SSLModel(\n",
    "                d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers,\n",
    "                dropout=cfg.dropout, projection_dim=cfg.projection_dim\n",
    "            ).to(cfg.device)\n",
    "\n",
    "            pretrain_loader = DataLoader(train_set, cfg.pretrain_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            ssl_opt = torch.optim.AdamW(ssl_model.parameters(), lr=cfg.pretrain_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            # ✅ Cosine + Warmup scheduler\n",
    "            total_steps = cfg.pretrain_epochs * len(pretrain_loader)\n",
    "            warmup_steps = cfg.pretrain_warmup_epochs * len(pretrain_loader)\n",
    "            # ssl_scheduler = get_cosine_schedule_with_warmup(ssl_opt, warmup_steps, total_steps)\n",
    "            ssl_scheduler = None\n",
    "\n",
    "            # ✅ EMA\n",
    "            ssl_ema = EMA(ssl_model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "            print(f\"Pretraining for {cfg.pretrain_epochs} epochs...\")\n",
    "            for epoch in range(1, cfg.pretrain_epochs + 1):\n",
    "                stats = pretrain_one_epoch(ssl_model, pretrain_loader, ssl_opt, ssl_scheduler, ssl_ema, cfg)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(f\"[Pretrain {epoch:03d}/{cfg.pretrain_epochs}] SSL Loss: {stats['ssl_loss']:.4f}\")\n",
    "\n",
    "            # ✅ EMA 적용\n",
    "            if ssl_ema is not None:\n",
    "                ssl_ema.apply_shadow(ssl_model)\n",
    "\n",
    "            print(\"✅ Pretraining Complete!\")\n",
    "\n",
    "            # Stage 2: Linear Eval or Fine-tune\n",
    "            print(f\"\\n📚 Stage 2: {exp_cfg['mode'].upper()} (With Labels)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            if exp_cfg['use_hyperbolic']:\n",
    "                head = HyperbolicClassificationHead(cfg.d_model, num_classes=6, c_init=cfg.hyperbolic_c_init).to(cfg.device)\n",
    "            else:\n",
    "                head = ClassificationHead(cfg.d_model, num_classes=6).to(cfg.device)\n",
    "\n",
    "            finetune_loader = DataLoader(train_set, cfg.finetune_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "            test_loader_orig = DataLoader(test_set_orig, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "            if exp_cfg['mode'] == 'linear_eval':\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                opt = torch.optim.AdamW(head.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                ema = None  # Linear eval에서는 백본이 frozen이므로 EMA 불필요\n",
    "\n",
    "                train_fn = lambda: linear_eval_epoch(ssl_model.backbone, head, finetune_loader, opt, cfg, train=True)\n",
    "\n",
    "            else:  # finetune\n",
    "                for param in ssl_model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                # ✅ 백본/헤드 분리 학습률\n",
    "                backbone_params = list(ssl_model.backbone.parameters())\n",
    "                head_params = list(head.parameters())\n",
    "\n",
    "                opt = torch.optim.AdamW([\n",
    "                    {'params': backbone_params, 'lr': cfg.finetune_lr * cfg.finetune_backbone_lr_ratio},\n",
    "                    {'params': head_params, 'lr': cfg.finetune_lr},\n",
    "                ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "                total_steps = cfg.finetune_epochs * len(finetune_loader)\n",
    "                warmup_steps = cfg.finetune_warmup_epochs * len(finetune_loader)\n",
    "                scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
    "\n",
    "                # ✅ EMA (백본만)\n",
    "                ema = EMA(ssl_model.backbone, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "                train_fn = lambda: finetune_epoch(ssl_model, head, finetune_loader, opt, scheduler, ema, cfg, train=True)\n",
    "\n",
    "            best_acc, best_wts = 0.0, None\n",
    "            print(f\"{exp_cfg['mode']} for {cfg.finetune_epochs} epochs...\")\n",
    "\n",
    "            for epoch in range(1, cfg.finetune_epochs + 1):\n",
    "                stats = train_fn()\n",
    "                te_acc, te_f1 = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "                if te_acc > best_acc:\n",
    "                    best_acc = te_acc\n",
    "                    best_wts = {\n",
    "                        'head': copy.deepcopy(head.state_dict()),\n",
    "                    }\n",
    "                    if exp_cfg['mode'] == 'finetune':\n",
    "                        best_wts['backbone'] = copy.deepcopy(ssl_model.backbone.state_dict())\n",
    "                        if ema is not None:\n",
    "                            best_wts['ema_shadow'] = copy.deepcopy(ema.shadow)\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    if 'cons_loss' in stats:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} CE:{stats['ce_loss']:.4f} Cons:{stats['cons_loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"[{exp_cfg['mode']} {epoch:02d}/{cfg.finetune_epochs}] \"\n",
    "                              f\"Train L:{stats['loss']:.4f} A:{stats['acc']:.4f} | \"\n",
    "                              f\"Test A:{te_acc:.4f} F1:{te_f1:.4f}\")\n",
    "\n",
    "            if best_wts:\n",
    "                head.load_state_dict(best_wts['head'])\n",
    "                if exp_cfg['mode'] == 'finetune':\n",
    "                    ssl_model.backbone.load_state_dict(best_wts['backbone'])\n",
    "                    if ema is not None and 'ema_shadow' in best_wts:\n",
    "                        ema.shadow = best_wts['ema_shadow']\n",
    "\n",
    "            print(f\"✅ Best Test Acc: {best_acc:.4f}\")\n",
    "\n",
    "            acc_orig, f1_orig = evaluate_model(ssl_model.backbone, head, test_loader_orig, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "\n",
    "            transition_results = []\n",
    "            print(\"\\n   🔍 전이 테스트셋 평가...\")\n",
    "\n",
    "            for i, (test_set_mod, info) in enumerate(transition_test_data):\n",
    "                test_loader_mod = DataLoader(test_set_mod, cfg.finetune_batch_size, num_workers=cfg.num_workers)\n",
    "                acc_trans, f1_trans = evaluate_model(ssl_model.backbone, head, test_loader_mod, ema, cfg, use_ema=(cfg.use_ema and exp_cfg['mode'] == 'finetune'))\n",
    "                drop = acc_orig - acc_trans\n",
    "\n",
    "                level = \"Moderate\" if i < 2 else \"Strong\"\n",
    "                transition_results.append({\n",
    "                    'scenario': i+1,\n",
    "                    'level': level,\n",
    "                    'class_A': info['class_A'],\n",
    "                    'class_B': info['class_B'],\n",
    "                    'p': info['p'],\n",
    "                    'mix': info['mix'],\n",
    "                    'class_acc': acc_trans,\n",
    "                    'class_f1': f1_trans,\n",
    "                    'class_drop': drop,\n",
    "                })\n",
    "\n",
    "                print(f\"     - [{level}] Scenario {i+1} ({info['class_A']}↔{info['class_B']}): \"\n",
    "                      f\"Acc={acc_trans:.4f} F1={f1_trans:.4f} (Drop={drop:.4f})\")\n",
    "\n",
    "            avg_trans_acc = np.mean([r['class_acc'] for r in transition_results])\n",
    "            avg_trans_f1 = np.mean([r['class_f1'] for r in transition_results])\n",
    "            avg_drop = acc_orig - avg_trans_acc\n",
    "            retention = (1 - avg_drop / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            # ✅ 레벨별 분석\n",
    "            moderate_results = [r for r in transition_results if r['level'] == 'Moderate']\n",
    "            strong_results = [r for r in transition_results if r['level'] == 'Strong']\n",
    "\n",
    "            avg_moderate_acc = np.mean([r['class_acc'] for r in moderate_results]) if moderate_results else 0\n",
    "            avg_strong_acc = np.mean([r['class_acc'] for r in strong_results]) if strong_results else 0\n",
    "\n",
    "            retention_moderate = (1 - (acc_orig - avg_moderate_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "            retention_strong = (1 - (acc_orig - avg_strong_acc) / acc_orig) * 100 if acc_orig > 0 else 0\n",
    "\n",
    "            results_table.append({\n",
    "                \"config\": exp_cfg[\"name\"],\n",
    "                \"method\": \"ssl\",\n",
    "                \"mode\": exp_cfg['mode'],\n",
    "                \"classifier\": \"Hyperbolic\" if exp_cfg[\"use_hyperbolic\"] else \"Linear\",\n",
    "                \"orig_acc\": acc_orig,\n",
    "                \"orig_f1\": f1_orig,\n",
    "                \"avg_trans_acc\": avg_trans_acc,\n",
    "                \"avg_trans_f1\": avg_trans_f1,\n",
    "                \"avg_drop\": avg_drop,\n",
    "                \"retention\": retention,\n",
    "                \"retention_moderate\": retention_moderate,\n",
    "                \"retention_strong\": retention_strong,\n",
    "                \"transition_results\": transition_results\n",
    "            })\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"   📊 SUPERVISED vs TRUE SSL 실험 결과 (최적화 버전)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Method':<12} {'Mode':<12} {'Classifier':<12} {'Orig Acc':<10} {'Trans Acc':<11} {'Drop':<10} {'Retention':<10}\")\n",
    "    print(\"-\" * 115)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['method']:<12} {r['mode']:<12} {r['classifier']:<12} \"\n",
    "              f\"{r['orig_acc']:.4f}     {r['avg_trans_acc']:.4f}      \"\n",
    "              f\"{r['avg_drop']:.4f}  {r['retention']:.2f}%\")\n",
    "\n",
    "    # ✅ 레벨별 결과 추가\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 레벨별 Retention 분석\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<35} {'Overall':<12} {'Moderate':<12} {'Strong':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for r in results_table:\n",
    "        print(f\"{r['config']:<35} {r['retention']:>6.2f}%      {r['retention_moderate']:>6.2f}%       {r['retention_strong']:>6.2f}%\")\n",
    "\n",
    "    # Detailed analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 상세 비교 분석\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Final ranking\n",
    "    sorted_results = sorted(results_table, key=lambda x: x['retention'], reverse=True)\n",
    "    print(\"\\n🏆 최종 성능 랭킹 (Overall Retention 기준)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for rank, r in enumerate(sorted_results, 1):\n",
    "        method_mode = f\"{r['method']}-{r['mode']}\" if r['method'] == 'ssl' else r['method']\n",
    "        print(f\"   {rank}. {r['config']:<35} ({method_mode:<20}) \"\n",
    "              f\"Retention: {r['retention']:.2f}% (Mod: {r['retention_moderate']:.2f}% | Str: {r['retention_strong']:.2f}%)\")\n",
    "\n",
    "    best_config = sorted_results[0]\n",
    "    best_ssl = max([r for r in results_table if r['method'] == 'ssl'], key=lambda x: x['retention'])\n",
    "    best_sup = max([r for r in results_table if r['method'] == 'supervised'], key=lambda x: x['retention'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 결론\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - 최고 성능: {best_config['config']} (Retention: {best_config['retention']:.2f}%)\")\n",
    "    print(f\"   - Supervised baseline: {best_sup['retention']:.2f}% (Mod: {best_sup['retention_moderate']:.2f}% | Str: {best_sup['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - SSL best: {best_ssl['retention']:.2f}% (Mod: {best_ssl['retention_moderate']:.2f}% | Str: {best_ssl['retention_strong']:.2f}%)\")\n",
    "    print(f\"   - Performance gap: {abs(best_ssl['retention'] - best_sup['retention']):.2f}pp\")\n",
    "\n",
    "    # ✅ 개선 효과 분석\n",
    "    print(\"\\n   ✨ 최적화 개선사항:\")\n",
    "    print(\"   - 정규화 버그 수정 ✅\")\n",
    "    print(\"   - Cosine + Warmup 스케줄러 ✅\")\n",
    "    print(\"   - EMA (Exponential Moving Average) ✅\")\n",
    "    print(\"   - 백본/헤드 분리 학습률 (Fine-tune) ✅\")\n",
    "    print(\"   - 전이-유사 일관성 손실 (Tail-Head Stitch) ✅\")\n",
    "    print(f\"   - 증강 강도 최적화 (warp: 0.10, cutout: 0.10) ✅\")\n",
    "    print(\"   - 학습 가능한 Hyperbolic c ✅\")\n",
    "    print(\"   - 2레벨 전이 시나리오 (Moderate/Strong) ✅\")\n",
    "\n",
    "    # Save results\n",
    "    save_path = os.path.join(cfg.save_dir, \"supervised_vs_ssl_results_optimized.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results_table, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    visualization_data = {\n",
    "        'configs': [r['config'] for r in results_table],\n",
    "        'methods': [r['method'] for r in results_table],\n",
    "        'modes': [r['mode'] for r in results_table],\n",
    "        'classifiers': [r['classifier'] for r in results_table],\n",
    "        'orig_acc': [r['orig_acc'] for r in results_table],\n",
    "        'orig_f1': [r['orig_f1'] for r in results_table],\n",
    "        'trans_acc': [r['avg_trans_acc'] for r in results_table],\n",
    "        'trans_f1': [r['avg_trans_f1'] for r in results_table],\n",
    "        'retention': [r['retention'] for r in results_table],\n",
    "        'retention_moderate': [r['retention_moderate'] for r in results_table],\n",
    "        'retention_strong': [r['retention_strong'] for r in results_table],\n",
    "        'avg_drop': [r['avg_drop'] for r in results_table]\n",
    "    }\n",
    "\n",
    "    viz_path = os.path.join(cfg.save_dir, \"visualization_data_optimized.json\")\n",
    "    with open(viz_path, \"w\") as f:\n",
    "        json.dump(visualization_data, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n✅ Results saved to:\")\n",
    "    print(f\"   - {save_path}\")\n",
    "    print(f\"   - {viz_path}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c68009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   🧪 UCI-HAR Comprehensive Comparison (✨ OPTIMIZED)\n",
      "   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\n",
      "   Architecture: ResNet + Transformer Encoder\n",
      "================================================================================\n",
      "\n",
      "   📋 실험 설계:\n",
      "   1. 동일한 백본 (ResNet + Transformer)\n",
      "   2. 2레벨 전이 데이터셋 (Moderate/Strong)\n",
      "   3. 6가지 설정 비교:\n",
      "      ├─ Supervised × (Linear, Hyperbolic)\n",
      "      ├─ SSL Linear Eval × (Linear, Hyperbolic)\n",
      "      └─ SSL Fine-tune × (Linear, Hyperbolic)\n",
      "================================================================================\n",
      "\n",
      "   ⚙️  Supervised 설정:\n",
      "   - Epochs: 50\n",
      "   - Batch size: 128\n",
      "   - Learning rate: 0.0003\n",
      "   - Warmup: 0 epochs\n",
      "   - EMA decay: Disabled\n",
      "   - Consistency weight: 0.0\n",
      "   - Training: End-to-end with labels + consistency loss\n",
      "\n",
      "   ⚙️  SSL 설정:\n",
      "   - Stage 1 (Pretrain): 100 epochs, batch=512, lr=0.001\n",
      "     → Contrastive learning only (NO LABELS)\n",
      "     → Label-independent augmentation\n",
      "     → Warmup: 0 epochs\n",
      "     → EMA decay: Disabled\n",
      "   - Stage 2 (Eval/FT): 50 epochs, batch=128, lr=0.0003\n",
      "     → Linear Eval: Freeze backbone\n",
      "     → Fine-tune: Train all (backbone LR × 0.1)\n",
      "     → Consistency weight: 0.0\n",
      "\n",
      "   🔧 Augmentations (SSL - Optimized):\n",
      "   - Jitter (scale=0.05)\n",
      "   - Scaling (range=(0.8, 1.2))\n",
      "   - Channel Drop (prob=0.2)\n",
      "   - Time Warp (prob=0.1) ✅ 0.3→0.10\n",
      "   - Cutout (prob=0.2, ratio=0.1) ✅ 0.2→0.10\n",
      "   - ALL label-independent!\n",
      "\n",
      "   🏗️  Architecture:\n",
      "   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads=4, layers=2)\n",
      "   - d_model: 128, dropout: 0.1\n",
      "   - Classifier: Linear vs Hyperbolic (c_init=1.0, learnable ✅)\n",
      "   - Projection dim (SSL): 128\n",
      "\n",
      "   🔬 SSL Contrastive Learning:\n",
      "   - Loss: NT-Xent (InfoNCE)\n",
      "   - Temperature: 0.07\n",
      "   - Negative samples: 2*batch_size - 2\n",
      "\n",
      "   ✨ 최적화 개선사항:\n",
      "   - 정규화 버그 수정 (전이 테스트셋)\n",
      "   - Cosine Annealing + Warmup\n",
      "   - EMA (Exponential Moving Average)\n",
      "   - 백본/헤드 분리 학습률 (Fine-tune)\n",
      "   - 전이-유사 일관성 손실 (Tail-Head Stitch)\n",
      "   - 증강 강도 최적화 (경계 정보 보존)\n",
      "   - 학습 가능한 Hyperbolic c\n",
      "   - 2레벨 전이 시나리오 (Moderate/Strong)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "📦 Loading UCI-HAR Dataset...\n",
      "[OK] train: X(7352, 9, 128), y(7352,)\n",
      "[OK] test: X(2947, 9, 128), y(2947,)\n",
      "   - Train samples: 7352\n",
      "   - Test samples: 2947\n",
      "\n",
      "================================================================================\n",
      "    🔬 TRANSITIONAL TEST SETS 생성 (2레벨 강도)\n",
      "================================================================================\n",
      "   - [Moderate] STANDING↔SITTING (p=0.50, mix=0.40): 511개 샘플 변형\n",
      "   - [Moderate] WALKING↔WALKING_UPSTAIRS (p=0.55, mix=0.42): 531개 샘플 변형\n",
      "   - [Strong] STANDING↔SITTING (p=0.70, mix=0.55): 715개 샘플 변형\n",
      "   - [Strong] WALKING↔WALKING_UPSTAIRS (p=0.65, mix=0.52): 628개 샘플 변형\n",
      "   - [Strong] SITTING↔LAYING (p=0.75, mix=0.58): 770개 샘플 변형\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:0.5001 CE:0.5001 Cons:0.0000 A:0.8864 | Test A:0.9260 F1:0.9269\n",
      "[Supervised 10/50] Train L:0.3215 CE:0.3215 Cons:0.0000 A:0.9597 | Test A:0.9328 F1:0.9328\n",
      "[Supervised 20/50] Train L:0.3040 CE:0.3040 Cons:0.0000 A:0.9642 | Test A:0.9284 F1:0.9290\n",
      "[Supervised 30/50] Train L:0.2881 CE:0.2881 Cons:0.0000 A:0.9748 | Test A:0.9413 F1:0.9416\n",
      "[Supervised 40/50] Train L:0.2874 CE:0.2874 Cons:0.0000 A:0.9769 | Test A:0.9525 F1:0.9532\n",
      "[Supervised 50/50] Train L:0.2775 CE:0.2775 Cons:0.0000 A:0.9826 | Test A:0.9454 F1:0.9457\n",
      "✅ Best Test Acc: 0.9545\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.9111 F1=0.9128 (Drop=0.0434)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.9091 F1=0.9085 (Drop=0.0455)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8585 F1=0.8620 (Drop=0.0960)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8388 F1=0.8344 (Drop=0.1157)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.8120 F1=0.8124 (Drop=0.1425)\n",
      "\n",
      "================================================================================\n",
      "   실험: Supervised_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Supervised Learning (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Training for 50 epochs...\n",
      "[Supervised 01/50] Train L:1.4078 CE:1.4078 Cons:0.0000 A:0.8394 | Test A:0.9209 F1:0.9208\n",
      "[Supervised 10/50] Train L:0.5614 CE:0.5614 Cons:0.0000 A:0.9589 | Test A:0.9002 F1:0.9013\n",
      "[Supervised 20/50] Train L:0.3679 CE:0.3679 Cons:0.0000 A:0.9640 | Test A:0.9291 F1:0.9309\n",
      "[Supervised 30/50] Train L:0.3104 CE:0.3104 Cons:0.0000 A:0.9763 | Test A:0.9253 F1:0.9262\n",
      "[Supervised 40/50] Train L:0.2853 CE:0.2853 Cons:0.0000 A:0.9791 | Test A:0.9403 F1:0.9419\n",
      "[Supervised 50/50] Train L:0.2724 CE:0.2724 Cons:0.0000 A:0.9879 | Test A:0.9505 F1:0.9505\n",
      "✅ Best Test Acc: 0.9505\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.9036 F1=0.9022 (Drop=0.0468)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.8907 F1=0.8892 (Drop=0.0597)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8510 F1=0.8466 (Drop=0.0994)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8334 F1=0.8282 (Drop=0.1171)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.8327 F1=0.8129 (Drop=0.1177)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.8074 A:0.2393 | Test A:0.4581 F1:0.4450\n",
      "[linear_eval 10/50] Train L:0.5546 A:0.9125 | Test A:0.8955 F1:0.8941\n",
      "[linear_eval 20/50] Train L:0.4666 A:0.9264 | Test A:0.9063 F1:0.9056\n",
      "[linear_eval 30/50] Train L:0.4429 A:0.9342 | Test A:0.9108 F1:0.9101\n",
      "[linear_eval 40/50] Train L:0.4306 A:0.9380 | Test A:0.9121 F1:0.9115\n",
      "[linear_eval 50/50] Train L:0.4221 A:0.9418 | Test A:0.9128 F1:0.9123\n",
      "✅ Best Test Acc: 0.9138\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.8890 F1=0.8891 (Drop=0.0248)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.8616 F1=0.8579 (Drop=0.0523)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8293 F1=0.8314 (Drop=0.0845)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8008 F1=0.7923 (Drop=0.1130)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.7760 F1=0.7806 (Drop=0.1378)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_LinearEval_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: LINEAR_EVAL (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "linear_eval for 50 epochs...\n",
      "[linear_eval 01/50] Train L:1.6645 A:0.6007 | Test A:0.8392 F1:0.8281\n",
      "[linear_eval 10/50] Train L:0.6622 A:0.9298 | Test A:0.9094 F1:0.9086\n",
      "[linear_eval 20/50] Train L:0.4351 A:0.9411 | Test A:0.9192 F1:0.9186\n",
      "[linear_eval 30/50] Train L:0.3778 A:0.9497 | Test A:0.9189 F1:0.9182\n",
      "[linear_eval 40/50] Train L:0.3584 A:0.9524 | Test A:0.9237 F1:0.9231\n",
      "[linear_eval 50/50] Train L:0.3506 A:0.9559 | Test A:0.9233 F1:0.9227\n",
      "✅ Best Test Acc: 0.9277\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.8999 F1=0.9006 (Drop=0.0278)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.8775 F1=0.8744 (Drop=0.0502)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8303 F1=0.8335 (Drop=0.0974)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8059 F1=0.7974 (Drop=0.1218)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.7679 F1=0.7724 (Drop=0.1598)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Linear\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.4352 CE:1.4352 Cons:0.0000 A:0.5197 | Test A:0.8514 F1:0.8502\n",
      "[finetune 10/50] Train L:0.3074 CE:0.3074 Cons:0.0000 A:0.9693 | Test A:0.9566 F1:0.9566\n",
      "[finetune 20/50] Train L:0.2734 CE:0.2734 Cons:0.0000 A:0.9861 | Test A:0.9674 F1:0.9674\n",
      "[finetune 30/50] Train L:0.2644 CE:0.2644 Cons:0.0000 A:0.9913 | Test A:0.9701 F1:0.9701\n",
      "[finetune 40/50] Train L:0.2603 CE:0.2603 Cons:0.0000 A:0.9931 | Test A:0.9684 F1:0.9684\n",
      "[finetune 50/50] Train L:0.2585 CE:0.2585 Cons:0.0000 A:0.9932 | Test A:0.9695 F1:0.9695\n",
      "✅ Best Test Acc: 0.9708\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.9213 F1=0.9224 (Drop=0.0495)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.9135 F1=0.9112 (Drop=0.0573)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8381 F1=0.8413 (Drop=0.1327)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8588 F1=0.8518 (Drop=0.1120)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.8178 F1=0.8124 (Drop=0.1530)\n",
      "\n",
      "================================================================================\n",
      "   실험: SSL_FineTune_Hyperbolic\n",
      "================================================================================\n",
      "\n",
      "📚 Stage 1: Self-Supervised Pretraining (No Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "Pretraining for 100 epochs...\n",
      "[Pretrain 001/100] SSL Loss: 4.8186\n",
      "[Pretrain 010/100] SSL Loss: 1.8973\n",
      "[Pretrain 020/100] SSL Loss: 1.0982\n",
      "[Pretrain 030/100] SSL Loss: 1.0177\n",
      "[Pretrain 040/100] SSL Loss: 1.0625\n",
      "[Pretrain 050/100] SSL Loss: 0.8192\n",
      "[Pretrain 060/100] SSL Loss: 0.7844\n",
      "[Pretrain 070/100] SSL Loss: 0.7696\n",
      "[Pretrain 080/100] SSL Loss: 0.8754\n",
      "[Pretrain 090/100] SSL Loss: 0.5887\n",
      "[Pretrain 100/100] SSL Loss: 0.6765\n",
      "✅ Pretraining Complete!\n",
      "\n",
      "📚 Stage 2: FINETUNE (With Labels)\n",
      "--------------------------------------------------------------------------------\n",
      "finetune for 50 epochs...\n",
      "[finetune 01/50] Train L:1.6141 CE:1.6141 Cons:0.0000 A:0.6625 | Test A:0.8884 F1:0.8839\n",
      "[finetune 10/50] Train L:0.5404 CE:0.5404 Cons:0.0000 A:0.9699 | Test A:0.9535 F1:0.9537\n",
      "[finetune 20/50] Train L:0.3409 CE:0.3409 Cons:0.0000 A:0.9879 | Test A:0.9688 F1:0.9688\n",
      "[finetune 30/50] Train L:0.2995 CE:0.2995 Cons:0.0000 A:0.9890 | Test A:0.9698 F1:0.9699\n",
      "[finetune 40/50] Train L:0.2814 CE:0.2814 Cons:0.0000 A:0.9935 | Test A:0.9701 F1:0.9702\n",
      "[finetune 50/50] Train L:0.2773 CE:0.2773 Cons:0.0000 A:0.9950 | Test A:0.9701 F1:0.9702\n",
      "✅ Best Test Acc: 0.9718\n",
      "\n",
      "   🔍 전이 테스트셋 평가...\n",
      "     - [Moderate] Scenario 1 (STANDING↔SITTING): Acc=0.9209 F1=0.9217 (Drop=0.0509)\n",
      "     - [Moderate] Scenario 2 (WALKING↔WALKING_UPSTAIRS): Acc=0.9063 F1=0.9032 (Drop=0.0655)\n",
      "     - [Strong] Scenario 3 (STANDING↔SITTING): Acc=0.8409 F1=0.8426 (Drop=0.1310)\n",
      "     - [Strong] Scenario 4 (WALKING↔WALKING_UPSTAIRS): Acc=0.8683 F1=0.8596 (Drop=0.1035)\n",
      "     - [Strong] Scenario 5 (SITTING↔LAYING): Acc=0.7879 F1=0.7935 (Drop=0.1839)\n",
      "\n",
      "================================================================================\n",
      "   📊 SUPERVISED vs TRUE SSL 실험 결과 (최적화 버전)\n",
      "================================================================================\n",
      "Config                              Method       Mode         Classifier   Orig Acc   Trans Acc   Drop       Retention \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Supervised_Linear                   supervised   supervised   Linear       0.9545     0.8659      0.0886  90.71%\n",
      "Supervised_Hyperbolic               supervised   supervised   Hyperbolic   0.9505     0.8623      0.0882  90.72%\n",
      "SSL_LinearEval_Linear               ssl          linear_eval  Linear       0.9138     0.8314      0.0825  90.98%\n",
      "SSL_LinearEval_Hyperbolic           ssl          linear_eval  Hyperbolic   0.9277     0.8363      0.0914  90.15%\n",
      "SSL_FineTune_Linear                 ssl          finetune     Linear       0.9708     0.8699      0.1009  89.61%\n",
      "SSL_FineTune_Hyperbolic             ssl          finetune     Hyperbolic   0.9718     0.8649      0.1070  88.99%\n",
      "\n",
      "================================================================================\n",
      "📊 레벨별 Retention 분석\n",
      "================================================================================\n",
      "Config                              Overall      Moderate     Strong      \n",
      "--------------------------------------------------------------------------------\n",
      "Supervised_Linear                    90.71%       95.34%        87.63%\n",
      "Supervised_Hyperbolic                90.72%       94.39%        88.28%\n",
      "SSL_LinearEval_Linear                90.98%       95.79%        87.77%\n",
      "SSL_LinearEval_Hyperbolic            90.15%       95.79%        86.38%\n",
      "SSL_FineTune_Linear                  89.61%       94.49%        86.35%\n",
      "SSL_FineTune_Hyperbolic              88.99%       94.01%        85.65%\n",
      "\n",
      "================================================================================\n",
      "📊 상세 비교 분석\n",
      "================================================================================\n",
      "\n",
      "🏆 최종 성능 랭킹 (Overall Retention 기준)\n",
      "--------------------------------------------------------------------------------\n",
      "   1. SSL_LinearEval_Linear               (ssl-linear_eval     ) Retention: 90.98% (Mod: 95.79% | Str: 87.77%)\n",
      "   2. Supervised_Hyperbolic               (supervised          ) Retention: 90.72% (Mod: 94.39% | Str: 88.28%)\n",
      "   3. Supervised_Linear                   (supervised          ) Retention: 90.71% (Mod: 95.34% | Str: 87.63%)\n",
      "   4. SSL_LinearEval_Hyperbolic           (ssl-linear_eval     ) Retention: 90.15% (Mod: 95.79% | Str: 86.38%)\n",
      "   5. SSL_FineTune_Linear                 (ssl-finetune        ) Retention: 89.61% (Mod: 94.49% | Str: 86.35%)\n",
      "   6. SSL_FineTune_Hyperbolic             (ssl-finetune        ) Retention: 88.99% (Mod: 94.01% | Str: 85.65%)\n",
      "\n",
      "================================================================================\n",
      "🎯 결론\n",
      "================================================================================\n",
      "   - 최고 성능: SSL_LinearEval_Linear (Retention: 90.98%)\n",
      "   - Supervised baseline: 90.72% (Mod: 94.39% | Str: 88.28%)\n",
      "   - SSL best: 90.98% (Mod: 95.79% | Str: 87.77%)\n",
      "   - Performance gap: 0.25pp\n",
      "\n",
      "   ✨ 최적화 개선사항:\n",
      "   - 정규화 버그 수정 ✅\n",
      "   - Cosine + Warmup 스케줄러 ✅\n",
      "   - EMA (Exponential Moving Average) ✅\n",
      "   - 백본/헤드 분리 학습률 (Fine-tune) ✅\n",
      "   - 전이-유사 일관성 손실 (Tail-Head Stitch) ✅\n",
      "   - 증강 강도 최적화 (warp: 0.10, cutout: 0.10) ✅\n",
      "   - 학습 가능한 Hyperbolic c ✅\n",
      "   - 2레벨 전이 시나리오 (Moderate/Strong) ✅\n",
      "\n",
      "✅ Results saved to:\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\\supervised_vs_ssl_results_optimized.json\n",
      "   - C://Users/park9/HAR/SSL_HAR/RESULTS/IMPROVE/cons+ema+CosOFF\\visualization_data_optimized.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================== Main Entry Point ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"   🧪 UCI-HAR Comprehensive Comparison (✨ OPTIMIZED)\")\n",
    "    print(\"   SUPERVISED vs TRUE SELF-SUPERVISED LEARNING\")\n",
    "    print(\"   Architecture: ResNet + Transformer Encoder\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   📋 실험 설계:\")\n",
    "    print(\"   1. 동일한 백본 (ResNet + Transformer)\")\n",
    "    print(\"   2. 2레벨 전이 데이터셋 (Moderate/Strong)\")\n",
    "    print(\"   3. 6가지 설정 비교:\")\n",
    "    print(\"      ├─ Supervised × (Linear, Hyperbolic)\")\n",
    "    print(\"      ├─ SSL Linear Eval × (Linear, Hyperbolic)\")\n",
    "    print(\"      └─ SSL Fine-tune × (Linear, Hyperbolic)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n   ⚙️  Supervised 설정:\")\n",
    "    print(f\"   - Epochs: {config.finetune_epochs}\")\n",
    "    print(f\"   - Batch size: {config.finetune_batch_size}\")\n",
    "    print(f\"   - Learning rate: {config.finetune_lr}\")\n",
    "    print(f\"   - Warmup: {config.finetune_warmup_epochs} epochs\")\n",
    "    print(f\"   - EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"   - Training: End-to-end with labels + consistency loss\")\n",
    "    print(f\"\\n   ⚙️  SSL 설정:\")\n",
    "    print(f\"   - Stage 1 (Pretrain): {config.pretrain_epochs} epochs, batch={config.pretrain_batch_size}, lr={config.pretrain_lr}\")\n",
    "    print(f\"     → Contrastive learning only (NO LABELS)\")\n",
    "    print(f\"     → Label-independent augmentation\")\n",
    "    print(f\"     → Warmup: {config.pretrain_warmup_epochs} epochs\")\n",
    "    print(f\"     → EMA decay: {config.ema_decay if config.use_ema else 'Disabled'}\")\n",
    "    print(f\"   - Stage 2 (Eval/FT): {config.finetune_epochs} epochs, batch={config.finetune_batch_size}, lr={config.finetune_lr}\")\n",
    "    print(f\"     → Linear Eval: Freeze backbone\")\n",
    "    print(f\"     → Fine-tune: Train all (backbone LR × {config.finetune_backbone_lr_ratio})\")\n",
    "    print(f\"     → Consistency weight: {config.consistency_weight}\")\n",
    "    print(f\"\\n   🔧 Augmentations (SSL - Optimized):\")\n",
    "    print(f\"   - Jitter (scale={config.aug_jitter_scale})\")\n",
    "    print(f\"   - Scaling (range={config.aug_scale_range})\")\n",
    "    print(f\"   - Channel Drop (prob={config.aug_channel_drop_prob})\")\n",
    "    print(f\"   - Time Warp (prob={config.aug_time_warp_prob}) ✅ 0.3→0.10\")\n",
    "    print(f\"   - Cutout (prob={config.aug_cutout_prob}, ratio={config.aug_cutout_ratio}) ✅ 0.2→0.10\")\n",
    "    print(\"   - ALL label-independent!\")\n",
    "    print(f\"\\n   🏗️  Architecture:\")\n",
    "    print(f\"   - Backbone: ResNet(layers=[2,2,2]) + Transformer(heads={config.n_heads}, layers={config.n_layers})\")\n",
    "    print(f\"   - d_model: {config.d_model}, dropout: {config.dropout}\")\n",
    "    print(f\"   - Classifier: Linear vs Hyperbolic (c_init={config.hyperbolic_c_init}, learnable ✅)\")\n",
    "    print(f\"   - Projection dim (SSL): {config.projection_dim}\")\n",
    "    print(f\"\\n   🔬 SSL Contrastive Learning:\")\n",
    "    print(f\"   - Loss: NT-Xent (InfoNCE)\")\n",
    "    print(f\"   - Temperature: {config.temperature}\")\n",
    "    print(f\"   - Negative samples: 2*batch_size - 2\")\n",
    "    print(f\"\\n   ✨ 최적화 개선사항:\")\n",
    "    print(\"   - 정규화 버그 수정 (전이 테스트셋)\")\n",
    "    print(\"   - Cosine Annealing + Warmup\")\n",
    "    print(\"   - EMA (Exponential Moving Average)\")\n",
    "    print(\"   - 백본/헤드 분리 학습률 (Fine-tune)\")\n",
    "    print(\"   - 전이-유사 일관성 손실 (Tail-Head Stitch)\")\n",
    "    print(\"   - 증강 강도 최적화 (경계 정보 보존)\")\n",
    "    print(\"   - 학습 가능한 Hyperbolic c\")\n",
    "    print(\"   - 2레벨 전이 시나리오 (Moderate/Strong)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    run_full_comparison(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee20d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
